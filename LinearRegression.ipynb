{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77901c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.value import Value\n",
    "import src.function as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c98e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_diabetes(return_X_y = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = Value(X_train, \"x_train\", requires_grad=False)\n",
    "X_test = Value(X_test, \"x_test\", requires_grad=False)\n",
    "y_train = Value(y_train, \"y_train\", requires_grad=False)\n",
    "y_test = Value(y_test, \"y_test\", requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a43794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Value(np.random.randn(X.shape[1]) * 10**(-1), \"w\", requires_grad = True)\n",
    "b = Value(np.array([np.mean(y_train.value)]), \"b\", requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21b1253f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Function of type <class 'src.function._Matmul'> with name matmul_0\n",
      "Creating Function of type <class 'src.function._Add'> with name add_0\n",
      "Creating Function of type <class 'src.function._MSELoss'> with name mse_loss_0\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "\n",
    "matmul1 = F._FunctionFactory().get_new_function_of_type(F._Matmul)\n",
    "add1 = F._FunctionFactory().get_new_function_of_type(F._Add)\n",
    "loss = F._FunctionFactory().get_new_function_of_type(F._MSELoss)\n",
    "    \n",
    "def fit(num_epoch):     \n",
    "    for i in range(num_epoch):\n",
    "        \n",
    "        y = add1.forward(matmul1.forward(X_train, w), b)\n",
    "        l = loss.forward(y, y_train)\n",
    "        loss.backward()\n",
    "        l_test = loss.forward(add1.forward(matmul1.forward(X_test, w), b), y_test)\n",
    "        w.value -= lr * w.grad\n",
    "        b.value -= lr * b.grad\n",
    "        l.zero_grad()   \n",
    "        \n",
    "        # if i % 10 == 0:\n",
    "        print(f\"Epoch {i+1}/{num_epoch}: Train Loss: {l.value} Test Loss: {l_test.value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "925bedab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4000: Train Loss: 2849.759719872876 Test Loss: 3015.128010904278\n",
      "Epoch 2/4000: Train Loss: 2849.745161024955 Test Loss: 3015.108925266985\n",
      "Epoch 3/4000: Train Loss: 2849.7306175452145 Test Loss: 3015.089856255902\n",
      "Epoch 4/4000: Train Loss: 2849.716089416884 Test Loss: 3015.0708038682083\n",
      "Epoch 5/4000: Train Loss: 2849.70157662322 Test Loss: 3015.0517681010356\n",
      "Epoch 6/4000: Train Loss: 2849.6870791475003 Test Loss: 3015.032748951451\n",
      "Epoch 7/4000: Train Loss: 2849.6725969730287 Test Loss: 3015.013746416471\n",
      "Epoch 8/4000: Train Loss: 2849.6581300831317 Test Loss: 3014.994760493052\n",
      "Epoch 9/4000: Train Loss: 2849.6436784611597 Test Loss: 3014.9757911780916\n",
      "Epoch 10/4000: Train Loss: 2849.6292420904874 Test Loss: 3014.9568384684308\n",
      "Epoch 11/4000: Train Loss: 2849.614820954513 Test Loss: 3014.937902360852\n",
      "Epoch 12/4000: Train Loss: 2849.600415036657 Test Loss: 3014.91898285208\n",
      "Epoch 13/4000: Train Loss: 2849.586024320364 Test Loss: 3014.900079938781\n",
      "Epoch 14/4000: Train Loss: 2849.5716487891023 Test Loss: 3014.88119361756\n",
      "Epoch 15/4000: Train Loss: 2849.557288426363 Test Loss: 3014.862323884966\n",
      "Epoch 16/4000: Train Loss: 2849.5429432156607 Test Loss: 3014.8434707374863\n",
      "Epoch 17/4000: Train Loss: 2849.528613140533 Test Loss: 3014.8246341715535\n",
      "Epoch 18/4000: Train Loss: 2849.5142981845397 Test Loss: 3014.805814183533\n",
      "Epoch 19/4000: Train Loss: 2849.4999983312637 Test Loss: 3014.7870107697413\n",
      "Epoch 20/4000: Train Loss: 2849.485713564312 Test Loss: 3014.768223926428\n",
      "Epoch 21/4000: Train Loss: 2849.4714438673122 Test Loss: 3014.7494536497834\n",
      "Epoch 22/4000: Train Loss: 2849.457189223917 Test Loss: 3014.730699935942\n",
      "Epoch 23/4000: Train Loss: 2849.4429496177986 Test Loss: 3014.7119627809793\n",
      "Epoch 24/4000: Train Loss: 2849.4287250326543 Test Loss: 3014.693242180907\n",
      "Epoch 25/4000: Train Loss: 2849.4145154522025 Test Loss: 3014.67453813168\n",
      "Epoch 26/4000: Train Loss: 2849.400320860185 Test Loss: 3014.6558506291954\n",
      "Epoch 27/4000: Train Loss: 2849.3861412403644 Test Loss: 3014.6371796692897\n",
      "Epoch 28/4000: Train Loss: 2849.3719765765254 Test Loss: 3014.6185252477403\n",
      "Epoch 29/4000: Train Loss: 2849.357826852476 Test Loss: 3014.599887360266\n",
      "Epoch 30/4000: Train Loss: 2849.3436920520458 Test Loss: 3014.581266002528\n",
      "Epoch 31/4000: Train Loss: 2849.3295721590866 Test Loss: 3014.5626611701246\n",
      "Epoch 32/4000: Train Loss: 2849.3154671574703 Test Loss: 3014.544072858599\n",
      "Epoch 33/4000: Train Loss: 2849.3013770310927 Test Loss: 3014.525501063439\n",
      "Epoch 34/4000: Train Loss: 2849.28730176387 Test Loss: 3014.506945780065\n",
      "Epoch 35/4000: Train Loss: 2849.2732413397403 Test Loss: 3014.48840700385\n",
      "Epoch 36/4000: Train Loss: 2849.2591957426644 Test Loss: 3014.4698847300997\n",
      "Epoch 37/4000: Train Loss: 2849.2451649566224 Test Loss: 3014.4513789540674\n",
      "Epoch 38/4000: Train Loss: 2849.231148965617 Test Loss: 3014.432889670948\n",
      "Epoch 39/4000: Train Loss: 2849.217147753673 Test Loss: 3014.4144168758785\n",
      "Epoch 40/4000: Train Loss: 2849.2031613048357 Test Loss: 3014.3959605639398\n",
      "Epoch 41/4000: Train Loss: 2849.18918960317 Test Loss: 3014.377520730154\n",
      "Epoch 42/4000: Train Loss: 2849.1752326327646 Test Loss: 3014.359097369487\n",
      "Epoch 43/4000: Train Loss: 2849.161290377728 Test Loss: 3014.340690476848\n",
      "Epoch 44/4000: Train Loss: 2849.1473628221893 Test Loss: 3014.322300047093\n",
      "Epoch 45/4000: Train Loss: 2849.1334499502996 Test Loss: 3014.3039260750174\n",
      "Epoch 46/4000: Train Loss: 2849.1195517462293 Test Loss: 3014.285568555364\n",
      "Epoch 47/4000: Train Loss: 2849.1056681941714 Test Loss: 3014.2672274828183\n",
      "Epoch 48/4000: Train Loss: 2849.0917992783375 Test Loss: 3014.2489028520126\n",
      "Epoch 49/4000: Train Loss: 2849.077944982963 Test Loss: 3014.23059465752\n",
      "Epoch 50/4000: Train Loss: 2849.0641052923 Test Loss: 3014.2123028938627\n",
      "Epoch 51/4000: Train Loss: 2849.0502801906237 Test Loss: 3014.194027555508\n",
      "Epoch 52/4000: Train Loss: 2849.036469662229 Test Loss: 3014.1757686368696\n",
      "Epoch 53/4000: Train Loss: 2849.0226736914315 Test Loss: 3014.157526132303\n",
      "Epoch 54/4000: Train Loss: 2849.0088922625673 Test Loss: 3014.1393000361145\n",
      "Epoch 55/4000: Train Loss: 2848.995125359991 Test Loss: 3014.121090342555\n",
      "Epoch 56/4000: Train Loss: 2848.98137296808 Test Loss: 3014.102897045821\n",
      "Epoch 57/4000: Train Loss: 2848.96763507123 Test Loss: 3014.08472014006\n",
      "Epoch 58/4000: Train Loss: 2848.9539116538576 Test Loss: 3014.066559619365\n",
      "Epoch 59/4000: Train Loss: 2848.940202700399 Test Loss: 3014.0484154777737\n",
      "Epoch 60/4000: Train Loss: 2848.926508195311 Test Loss: 3014.0302877092777\n",
      "Epoch 61/4000: Train Loss: 2848.9128281230687 Test Loss: 3014.012176307811\n",
      "Epoch 62/4000: Train Loss: 2848.89916246817 Test Loss: 3013.994081267259\n",
      "Epoch 63/4000: Train Loss: 2848.8855112151296 Test Loss: 3013.9760025814594\n",
      "Epoch 64/4000: Train Loss: 2848.8718743484833 Test Loss: 3013.957940244192\n",
      "Epoch 65/4000: Train Loss: 2848.858251852786 Test Loss: 3013.9398942491903\n",
      "Epoch 66/4000: Train Loss: 2848.844643712613 Test Loss: 3013.9218645901383\n",
      "Epoch 67/4000: Train Loss: 2848.8310499125596 Test Loss: 3013.9038512606694\n",
      "Epoch 68/4000: Train Loss: 2848.8174704372377 Test Loss: 3013.885854254364\n",
      "Epoch 69/4000: Train Loss: 2848.803905271283 Test Loss: 3013.8678735647572\n",
      "Epoch 70/4000: Train Loss: 2848.7903543993452 Test Loss: 3013.8499091853346\n",
      "Epoch 71/4000: Train Loss: 2848.7768178061 Test Loss: 3013.831961109532\n",
      "Epoch 72/4000: Train Loss: 2848.763295476236 Test Loss: 3013.814029330737\n",
      "Epoch 73/4000: Train Loss: 2848.749787394466 Test Loss: 3013.796113842289\n",
      "Epoch 74/4000: Train Loss: 2848.7362935455167 Test Loss: 3013.7782146374807\n",
      "Epoch 75/4000: Train Loss: 2848.7228139141403 Test Loss: 3013.7603317095563\n",
      "Epoch 76/4000: Train Loss: 2848.7093484851025 Test Loss: 3013.742465051712\n",
      "Epoch 77/4000: Train Loss: 2848.695897243191 Test Loss: 3013.724614657098\n",
      "Epoch 78/4000: Train Loss: 2848.6824601732123 Test Loss: 3013.7067805188203\n",
      "Epoch 79/4000: Train Loss: 2848.66903725999 Test Loss: 3013.6889626299353\n",
      "Epoch 80/4000: Train Loss: 2848.6556284883686 Test Loss: 3013.6711609834533\n",
      "Epoch 81/4000: Train Loss: 2848.64223384321 Test Loss: 3013.653375572342\n",
      "Epoch 82/4000: Train Loss: 2848.6288533093957 Test Loss: 3013.6356063895223\n",
      "Epoch 83/4000: Train Loss: 2848.615486871826 Test Loss: 3013.617853427868\n",
      "Epoch 84/4000: Train Loss: 2848.602134515419 Test Loss: 3013.600116680211\n",
      "Epoch 85/4000: Train Loss: 2848.588796225112 Test Loss: 3013.582396139338\n",
      "Epoch 86/4000: Train Loss: 2848.575471985861 Test Loss: 3013.5646917979902\n",
      "Epoch 87/4000: Train Loss: 2848.5621617826396 Test Loss: 3013.547003648866\n",
      "Epoch 88/4000: Train Loss: 2848.5488656004413 Test Loss: 3013.529331684622\n",
      "Epoch 89/4000: Train Loss: 2848.5355834242764 Test Loss: 3013.5116758978693\n",
      "Epoch 90/4000: Train Loss: 2848.5223152391745 Test Loss: 3013.4940362811744\n",
      "Epoch 91/4000: Train Loss: 2848.5090610301845 Test Loss: 3013.4764128270663\n",
      "Epoch 92/4000: Train Loss: 2848.495820782371 Test Loss: 3013.4588055280283\n",
      "Epoch 93/4000: Train Loss: 2848.4825944808194 Test Loss: 3013.441214376501\n",
      "Epoch 94/4000: Train Loss: 2848.4693821106316 Test Loss: 3013.423639364886\n",
      "Epoch 95/4000: Train Loss: 2848.4561836569287 Test Loss: 3013.4060804855408\n",
      "Epoch 96/4000: Train Loss: 2848.4429991048487 Test Loss: 3013.3885377307834\n",
      "Epoch 97/4000: Train Loss: 2848.4298284395486 Test Loss: 3013.3710110928905\n",
      "Epoch 98/4000: Train Loss: 2848.4166716462037 Test Loss: 3013.3535005640997\n",
      "Epoch 99/4000: Train Loss: 2848.4035287100055 Test Loss: 3013.336006136605\n",
      "Epoch 100/4000: Train Loss: 2848.3903996161657 Test Loss: 3013.318527802561\n",
      "Epoch 101/4000: Train Loss: 2848.3772843499123 Test Loss: 3013.301065554088\n",
      "Epoch 102/4000: Train Loss: 2848.3641828964915 Test Loss: 3013.28361938326\n",
      "Epoch 103/4000: Train Loss: 2848.351095241167 Test Loss: 3013.2661892821156\n",
      "Epoch 104/4000: Train Loss: 2848.3380213692217 Test Loss: 3013.2487752426537\n",
      "Epoch 105/4000: Train Loss: 2848.324961265954 Test Loss: 3013.2313772568355\n",
      "Epoch 106/4000: Train Loss: 2848.311914916682 Test Loss: 3013.2139953165824\n",
      "Epoch 107/4000: Train Loss: 2848.298882306739 Test Loss: 3013.1966294137787\n",
      "Epoch 108/4000: Train Loss: 2848.2858634214786 Test Loss: 3013.1792795402725\n",
      "Epoch 109/4000: Train Loss: 2848.2728582462705 Test Loss: 3013.161945687871\n",
      "Epoch 110/4000: Train Loss: 2848.2598667665006 Test Loss: 3013.1446278483495\n",
      "Epoch 111/4000: Train Loss: 2848.2468889675756 Test Loss: 3013.1273260134394\n",
      "Epoch 112/4000: Train Loss: 2848.2339248349167 Test Loss: 3013.1100401748427\n",
      "Epoch 113/4000: Train Loss: 2848.220974353963 Test Loss: 3013.092770324221\n",
      "Epoch 114/4000: Train Loss: 2848.2080375101723 Test Loss: 3013.0755164532006\n",
      "Epoch 115/4000: Train Loss: 2848.1951142890184 Test Loss: 3013.0582785533716\n",
      "Epoch 116/4000: Train Loss: 2848.182204675992 Test Loss: 3013.0410566162923\n",
      "Epoch 117/4000: Train Loss: 2848.169308656603 Test Loss: 3013.0238506334813\n",
      "Epoch 118/4000: Train Loss: 2848.1564262163756 Test Loss: 3013.0066605964244\n",
      "Epoch 119/4000: Train Loss: 2848.1435573408535 Test Loss: 3012.9894864965695\n",
      "Epoch 120/4000: Train Loss: 2848.1307020155964 Test Loss: 3012.972328325337\n",
      "Epoch 121/4000: Train Loss: 2848.117860226182 Test Loss: 3012.955186074108\n",
      "Epoch 122/4000: Train Loss: 2848.105031958203 Test Loss: 3012.9380597342315\n",
      "Epoch 123/4000: Train Loss: 2848.092217197271 Test Loss: 3012.9209492970217\n",
      "Epoch 124/4000: Train Loss: 2848.079415929015 Test Loss: 3012.9038547537602\n",
      "Epoch 125/4000: Train Loss: 2848.0666281390777 Test Loss: 3012.886776095696\n",
      "Epoch 126/4000: Train Loss: 2848.053853813122 Test Loss: 3012.869713314045\n",
      "Epoch 127/4000: Train Loss: 2848.0410929368268 Test Loss: 3012.852666399992\n",
      "Epoch 128/4000: Train Loss: 2848.0283454958862 Test Loss: 3012.8356353446848\n",
      "Epoch 129/4000: Train Loss: 2848.0156114760134 Test Loss: 3012.8186201392446\n",
      "Epoch 130/4000: Train Loss: 2848.0028908629365 Test Loss: 3012.801620774759\n",
      "Epoch 131/4000: Train Loss: 2847.9901836424015 Test Loss: 3012.7846372422855\n",
      "Epoch 132/4000: Train Loss: 2847.97748980017 Test Loss: 3012.7676695328455\n",
      "Epoch 133/4000: Train Loss: 2847.9648093220208 Test Loss: 3012.750717637436\n",
      "Epoch 134/4000: Train Loss: 2847.952142193748 Test Loss: 3012.7337815470155\n",
      "Epoch 135/4000: Train Loss: 2847.939488401166 Test Loss: 3012.71686125252\n",
      "Epoch 136/4000: Train Loss: 2847.9268479301013 Test Loss: 3012.6999567448515\n",
      "Epoch 137/4000: Train Loss: 2847.9142207663995 Test Loss: 3012.683068014883\n",
      "Epoch 138/4000: Train Loss: 2847.9016068959213 Test Loss: 3012.6661950534544\n",
      "Epoch 139/4000: Train Loss: 2847.889006304544 Test Loss: 3012.649337851381\n",
      "Epoch 140/4000: Train Loss: 2847.8764189781627 Test Loss: 3012.6324963994452\n",
      "Epoch 141/4000: Train Loss: 2847.863844902687 Test Loss: 3012.6156706884035\n",
      "Epoch 142/4000: Train Loss: 2847.8512840640437 Test Loss: 3012.5988607089803\n",
      "Epoch 143/4000: Train Loss: 2847.838736448176 Test Loss: 3012.582066451874\n",
      "Epoch 144/4000: Train Loss: 2847.8262020410425 Test Loss: 3012.565287907754\n",
      "Epoch 145/4000: Train Loss: 2847.8136808286195 Test Loss: 3012.54852506726\n",
      "Epoch 146/4000: Train Loss: 2847.801172796897 Test Loss: 3012.5317779210072\n",
      "Epoch 147/4000: Train Loss: 2847.788677931884 Test Loss: 3012.5150464595818\n",
      "Epoch 148/4000: Train Loss: 2847.776196219604 Test Loss: 3012.498330673538\n",
      "Epoch 149/4000: Train Loss: 2847.763727646097 Test Loss: 3012.4816305534114\n",
      "Epoch 150/4000: Train Loss: 2847.751272197418 Test Loss: 3012.464946089705\n",
      "Epoch 151/4000: Train Loss: 2847.73882985964 Test Loss: 3012.4482772728943\n",
      "Epoch 152/4000: Train Loss: 2847.7264006188507 Test Loss: 3012.431624093435\n",
      "Epoch 153/4000: Train Loss: 2847.7139844611543 Test Loss: 3012.414986541749\n",
      "Epoch 154/4000: Train Loss: 2847.70158137267 Test Loss: 3012.398364608235\n",
      "Epoch 155/4000: Train Loss: 2847.6891913395325 Test Loss: 3012.3817582832685\n",
      "Epoch 156/4000: Train Loss: 2847.6768143478953 Test Loss: 3012.365167557194\n",
      "Epoch 157/4000: Train Loss: 2847.6644503839243 Test Loss: 3012.3485924203374\n",
      "Epoch 158/4000: Train Loss: 2847.6520994338034 Test Loss: 3012.3320328629934\n",
      "Epoch 159/4000: Train Loss: 2847.6397614837315 Test Loss: 3012.3154888754366\n",
      "Epoch 160/4000: Train Loss: 2847.6274365199233 Test Loss: 3012.2989604479117\n",
      "Epoch 161/4000: Train Loss: 2847.6151245286087 Test Loss: 3012.282447570645\n",
      "Epoch 162/4000: Train Loss: 2847.6028254960347 Test Loss: 3012.2659502338333\n",
      "Epoch 163/4000: Train Loss: 2847.5905394084616 Test Loss: 3012.2494684276526\n",
      "Epoch 164/4000: Train Loss: 2847.578266252168 Test Loss: 3012.233002142255\n",
      "Epoch 165/4000: Train Loss: 2847.5660060134455 Test Loss: 3012.2165513677664\n",
      "Epoch 166/4000: Train Loss: 2847.553758678604 Test Loss: 3012.200116094291\n",
      "Epoch 167/4000: Train Loss: 2847.5415242339673 Test Loss: 3012.183696311911\n",
      "Epoch 168/4000: Train Loss: 2847.529302665874 Test Loss: 3012.167292010684\n",
      "Epoch 169/4000: Train Loss: 2847.5170939606796 Test Loss: 3012.150903180645\n",
      "Epoch 170/4000: Train Loss: 2847.5048981047553 Test Loss: 3012.1345298118063\n",
      "Epoch 171/4000: Train Loss: 2847.492715084486 Test Loss: 3012.1181718941584\n",
      "Epoch 172/4000: Train Loss: 2847.480544886273 Test Loss: 3012.10182941767\n",
      "Epoch 173/4000: Train Loss: 2847.468387496534 Test Loss: 3012.085502372287\n",
      "Epoch 174/4000: Train Loss: 2847.456242901699 Test Loss: 3012.069190747934\n",
      "Epoch 175/4000: Train Loss: 2847.4441110882176 Test Loss: 3012.0528945345145\n",
      "Epoch 176/4000: Train Loss: 2847.4319920425505 Test Loss: 3012.0366137219066\n",
      "Epoch 177/4000: Train Loss: 2847.4198857511774 Test Loss: 3012.020348299975\n",
      "Epoch 178/4000: Train Loss: 2847.407792200589 Test Loss: 3012.004098258558\n",
      "Epoch 179/4000: Train Loss: 2847.3957113772954 Test Loss: 3011.987863587473\n",
      "Epoch 180/4000: Train Loss: 2847.38364326782 Test Loss: 3011.971644276519\n",
      "Epoch 181/4000: Train Loss: 2847.3715878587 Test Loss: 3011.9554403154725\n",
      "Epoch 182/4000: Train Loss: 2847.35954513649 Test Loss: 3011.939251694092\n",
      "Epoch 183/4000: Train Loss: 2847.347515087759 Test Loss: 3011.923078402114\n",
      "Epoch 184/4000: Train Loss: 2847.335497699091 Test Loss: 3011.9069204292587\n",
      "Epoch 185/4000: Train Loss: 2847.3234929570845 Test Loss: 3011.89077776522\n",
      "Epoch 186/4000: Train Loss: 2847.311500848353 Test Loss: 3011.8746503996795\n",
      "Epoch 187/4000: Train Loss: 2847.2995213595264 Test Loss: 3011.858538322297\n",
      "Epoch 188/4000: Train Loss: 2847.2875544772482 Test Loss: 3011.8424415227087\n",
      "Epoch 189/4000: Train Loss: 2847.2756001881776 Test Loss: 3011.8263599905404\n",
      "Epoch 190/4000: Train Loss: 2847.263658478987 Test Loss: 3011.810293715393\n",
      "Epoch 191/4000: Train Loss: 2847.2517293363676 Test Loss: 3011.794242686851\n",
      "Epoch 192/4000: Train Loss: 2847.2398127470206 Test Loss: 3011.7782068944803\n",
      "Epoch 193/4000: Train Loss: 2847.2279086976646 Test Loss: 3011.7621863278273\n",
      "Epoch 194/4000: Train Loss: 2847.216017175034 Test Loss: 3011.7461809764227\n",
      "Epoch 195/4000: Train Loss: 2847.2041381658764 Test Loss: 3011.7301908297786\n",
      "Epoch 196/4000: Train Loss: 2847.1922716569543 Test Loss: 3011.714215877387\n",
      "Epoch 197/4000: Train Loss: 2847.1804176350447 Test Loss: 3011.698256108728\n",
      "Epoch 198/4000: Train Loss: 2847.168576086941 Test Loss: 3011.682311513259\n",
      "Epoch 199/4000: Train Loss: 2847.1567469994493 Test Loss: 3011.666382080423\n",
      "Epoch 200/4000: Train Loss: 2847.1449303593913 Test Loss: 3011.650467799645\n",
      "Epoch 201/4000: Train Loss: 2847.133126153603 Test Loss: 3011.634568660334\n",
      "Epoch 202/4000: Train Loss: 2847.1213343689355 Test Loss: 3011.6186846518813\n",
      "Epoch 203/4000: Train Loss: 2847.109554992255 Test Loss: 3011.6028157636633\n",
      "Epoch 204/4000: Train Loss: 2847.09778801044 Test Loss: 3011.5869619850396\n",
      "Epoch 205/4000: Train Loss: 2847.086033410387 Test Loss: 3011.5711233053526\n",
      "Epoch 206/4000: Train Loss: 2847.074291179003 Test Loss: 3011.5552997139303\n",
      "Epoch 207/4000: Train Loss: 2847.0625613032125 Test Loss: 3011.5394912000847\n",
      "Epoch 208/4000: Train Loss: 2847.0508437699546 Test Loss: 3011.5236977531117\n",
      "Epoch 209/4000: Train Loss: 2847.0391385661806 Test Loss: 3011.5079193622905\n",
      "Epoch 210/4000: Train Loss: 2847.0274456788575 Test Loss: 3011.4921560168873\n",
      "Epoch 211/4000: Train Loss: 2847.0157650949673 Test Loss: 3011.476407706153\n",
      "Epoch 212/4000: Train Loss: 2847.004096801506 Test Loss: 3011.4606744193215\n",
      "Epoch 213/4000: Train Loss: 2846.9924407854833 Test Loss: 3011.4449561456154\n",
      "Epoch 214/4000: Train Loss: 2846.980797033924 Test Loss: 3011.42925287424\n",
      "Epoch 215/4000: Train Loss: 2846.969165533866 Test Loss: 3011.413564594383\n",
      "Epoch 216/4000: Train Loss: 2846.957546272364 Test Loss: 3011.3978912952266\n",
      "Epoch 217/4000: Train Loss: 2846.9459392364843 Test Loss: 3011.382232965929\n",
      "Epoch 218/4000: Train Loss: 2846.9343444133087 Test Loss: 3011.366589595642\n",
      "Epoch 219/4000: Train Loss: 2846.9227617899337 Test Loss: 3011.3509611735008\n",
      "Epoch 220/4000: Train Loss: 2846.911191353469 Test Loss: 3011.335347688626\n",
      "Epoch 221/4000: Train Loss: 2846.899633091039 Test Loss: 3011.319749130125\n",
      "Epoch 222/4000: Train Loss: 2846.8880869897816 Test Loss: 3011.3041654870926\n",
      "Epoch 223/4000: Train Loss: 2846.8765530368505 Test Loss: 3011.28859674861\n",
      "Epoch 224/4000: Train Loss: 2846.8650312194122 Test Loss: 3011.2730429037447\n",
      "Epoch 225/4000: Train Loss: 2846.853521524647 Test Loss: 3011.2575039415524\n",
      "Epoch 226/4000: Train Loss: 2846.8420239397506 Test Loss: 3011.2419798510764\n",
      "Epoch 227/4000: Train Loss: 2846.8305384519317 Test Loss: 3011.226470621343\n",
      "Epoch 228/4000: Train Loss: 2846.819065048413 Test Loss: 3011.210976241373\n",
      "Epoch 229/4000: Train Loss: 2846.8076037164315 Test Loss: 3011.195496700168\n",
      "Epoch 230/4000: Train Loss: 2846.796154443239 Test Loss: 3011.1800319867234\n",
      "Epoch 231/4000: Train Loss: 2846.7847172160996 Test Loss: 3011.164582090018\n",
      "Epoch 232/4000: Train Loss: 2846.7732920222943 Test Loss: 3011.1491469990196\n",
      "Epoch 233/4000: Train Loss: 2846.7618788491136 Test Loss: 3011.1337267026875\n",
      "Epoch 234/4000: Train Loss: 2846.750477683866 Test Loss: 3011.1183211899647\n",
      "Epoch 235/4000: Train Loss: 2846.739088513871 Test Loss: 3011.1029304497856\n",
      "Epoch 236/4000: Train Loss: 2846.727711326465 Test Loss: 3011.0875544710707\n",
      "Epoch 237/4000: Train Loss: 2846.716346108995 Test Loss: 3011.0721932427323\n",
      "Epoch 238/4000: Train Loss: 2846.704992848823 Test Loss: 3011.0568467536723\n",
      "Epoch 239/4000: Train Loss: 2846.6936515333264 Test Loss: 3011.0415149927767\n",
      "Epoch 240/4000: Train Loss: 2846.6823221498953 Test Loss: 3011.0261979489233\n",
      "Epoch 241/4000: Train Loss: 2846.671004685932 Test Loss: 3011.010895610982\n",
      "Epoch 242/4000: Train Loss: 2846.6596991288548 Test Loss: 3010.9956079678063\n",
      "Epoch 243/4000: Train Loss: 2846.648405466095 Test Loss: 3010.9803350082466\n",
      "Epoch 244/4000: Train Loss: 2846.6371236850982 Test Loss: 3010.9650767211356\n",
      "Epoch 245/4000: Train Loss: 2846.6258537733206 Test Loss: 3010.9498330952997\n",
      "Epoch 246/4000: Train Loss: 2846.614595718237 Test Loss: 3010.9346041195568\n",
      "Epoch 247/4000: Train Loss: 2846.6033495073325 Test Loss: 3010.91938978271\n",
      "Epoch 248/4000: Train Loss: 2846.5921151281063 Test Loss: 3010.904190073558\n",
      "Epoch 249/4000: Train Loss: 2846.5808925680717 Test Loss: 3010.8890049808856\n",
      "Epoch 250/4000: Train Loss: 2846.5696818147558 Test Loss: 3010.873834493471\n",
      "Epoch 251/4000: Train Loss: 2846.558482855699 Test Loss: 3010.858678600082\n",
      "Epoch 252/4000: Train Loss: 2846.5472956784547 Test Loss: 3010.843537289475\n",
      "Epoch 253/4000: Train Loss: 2846.536120270591 Test Loss: 3010.8284105504013\n",
      "Epoch 254/4000: Train Loss: 2846.5249566196885 Test Loss: 3010.813298371601\n",
      "Epoch 255/4000: Train Loss: 2846.5138047133414 Test Loss: 3010.7982007418036\n",
      "Epoch 256/4000: Train Loss: 2846.502664539159 Test Loss: 3010.783117649734\n",
      "Epoch 257/4000: Train Loss: 2846.491536084761 Test Loss: 3010.7680490841035\n",
      "Epoch 258/4000: Train Loss: 2846.480419337783 Test Loss: 3010.7529950336207\n",
      "Epoch 259/4000: Train Loss: 2846.469314285874 Test Loss: 3010.7379554869785\n",
      "Epoch 260/4000: Train Loss: 2846.458220916695 Test Loss: 3010.7229304328694\n",
      "Epoch 261/4000: Train Loss: 2846.447139217921 Test Loss: 3010.707919859973\n",
      "Epoch 262/4000: Train Loss: 2846.436069177241 Test Loss: 3010.6929237569598\n",
      "Epoch 263/4000: Train Loss: 2846.4250107823564 Test Loss: 3010.677942112496\n",
      "Epoch 264/4000: Train Loss: 2846.4139640209823 Test Loss: 3010.662974915238\n",
      "Epoch 265/4000: Train Loss: 2846.402928880848 Test Loss: 3010.648022153835\n",
      "Epoch 266/4000: Train Loss: 2846.3919053496934 Test Loss: 3010.633083816928\n",
      "Epoch 267/4000: Train Loss: 2846.3808934152758 Test Loss: 3010.618159893151\n",
      "Epoch 268/4000: Train Loss: 2846.3698930653627 Test Loss: 3010.6032503711313\n",
      "Epoch 269/4000: Train Loss: 2846.3589042877347 Test Loss: 3010.5883552394876\n",
      "Epoch 270/4000: Train Loss: 2846.347927070188 Test Loss: 3010.573474486833\n",
      "Epoch 271/4000: Train Loss: 2846.336961400529 Test Loss: 3010.5586081017705\n",
      "Epoch 272/4000: Train Loss: 2846.3260072665807 Test Loss: 3010.543756072901\n",
      "Epoch 273/4000: Train Loss: 2846.3150646561767 Test Loss: 3010.528918388816\n",
      "Epoch 274/4000: Train Loss: 2846.3041335571643 Test Loss: 3010.5140950380974\n",
      "Epoch 275/4000: Train Loss: 2846.2932139574036 Test Loss: 3010.4992860093275\n",
      "Epoch 276/4000: Train Loss: 2846.28230584477 Test Loss: 3010.4844912910758\n",
      "Epoch 277/4000: Train Loss: 2846.2714092071483 Test Loss: 3010.469710871907\n",
      "Epoch 278/4000: Train Loss: 2846.2605240324406 Test Loss: 3010.4549447403824\n",
      "Epoch 279/4000: Train Loss: 2846.2496503085586 Test Loss: 3010.440192885054\n",
      "Epoch 280/4000: Train Loss: 2846.2387880234273 Test Loss: 3010.4254552944685\n",
      "Epoch 281/4000: Train Loss: 2846.227937164988 Test Loss: 3010.4107319571667\n",
      "Epoch 282/4000: Train Loss: 2846.217097721192 Test Loss: 3010.396022861686\n",
      "Epoch 283/4000: Train Loss: 2846.206269680004 Test Loss: 3010.3813279965543\n",
      "Epoch 284/4000: Train Loss: 2846.1954530294024 Test Loss: 3010.366647350296\n",
      "Epoch 285/4000: Train Loss: 2846.1846477573786 Test Loss: 3010.351980911428\n",
      "Epoch 286/4000: Train Loss: 2846.1738538519357 Test Loss: 3010.3373286684655\n",
      "Epoch 287/4000: Train Loss: 2846.1630713010913 Test Loss: 3010.322690609915\n",
      "Epoch 288/4000: Train Loss: 2846.1523000928746 Test Loss: 3010.30806672428\n",
      "Epoch 289/4000: Train Loss: 2846.1415402153293 Test Loss: 3010.293457000057\n",
      "Epoch 290/4000: Train Loss: 2846.1307916565106 Test Loss: 3010.278861425738\n",
      "Epoch 291/4000: Train Loss: 2846.1200544044864 Test Loss: 3010.264279989811\n",
      "Epoch 292/4000: Train Loss: 2846.109328447338 Test Loss: 3010.2497126807584\n",
      "Epoch 293/4000: Train Loss: 2846.098613773161 Test Loss: 3010.2351594870584\n",
      "Epoch 294/4000: Train Loss: 2846.087910370061 Test Loss: 3010.2206203971846\n",
      "Epoch 295/4000: Train Loss: 2846.077218226157 Test Loss: 3010.206095399605\n",
      "Epoch 296/4000: Train Loss: 2846.0665373295838 Test Loss: 3010.1915844827854\n",
      "Epoch 297/4000: Train Loss: 2846.055867668485 Test Loss: 3010.177087635183\n",
      "Epoch 298/4000: Train Loss: 2846.045209231019 Test Loss: 3010.1626048452554\n",
      "Epoch 299/4000: Train Loss: 2846.0345620053567 Test Loss: 3010.1481361014535\n",
      "Epoch 300/4000: Train Loss: 2846.0239259796813 Test Loss: 3010.1336813922276\n",
      "Epoch 301/4000: Train Loss: 2846.0133011421894 Test Loss: 3010.119240706017\n",
      "Epoch 302/4000: Train Loss: 2846.0026874810897 Test Loss: 3010.104814031265\n",
      "Epoch 303/4000: Train Loss: 2845.9920849846035 Test Loss: 3010.090401356405\n",
      "Epoch 304/4000: Train Loss: 2845.981493640965 Test Loss: 3010.076002669871\n",
      "Epoch 305/4000: Train Loss: 2845.970913438421 Test Loss: 3010.0616179600897\n",
      "Epoch 306/4000: Train Loss: 2845.960344365231 Test Loss: 3010.047247215489\n",
      "Epoch 307/4000: Train Loss: 2845.9497864096666 Test Loss: 3010.03289042449\n",
      "Epoch 308/4000: Train Loss: 2845.939239560013 Test Loss: 3010.0185475755097\n",
      "Epoch 309/4000: Train Loss: 2845.9287038045672 Test Loss: 3010.0042186569644\n",
      "Epoch 310/4000: Train Loss: 2845.918179131638 Test Loss: 3009.9899036572656\n",
      "Epoch 311/4000: Train Loss: 2845.90766552955 Test Loss: 3009.975602564824\n",
      "Epoch 312/4000: Train Loss: 2845.8971629866346 Test Loss: 3009.9613153680425\n",
      "Epoch 313/4000: Train Loss: 2845.8866714912415 Test Loss: 3009.9470420553266\n",
      "Epoch 314/4000: Train Loss: 2845.8761910317303 Test Loss: 3009.932782615077\n",
      "Epoch 315/4000: Train Loss: 2845.8657215964718 Test Loss: 3009.91853703569\n",
      "Epoch 316/4000: Train Loss: 2845.855263173852 Test Loss: 3009.9043053055607\n",
      "Epoch 317/4000: Train Loss: 2845.844815752268 Test Loss: 3009.890087413083\n",
      "Epoch 318/4000: Train Loss: 2845.8343793201284 Test Loss: 3009.875883346645\n",
      "Epoch 319/4000: Train Loss: 2845.823953865856 Test Loss: 3009.8616930946346\n",
      "Epoch 320/4000: Train Loss: 2845.813539377885 Test Loss: 3009.8475166454377\n",
      "Epoch 321/4000: Train Loss: 2845.8031358446606 Test Loss: 3009.8333539874384\n",
      "Epoch 322/4000: Train Loss: 2845.792743254645 Test Loss: 3009.819205109017\n",
      "Epoch 323/4000: Train Loss: 2845.782361596307 Test Loss: 3009.805069998551\n",
      "Epoch 324/4000: Train Loss: 2845.7719908581316 Test Loss: 3009.79094864442\n",
      "Epoch 325/4000: Train Loss: 2845.761631028615 Test Loss: 3009.776841034996\n",
      "Epoch 326/4000: Train Loss: 2845.7512820962643 Test Loss: 3009.762747158654\n",
      "Epoch 327/4000: Train Loss: 2845.7409440496017 Test Loss: 3009.7486670037656\n",
      "Epoch 328/4000: Train Loss: 2845.73061687716 Test Loss: 3009.7346005587\n",
      "Epoch 329/4000: Train Loss: 2845.7203005674837 Test Loss: 3009.720547811826\n",
      "Epoch 330/4000: Train Loss: 2845.709995109131 Test Loss: 3009.706508751509\n",
      "Epoch 331/4000: Train Loss: 2845.6997004906707 Test Loss: 3009.6924833661155\n",
      "Epoch 332/4000: Train Loss: 2845.689416700685 Test Loss: 3009.6784716440106\n",
      "Epoch 333/4000: Train Loss: 2845.679143727769 Test Loss: 3009.6644735735563\n",
      "Epoch 334/4000: Train Loss: 2845.6688815605276 Test Loss: 3009.6504891431105\n",
      "Epoch 335/4000: Train Loss: 2845.6586301875805 Test Loss: 3009.6365183410376\n",
      "Epoch 336/4000: Train Loss: 2845.648389597557 Test Loss: 3009.6225611556965\n",
      "Epoch 337/4000: Train Loss: 2845.638159779101 Test Loss: 3009.6086175754454\n",
      "Epoch 338/4000: Train Loss: 2845.627940720867 Test Loss: 3009.5946875886398\n",
      "Epoch 339/4000: Train Loss: 2845.617732411522 Test Loss: 3009.5807711836387\n",
      "Epoch 340/4000: Train Loss: 2845.607534839745 Test Loss: 3009.566868348797\n",
      "Epoch 341/4000: Train Loss: 2845.5973479942268 Test Loss: 3009.5529790724695\n",
      "Epoch 342/4000: Train Loss: 2845.5871718636718 Test Loss: 3009.5391033430105\n",
      "Epoch 343/4000: Train Loss: 2845.577006436794 Test Loss: 3009.5252411487772\n",
      "Epoch 344/4000: Train Loss: 2845.566851702322 Test Loss: 3009.511392478119\n",
      "Epoch 345/4000: Train Loss: 2845.556707648993 Test Loss: 3009.49755731939\n",
      "Epoch 346/4000: Train Loss: 2845.546574265561 Test Loss: 3009.483735660944\n",
      "Epoch 347/4000: Train Loss: 2845.536451540788 Test Loss: 3009.469927491133\n",
      "Epoch 348/4000: Train Loss: 2845.5263394634494 Test Loss: 3009.4561327983106\n",
      "Epoch 349/4000: Train Loss: 2845.5162380223323 Test Loss: 3009.4423515708277\n",
      "Epoch 350/4000: Train Loss: 2845.506147206236 Test Loss: 3009.428583797034\n",
      "Epoch 351/4000: Train Loss: 2845.4960670039723 Test Loss: 3009.4148294652864\n",
      "Epoch 352/4000: Train Loss: 2845.4859974043648 Test Loss: 3009.401088563935\n",
      "Epoch 353/4000: Train Loss: 2845.4759383962464 Test Loss: 3009.387361081333\n",
      "Epoch 354/4000: Train Loss: 2845.4658899684655 Test Loss: 3009.3736470058307\n",
      "Epoch 355/4000: Train Loss: 2845.4558521098807 Test Loss: 3009.359946325784\n",
      "Epoch 356/4000: Train Loss: 2845.4458248093624 Test Loss: 3009.346259029544\n",
      "Epoch 357/4000: Train Loss: 2845.4358080557936 Test Loss: 3009.3325851054624\n",
      "Epoch 358/4000: Train Loss: 2845.4258018380683 Test Loss: 3009.318924541899\n",
      "Epoch 359/4000: Train Loss: 2845.415806145093 Test Loss: 3009.3052773272043\n",
      "Epoch 360/4000: Train Loss: 2845.4058209657846 Test Loss: 3009.291643449733\n",
      "Epoch 361/4000: Train Loss: 2845.395846289074 Test Loss: 3009.2780228978436\n",
      "Epoch 362/4000: Train Loss: 2845.385882103902 Test Loss: 3009.264415659891\n",
      "Epoch 363/4000: Train Loss: 2845.375928399223 Test Loss: 3009.2508217242316\n",
      "Epoch 364/4000: Train Loss: 2845.365985164001 Test Loss: 3009.2372410792273\n",
      "Epoch 365/4000: Train Loss: 2845.3560523872134 Test Loss: 3009.2236737132334\n",
      "Epoch 366/4000: Train Loss: 2845.3461300578488 Test Loss: 3009.210119614611\n",
      "Epoch 367/4000: Train Loss: 2845.3362181649068 Test Loss: 3009.196578771722\n",
      "Epoch 368/4000: Train Loss: 2845.3263166973998 Test Loss: 3009.1830511729286\n",
      "Epoch 369/4000: Train Loss: 2845.3164256443515 Test Loss: 3009.1695368065944\n",
      "Epoch 370/4000: Train Loss: 2845.306544994797 Test Loss: 3009.156035661082\n",
      "Epoch 371/4000: Train Loss: 2845.2966747377836 Test Loss: 3009.1425477247603\n",
      "Epoch 372/4000: Train Loss: 2845.2868148623693 Test Loss: 3009.1290729859925\n",
      "Epoch 373/4000: Train Loss: 2845.2769653576256 Test Loss: 3009.11561143315\n",
      "Epoch 374/4000: Train Loss: 2845.2671262126332 Test Loss: 3009.1021630546024\n",
      "Epoch 375/4000: Train Loss: 2845.2572974164855 Test Loss: 3009.088727838719\n",
      "Epoch 376/4000: Train Loss: 2845.2474789582884 Test Loss: 3009.0753057738766\n",
      "Epoch 377/4000: Train Loss: 2845.237670827158 Test Loss: 3009.0618968484478\n",
      "Epoch 378/4000: Train Loss: 2845.2278730122225 Test Loss: 3009.0485010508078\n",
      "Epoch 379/4000: Train Loss: 2845.2180855026213 Test Loss: 3009.0351183693356\n",
      "Epoch 380/4000: Train Loss: 2845.208308287507 Test Loss: 3009.0217487924124\n",
      "Epoch 381/4000: Train Loss: 2845.198541356041 Test Loss: 3009.008392308418\n",
      "Epoch 382/4000: Train Loss: 2845.188784697398 Test Loss: 3008.9950489057364\n",
      "Epoch 383/4000: Train Loss: 2845.1790383007647 Test Loss: 3008.9817185727543\n",
      "Epoch 384/4000: Train Loss: 2845.169302155337 Test Loss: 3008.968401297856\n",
      "Epoch 385/4000: Train Loss: 2845.1595762503243 Test Loss: 3008.9550970694336\n",
      "Epoch 386/4000: Train Loss: 2845.149860574947 Test Loss: 3008.94180587588\n",
      "Epoch 387/4000: Train Loss: 2845.140155118437 Test Loss: 3008.9285277055865\n",
      "Epoch 388/4000: Train Loss: 2845.1304598700367 Test Loss: 3008.9152625469487\n",
      "Epoch 389/4000: Train Loss: 2845.120774819001 Test Loss: 3008.902010388367\n",
      "Epoch 390/4000: Train Loss: 2845.1110999545954 Test Loss: 3008.888771218242\n",
      "Epoch 391/4000: Train Loss: 2845.1014352660977 Test Loss: 3008.8755450249723\n",
      "Epoch 392/4000: Train Loss: 2845.0917807427963 Test Loss: 3008.8623317969696\n",
      "Epoch 393/4000: Train Loss: 2845.082136373992 Test Loss: 3008.8491315226383\n",
      "Epoch 394/4000: Train Loss: 2845.072502148995 Test Loss: 3008.8359441903885\n",
      "Epoch 395/4000: Train Loss: 2845.062878057129 Test Loss: 3008.8227697886337\n",
      "Epoch 396/4000: Train Loss: 2845.053264087728 Test Loss: 3008.8096083057903\n",
      "Epoch 397/4000: Train Loss: 2845.043660230136 Test Loss: 3008.796459730275\n",
      "Epoch 398/4000: Train Loss: 2845.034066473712 Test Loss: 3008.78332405051\n",
      "Epoch 399/4000: Train Loss: 2845.024482807822 Test Loss: 3008.770201254918\n",
      "Epoch 400/4000: Train Loss: 2845.014909221846 Test Loss: 3008.757091331927\n",
      "Epoch 401/4000: Train Loss: 2845.0053457051754 Test Loss: 3008.7439942699643\n",
      "Epoch 402/4000: Train Loss: 2844.9957922472104 Test Loss: 3008.730910057465\n",
      "Epoch 403/4000: Train Loss: 2844.9862488373647 Test Loss: 3008.717838682861\n",
      "Epoch 404/4000: Train Loss: 2844.976715465062 Test Loss: 3008.704780134592\n",
      "Epoch 405/4000: Train Loss: 2844.967192119739 Test Loss: 3008.6917344011013\n",
      "Epoch 406/4000: Train Loss: 2844.9576787908413 Test Loss: 3008.6787014708307\n",
      "Epoch 407/4000: Train Loss: 2844.9481754678263 Test Loss: 3008.665681332227\n",
      "Epoch 408/4000: Train Loss: 2844.938682140164 Test Loss: 3008.6526739737437\n",
      "Epoch 409/4000: Train Loss: 2844.9291987973347 Test Loss: 3008.639679383833\n",
      "Epoch 410/4000: Train Loss: 2844.9197254288283 Test Loss: 3008.6266975509543\n",
      "Epoch 411/4000: Train Loss: 2844.9102620241483 Test Loss: 3008.6137284635665\n",
      "Epoch 412/4000: Train Loss: 2844.9008085728087 Test Loss: 3008.6007721101323\n",
      "Epoch 413/4000: Train Loss: 2844.891365064332 Test Loss: 3008.5878284791206\n",
      "Epoch 414/4000: Train Loss: 2844.881931488256 Test Loss: 3008.5748975590027\n",
      "Epoch 415/4000: Train Loss: 2844.872507834127 Test Loss: 3008.561979338252\n",
      "Epoch 416/4000: Train Loss: 2844.863094091503 Test Loss: 3008.549073805347\n",
      "Epoch 417/4000: Train Loss: 2844.8536902499527 Test Loss: 3008.536180948769\n",
      "Epoch 418/4000: Train Loss: 2844.8442962990557 Test Loss: 3008.5233007570023\n",
      "Epoch 419/4000: Train Loss: 2844.8349122284044 Test Loss: 3008.5104332185365\n",
      "Epoch 420/4000: Train Loss: 2844.825538027599 Test Loss: 3008.4975783218642\n",
      "Epoch 421/4000: Train Loss: 2844.816173686254 Test Loss: 3008.484736055482\n",
      "Epoch 422/4000: Train Loss: 2844.806819193993 Test Loss: 3008.4719064078895\n",
      "Epoch 423/4000: Train Loss: 2844.7974745404513 Test Loss: 3008.459089367589\n",
      "Epoch 424/4000: Train Loss: 2844.7881397152746 Test Loss: 3008.4462849230895\n",
      "Epoch 425/4000: Train Loss: 2844.7788147081196 Test Loss: 3008.4334930629047\n",
      "Epoch 426/4000: Train Loss: 2844.7694995086545 Test Loss: 3008.420713775548\n",
      "Epoch 427/4000: Train Loss: 2844.7601941065586 Test Loss: 3008.40794704954\n",
      "Epoch 428/4000: Train Loss: 2844.7508984915216 Test Loss: 3008.3951928734036\n",
      "Epoch 429/4000: Train Loss: 2844.741612653243 Test Loss: 3008.3824512356664\n",
      "Epoch 430/4000: Train Loss: 2844.7323365814364 Test Loss: 3008.3697221248603\n",
      "Epoch 431/4000: Train Loss: 2844.7230702658235 Test Loss: 3008.3570055295245\n",
      "Epoch 432/4000: Train Loss: 2844.7138136961366 Test Loss: 3008.344301438195\n",
      "Epoch 433/4000: Train Loss: 2844.7045668621213 Test Loss: 3008.331609839419\n",
      "Epoch 434/4000: Train Loss: 2844.695329753532 Test Loss: 3008.3189307217426\n",
      "Epoch 435/4000: Train Loss: 2844.686102360134 Test Loss: 3008.306264073722\n",
      "Epoch 436/4000: Train Loss: 2844.6768846717055 Test Loss: 3008.2936098839123\n",
      "Epoch 437/4000: Train Loss: 2844.667676678034 Test Loss: 3008.280968140877\n",
      "Epoch 438/4000: Train Loss: 2844.6584783689163 Test Loss: 3008.2683388331825\n",
      "Epoch 439/4000: Train Loss: 2844.6492897341623 Test Loss: 3008.2557219493974\n",
      "Epoch 440/4000: Train Loss: 2844.640110763593 Test Loss: 3008.243117478098\n",
      "Epoch 441/4000: Train Loss: 2844.6309414470375 Test Loss: 3008.230525407865\n",
      "Epoch 442/4000: Train Loss: 2844.6217817743377 Test Loss: 3008.217945727283\n",
      "Epoch 443/4000: Train Loss: 2844.612631735346 Test Loss: 3008.2053784249388\n",
      "Epoch 444/4000: Train Loss: 2844.6034913199264 Test Loss: 3008.1928234894267\n",
      "Epoch 445/4000: Train Loss: 2844.5943605179505 Test Loss: 3008.1802809093474\n",
      "Epoch 446/4000: Train Loss: 2844.5852393193036 Test Loss: 3008.1677506733004\n",
      "Epoch 447/4000: Train Loss: 2844.576127713881 Test Loss: 3008.155232769895\n",
      "Epoch 448/4000: Train Loss: 2844.5670256915873 Test Loss: 3008.142727187744\n",
      "Epoch 449/4000: Train Loss: 2844.557933242341 Test Loss: 3008.1302339154636\n",
      "Epoch 450/4000: Train Loss: 2844.548850356067 Test Loss: 3008.117752941676\n",
      "Epoch 451/4000: Train Loss: 2844.539777022704 Test Loss: 3008.1052842550107\n",
      "Epoch 452/4000: Train Loss: 2844.5307132322005 Test Loss: 3008.0928278440974\n",
      "Epoch 453/4000: Train Loss: 2844.521658974515 Test Loss: 3008.080383697573\n",
      "Epoch 454/4000: Train Loss: 2844.512614239617 Test Loss: 3008.0679518040793\n",
      "Epoch 455/4000: Train Loss: 2844.503579017487 Test Loss: 3008.055532152264\n",
      "Epoch 456/4000: Train Loss: 2844.494553298116 Test Loss: 3008.0431247307806\n",
      "Epoch 457/4000: Train Loss: 2844.485537071505 Test Loss: 3008.030729528283\n",
      "Epoch 458/4000: Train Loss: 2844.476530327665 Test Loss: 3008.0183465334335\n",
      "Epoch 459/4000: Train Loss: 2844.46753305662 Test Loss: 3008.005975734902\n",
      "Epoch 460/4000: Train Loss: 2844.4585452484025 Test Loss: 3007.993617121359\n",
      "Epoch 461/4000: Train Loss: 2844.4495668930554 Test Loss: 3007.9812706814832\n",
      "Epoch 462/4000: Train Loss: 2844.4405979806334 Test Loss: 3007.9689364039564\n",
      "Epoch 463/4000: Train Loss: 2844.4316385012003 Test Loss: 3007.956614277466\n",
      "Epoch 464/4000: Train Loss: 2844.422688444832 Test Loss: 3007.9443042907083\n",
      "Epoch 465/4000: Train Loss: 2844.4137478016137 Test Loss: 3007.93200643238\n",
      "Epoch 466/4000: Train Loss: 2844.4048165616414 Test Loss: 3007.919720691187\n",
      "Epoch 467/4000: Train Loss: 2844.395894715022 Test Loss: 3007.9074470558376\n",
      "Epoch 468/4000: Train Loss: 2844.3869822518714 Test Loss: 3007.8951855150463\n",
      "Epoch 469/4000: Train Loss: 2844.3780791623176 Test Loss: 3007.882936057533\n",
      "Epoch 470/4000: Train Loss: 2844.3691854364984 Test Loss: 3007.870698672025\n",
      "Epoch 471/4000: Train Loss: 2844.3603010645616 Test Loss: 3007.8584733472526\n",
      "Epoch 472/4000: Train Loss: 2844.351426036666 Test Loss: 3007.8462600719545\n",
      "Epoch 473/4000: Train Loss: 2844.3425603429814 Test Loss: 3007.8340588348724\n",
      "Epoch 474/4000: Train Loss: 2844.3337039736857 Test Loss: 3007.8218696247523\n",
      "Epoch 475/4000: Train Loss: 2844.32485691897 Test Loss: 3007.8096924303513\n",
      "Epoch 476/4000: Train Loss: 2844.316019169033 Test Loss: 3007.7975272404256\n",
      "Epoch 477/4000: Train Loss: 2844.3071907140866 Test Loss: 3007.7853740437413\n",
      "Epoch 478/4000: Train Loss: 2844.2983715443506 Test Loss: 3007.77323282907\n",
      "Epoch 479/4000: Train Loss: 2844.2895616500564 Test Loss: 3007.7611035851883\n",
      "Epoch 480/4000: Train Loss: 2844.280761021445 Test Loss: 3007.7489863008764\n",
      "Epoch 481/4000: Train Loss: 2844.271969648769 Test Loss: 3007.736880964924\n",
      "Epoch 482/4000: Train Loss: 2844.2631875222896 Test Loss: 3007.7247875661237\n",
      "Epoch 483/4000: Train Loss: 2844.25441463228 Test Loss: 3007.7127060932767\n",
      "Epoch 484/4000: Train Loss: 2844.245650969022 Test Loss: 3007.7006365351854\n",
      "Epoch 485/4000: Train Loss: 2844.236896522808 Test Loss: 3007.6885788806653\n",
      "Epoch 486/4000: Train Loss: 2844.228151283942 Test Loss: 3007.67653311853\n",
      "Epoch 487/4000: Train Loss: 2844.2194152427373 Test Loss: 3007.6644992376055\n",
      "Epoch 488/4000: Train Loss: 2844.2106883895167 Test Loss: 3007.652477226718\n",
      "Epoch 489/4000: Train Loss: 2844.201970714614 Test Loss: 3007.640467074706\n",
      "Epoch 490/4000: Train Loss: 2844.1932622083737 Test Loss: 3007.628468770409\n",
      "Epoch 491/4000: Train Loss: 2844.18456286115 Test Loss: 3007.616482302673\n",
      "Epoch 492/4000: Train Loss: 2844.1758726633066 Test Loss: 3007.604507660353\n",
      "Epoch 493/4000: Train Loss: 2844.167191605218 Test Loss: 3007.5925448323082\n",
      "Epoch 494/4000: Train Loss: 2844.158519677269 Test Loss: 3007.580593807403\n",
      "Epoch 495/4000: Train Loss: 2844.1498568698544 Test Loss: 3007.5686545745093\n",
      "Epoch 496/4000: Train Loss: 2844.1412031733794 Test Loss: 3007.5567271225063\n",
      "Epoch 497/4000: Train Loss: 2844.132558578259 Test Loss: 3007.5448114402757\n",
      "Epoch 498/4000: Train Loss: 2844.123923074917 Test Loss: 3007.53290751671\n",
      "Epoch 499/4000: Train Loss: 2844.1152966537907 Test Loss: 3007.5210153407043\n",
      "Epoch 500/4000: Train Loss: 2844.106679305324 Test Loss: 3007.5091349011595\n",
      "Epoch 501/4000: Train Loss: 2844.098071019973 Test Loss: 3007.497266186987\n",
      "Epoch 502/4000: Train Loss: 2844.0894717882024 Test Loss: 3007.4854091871002\n",
      "Epoch 503/4000: Train Loss: 2844.080881600489 Test Loss: 3007.473563890424\n",
      "Epoch 504/4000: Train Loss: 2844.0723004473175 Test Loss: 3007.4617302858824\n",
      "Epoch 505/4000: Train Loss: 2844.0637283191836 Test Loss: 3007.4499083624105\n",
      "Epoch 506/4000: Train Loss: 2844.055165206593 Test Loss: 3007.4380981089507\n",
      "Epoch 507/4000: Train Loss: 2844.046611100061 Test Loss: 3007.426299514448\n",
      "Epoch 508/4000: Train Loss: 2844.0380659901143 Test Loss: 3007.414512567857\n",
      "Epoch 509/4000: Train Loss: 2844.0295298672877 Test Loss: 3007.4027372581363\n",
      "Epoch 510/4000: Train Loss: 2844.021002722127 Test Loss: 3007.3909735742527\n",
      "Epoch 511/4000: Train Loss: 2844.012484545188 Test Loss: 3007.379221505181\n",
      "Epoch 512/4000: Train Loss: 2844.0039753270357 Test Loss: 3007.3674810398966\n",
      "Epoch 513/4000: Train Loss: 2843.995475058247 Test Loss: 3007.355752167389\n",
      "Epoch 514/4000: Train Loss: 2843.9869837294054 Test Loss: 3007.34403487665\n",
      "Epoch 515/4000: Train Loss: 2843.9785013311084 Test Loss: 3007.3323291566753\n",
      "Epoch 516/4000: Train Loss: 2843.9700278539594 Test Loss: 3007.3206349964767\n",
      "Epoch 517/4000: Train Loss: 2843.961563288575 Test Loss: 3007.3089523850595\n",
      "Epoch 518/4000: Train Loss: 2843.9531076255794 Test Loss: 3007.2972813114475\n",
      "Epoch 519/4000: Train Loss: 2843.9446608556077 Test Loss: 3007.285621764665\n",
      "Epoch 520/4000: Train Loss: 2843.936222969305 Test Loss: 3007.273973733742\n",
      "Epoch 521/4000: Train Loss: 2843.927793957327 Test Loss: 3007.26233720772\n",
      "Epoch 522/4000: Train Loss: 2843.919373810337 Test Loss: 3007.2507121756444\n",
      "Epoch 523/4000: Train Loss: 2843.91096251901 Test Loss: 3007.2390986265655\n",
      "Epoch 524/4000: Train Loss: 2843.902560074029 Test Loss: 3007.2274965495444\n",
      "Epoch 525/4000: Train Loss: 2843.8941664660906 Test Loss: 3007.215905933645\n",
      "Epoch 526/4000: Train Loss: 2843.8857816858963 Test Loss: 3007.2043267679423\n",
      "Epoch 527/4000: Train Loss: 2843.8774057241603 Test Loss: 3007.192759041513\n",
      "Epoch 528/4000: Train Loss: 2843.8690385716072 Test Loss: 3007.181202743445\n",
      "Epoch 529/4000: Train Loss: 2843.8606802189697 Test Loss: 3007.169657862831\n",
      "Epoch 530/4000: Train Loss: 2843.85233065699 Test Loss: 3007.1581243887713\n",
      "Epoch 531/4000: Train Loss: 2843.8439898764227 Test Loss: 3007.146602310373\n",
      "Epoch 532/4000: Train Loss: 2843.835657868028 Test Loss: 3007.135091616748\n",
      "Epoch 533/4000: Train Loss: 2843.82733462258 Test Loss: 3007.123592297018\n",
      "Epoch 534/4000: Train Loss: 2843.8190201308603 Test Loss: 3007.112104340312\n",
      "Epoch 535/4000: Train Loss: 2843.81071438366 Test Loss: 3007.1006277357606\n",
      "Epoch 536/4000: Train Loss: 2843.8024173717813 Test Loss: 3007.0891624725095\n",
      "Epoch 537/4000: Train Loss: 2843.794129086035 Test Loss: 3007.077708539706\n",
      "Epoch 538/4000: Train Loss: 2843.785849517242 Test Loss: 3007.0662659265013\n",
      "Epoch 539/4000: Train Loss: 2843.777578656232 Test Loss: 3007.0548346220644\n",
      "Epoch 540/4000: Train Loss: 2843.769316493846 Test Loss: 3007.0434146155594\n",
      "Epoch 541/4000: Train Loss: 2843.7610630209333 Test Loss: 3007.032005896166\n",
      "Epoch 542/4000: Train Loss: 2843.7528182283545 Test Loss: 3007.0206084530664\n",
      "Epoch 543/4000: Train Loss: 2843.744582106977 Test Loss: 3007.0092222754515\n",
      "Epoch 544/4000: Train Loss: 2843.736354647681 Test Loss: 3006.997847352519\n",
      "Epoch 545/4000: Train Loss: 2843.7281358413534 Test Loss: 3006.9864836734732\n",
      "Epoch 546/4000: Train Loss: 2843.719925678893 Test Loss: 3006.9751312275275\n",
      "Epoch 547/4000: Train Loss: 2843.7117241512074 Test Loss: 3006.9637900038983\n",
      "Epoch 548/4000: Train Loss: 2843.703531249213 Test Loss: 3006.952459991815\n",
      "Epoch 549/4000: Train Loss: 2843.6953469638374 Test Loss: 3006.9411411805086\n",
      "Epoch 550/4000: Train Loss: 2843.687171286016 Test Loss: 3006.9298335592193\n",
      "Epoch 551/4000: Train Loss: 2843.679004206695 Test Loss: 3006.918537117198\n",
      "Epoch 552/4000: Train Loss: 2843.6708457168293 Test Loss: 3006.9072518436965\n",
      "Epoch 553/4000: Train Loss: 2843.662695807384 Test Loss: 3006.8959777279774\n",
      "Epoch 554/4000: Train Loss: 2843.6545544693336 Test Loss: 3006.8847147593133\n",
      "Epoch 555/4000: Train Loss: 2843.6464216936615 Test Loss: 3006.8734629269766\n",
      "Epoch 556/4000: Train Loss: 2843.6382974713615 Test Loss: 3006.8622222202525\n",
      "Epoch 557/4000: Train Loss: 2843.6301817934363 Test Loss: 3006.850992628434\n",
      "Epoch 558/4000: Train Loss: 2843.622074650899 Test Loss: 3006.839774140818\n",
      "Epoch 559/4000: Train Loss: 2843.6139760347696 Test Loss: 3006.8285667467107\n",
      "Epoch 560/4000: Train Loss: 2843.60588593608 Test Loss: 3006.817370435425\n",
      "Epoch 561/4000: Train Loss: 2843.5978043458726 Test Loss: 3006.806185196281\n",
      "Epoch 562/4000: Train Loss: 2843.589731255196 Test Loss: 3006.7950110186093\n",
      "Epoch 563/4000: Train Loss: 2843.5816666551095 Test Loss: 3006.7838478917415\n",
      "Epoch 564/4000: Train Loss: 2843.573610536683 Test Loss: 3006.772695805022\n",
      "Epoch 565/4000: Train Loss: 2843.565562890995 Test Loss: 3006.7615547478003\n",
      "Epoch 566/4000: Train Loss: 2843.557523709132 Test Loss: 3006.7504247094353\n",
      "Epoch 567/4000: Train Loss: 2843.5494929821925 Test Loss: 3006.739305679289\n",
      "Epoch 568/4000: Train Loss: 2843.5414707012824 Test Loss: 3006.7281976467343\n",
      "Epoch 569/4000: Train Loss: 2843.5334568575186 Test Loss: 3006.7171006011513\n",
      "Epoch 570/4000: Train Loss: 2843.525451442025 Test Loss: 3006.706014531928\n",
      "Epoch 571/4000: Train Loss: 2843.517454445937 Test Loss: 3006.694939428458\n",
      "Epoch 572/4000: Train Loss: 2843.509465860398 Test Loss: 3006.6838752801436\n",
      "Epoch 573/4000: Train Loss: 2843.5014856765624 Test Loss: 3006.672822076394\n",
      "Epoch 574/4000: Train Loss: 2843.4935138855917 Test Loss: 3006.661779806626\n",
      "Epoch 575/4000: Train Loss: 2843.4855504786588 Test Loss: 3006.650748460264\n",
      "Epoch 576/4000: Train Loss: 2843.477595446944 Test Loss: 3006.639728026741\n",
      "Epoch 577/4000: Train Loss: 2843.469648781639 Test Loss: 3006.628718495496\n",
      "Epoch 578/4000: Train Loss: 2843.4617104739423 Test Loss: 3006.6177198559753\n",
      "Epoch 579/4000: Train Loss: 2843.4537805150644 Test Loss: 3006.606732097636\n",
      "Epoch 580/4000: Train Loss: 2843.4458588962225 Test Loss: 3006.595755209937\n",
      "Epoch 581/4000: Train Loss: 2843.437945608644 Test Loss: 3006.5847891823505\n",
      "Epoch 582/4000: Train Loss: 2843.430040643567 Test Loss: 3006.573834004353\n",
      "Epoch 583/4000: Train Loss: 2843.422143992236 Test Loss: 3006.5628896654293\n",
      "Epoch 584/4000: Train Loss: 2843.4142556459074 Test Loss: 3006.551956155073\n",
      "Epoch 585/4000: Train Loss: 2843.4063755958455 Test Loss: 3006.541033462782\n",
      "Epoch 586/4000: Train Loss: 2843.3985038333235 Test Loss: 3006.530121578066\n",
      "Epoch 587/4000: Train Loss: 2843.3906403496244 Test Loss: 3006.519220490441\n",
      "Epoch 588/4000: Train Loss: 2843.3827851360406 Test Loss: 3006.5083301894288\n",
      "Epoch 589/4000: Train Loss: 2843.374938183873 Test Loss: 3006.4974506645603\n",
      "Epoch 590/4000: Train Loss: 2843.3670994844324 Test Loss: 3006.486581905374\n",
      "Epoch 591/4000: Train Loss: 2843.359269029038 Test Loss: 3006.475723901415\n",
      "Epoch 592/4000: Train Loss: 2843.351446809018 Test Loss: 3006.4648766422406\n",
      "Epoch 593/4000: Train Loss: 2843.343632815711 Test Loss: 3006.454040117409\n",
      "Epoch 594/4000: Train Loss: 2843.3358270404633 Test Loss: 3006.44321431649\n",
      "Epoch 595/4000: Train Loss: 2843.3280294746314 Test Loss: 3006.4323992290615\n",
      "Epoch 596/4000: Train Loss: 2843.32024010958 Test Loss: 3006.4215948447063\n",
      "Epoch 597/4000: Train Loss: 2843.3124589366844 Test Loss: 3006.4108011530184\n",
      "Epoch 598/4000: Train Loss: 2843.304685947326 Test Loss: 3006.4000181435977\n",
      "Epoch 599/4000: Train Loss: 2843.296921132899 Test Loss: 3006.3892458060504\n",
      "Epoch 600/4000: Train Loss: 2843.289164484804 Test Loss: 3006.3784841299953\n",
      "Epoch 601/4000: Train Loss: 2843.2814159944523 Test Loss: 3006.367733105054\n",
      "Epoch 602/4000: Train Loss: 2843.2736756532618 Test Loss: 3006.356992720856\n",
      "Epoch 603/4000: Train Loss: 2843.265943452662 Test Loss: 3006.3462629670425\n",
      "Epoch 604/4000: Train Loss: 2843.2582193840917 Test Loss: 3006.335543833261\n",
      "Epoch 605/4000: Train Loss: 2843.2505034389956 Test Loss: 3006.324835309164\n",
      "Epoch 606/4000: Train Loss: 2843.24279560883 Test Loss: 3006.3141373844132\n",
      "Epoch 607/4000: Train Loss: 2843.2350958850598 Test Loss: 3006.3034500486815\n",
      "Epoch 608/4000: Train Loss: 2843.2274042591584 Test Loss: 3006.2927732916455\n",
      "Epoch 609/4000: Train Loss: 2843.2197207226086 Test Loss: 3006.2821071029903\n",
      "Epoch 610/4000: Train Loss: 2843.2120452669014 Test Loss: 3006.2714514724107\n",
      "Epoch 611/4000: Train Loss: 2843.2043778835377 Test Loss: 3006.260806389608\n",
      "Epoch 612/4000: Train Loss: 2843.196718564027 Test Loss: 3006.250171844293\n",
      "Epoch 613/4000: Train Loss: 2843.189067299888 Test Loss: 3006.2395478261806\n",
      "Epoch 614/4000: Train Loss: 2843.181424082647 Test Loss: 3006.228934324996\n",
      "Epoch 615/4000: Train Loss: 2843.173788903841 Test Loss: 3006.2183313304727\n",
      "Epoch 616/4000: Train Loss: 2843.1661617550153 Test Loss: 3006.2077388323514\n",
      "Epoch 617/4000: Train Loss: 2843.158542627724 Test Loss: 3006.1971568203826\n",
      "Epoch 618/4000: Train Loss: 2843.150931513529 Test Loss: 3006.1865852843216\n",
      "Epoch 619/4000: Train Loss: 2843.1433284040036 Test Loss: 3006.176024213934\n",
      "Epoch 620/4000: Train Loss: 2843.1357332907273 Test Loss: 3006.16547359899\n",
      "Epoch 621/4000: Train Loss: 2843.12814616529 Test Loss: 3006.154933429271\n",
      "Epoch 622/4000: Train Loss: 2843.120567019291 Test Loss: 3006.1444036945677\n",
      "Epoch 623/4000: Train Loss: 2843.1129958443366 Test Loss: 3006.1338843846743\n",
      "Epoch 624/4000: Train Loss: 2843.105432632043 Test Loss: 3006.1233754893947\n",
      "Epoch 625/4000: Train Loss: 2843.097877374035 Test Loss: 3006.1128769985403\n",
      "Epoch 626/4000: Train Loss: 2843.0903300619475 Test Loss: 3006.102388901935\n",
      "Epoch 627/4000: Train Loss: 2843.0827906874215 Test Loss: 3006.091911189404\n",
      "Epoch 628/4000: Train Loss: 2843.075259242109 Test Loss: 3006.0814438507823\n",
      "Epoch 629/4000: Train Loss: 2843.06773571767 Test Loss: 3006.0709868759177\n",
      "Epoch 630/4000: Train Loss: 2843.0602201057736 Test Loss: 3006.060540254657\n",
      "Epoch 631/4000: Train Loss: 2843.052712398098 Test Loss: 3006.050103976865\n",
      "Epoch 632/4000: Train Loss: 2843.0452125863285 Test Loss: 3006.039678032408\n",
      "Epoch 633/4000: Train Loss: 2843.0377206621606 Test Loss: 3006.0292624111644\n",
      "Epoch 634/4000: Train Loss: 2843.0302366172987 Test Loss: 3006.018857103012\n",
      "Epoch 635/4000: Train Loss: 2843.0227604434554 Test Loss: 3006.0084620978478\n",
      "Epoch 636/4000: Train Loss: 2843.015292132352 Test Loss: 3005.9980773855705\n",
      "Epoch 637/4000: Train Loss: 2843.0078316757185 Test Loss: 3005.9877029560876\n",
      "Epoch 638/4000: Train Loss: 2843.000379065293 Test Loss: 3005.977338799316\n",
      "Epoch 639/4000: Train Loss: 2842.9929342928244 Test Loss: 3005.96698490518\n",
      "Epoch 640/4000: Train Loss: 2842.9854973500687 Test Loss: 3005.9566412636123\n",
      "Epoch 641/4000: Train Loss: 2842.97806822879 Test Loss: 3005.946307864548\n",
      "Epoch 642/4000: Train Loss: 2842.970646920762 Test Loss: 3005.935984697943\n",
      "Epoch 643/4000: Train Loss: 2842.9632334177672 Test Loss: 3005.92567175375\n",
      "Epoch 644/4000: Train Loss: 2842.955827711596 Test Loss: 3005.9153690219296\n",
      "Epoch 645/4000: Train Loss: 2842.9484297940485 Test Loss: 3005.90507649246\n",
      "Epoch 646/4000: Train Loss: 2842.9410396569324 Test Loss: 3005.894794155319\n",
      "Epoch 647/4000: Train Loss: 2842.9336572920647 Test Loss: 3005.884522000495\n",
      "Epoch 648/4000: Train Loss: 2842.9262826912704 Test Loss: 3005.874260017984\n",
      "Epoch 649/4000: Train Loss: 2842.9189158463832 Test Loss: 3005.864008197793\n",
      "Epoch 650/4000: Train Loss: 2842.9115567492468 Test Loss: 3005.8537665299327\n",
      "Epoch 651/4000: Train Loss: 2842.904205391711 Test Loss: 3005.8435350044238\n",
      "Epoch 652/4000: Train Loss: 2842.8968617656365 Test Loss: 3005.8333136112956\n",
      "Epoch 653/4000: Train Loss: 2842.889525862891 Test Loss: 3005.8231023405856\n",
      "Epoch 654/4000: Train Loss: 2842.8821976753516 Test Loss: 3005.8129011823385\n",
      "Epoch 655/4000: Train Loss: 2842.8748771949026 Test Loss: 3005.8027101266066\n",
      "Epoch 656/4000: Train Loss: 2842.8675644134405 Test Loss: 3005.7925291634538\n",
      "Epoch 657/4000: Train Loss: 2842.860259322865 Test Loss: 3005.782358282947\n",
      "Epoch 658/4000: Train Loss: 2842.8529619150886 Test Loss: 3005.772197475164\n",
      "Epoch 659/4000: Train Loss: 2842.8456721820303 Test Loss: 3005.762046730191\n",
      "Epoch 660/4000: Train Loss: 2842.8383901156185 Test Loss: 3005.751906038122\n",
      "Epoch 661/4000: Train Loss: 2842.83111570779 Test Loss: 3005.741775389057\n",
      "Epoch 662/4000: Train Loss: 2842.823848950488 Test Loss: 3005.731654773108\n",
      "Epoch 663/4000: Train Loss: 2842.8165898356683 Test Loss: 3005.7215441803933\n",
      "Epoch 664/4000: Train Loss: 2842.8093383552905 Test Loss: 3005.711443601038\n",
      "Epoch 665/4000: Train Loss: 2842.802094501327 Test Loss: 3005.701353025176\n",
      "Epoch 666/4000: Train Loss: 2842.7948582657555 Test Loss: 3005.691272442951\n",
      "Epoch 667/4000: Train Loss: 2842.787629640563 Test Loss: 3005.6812018445135\n",
      "Epoch 668/4000: Train Loss: 2842.780408617746 Test Loss: 3005.6711412200225\n",
      "Epoch 669/4000: Train Loss: 2842.7731951893084 Test Loss: 3005.661090559643\n",
      "Epoch 670/4000: Train Loss: 2842.7659893472633 Test Loss: 3005.6510498535526\n",
      "Epoch 671/4000: Train Loss: 2842.7587910836296 Test Loss: 3005.641019091932\n",
      "Epoch 672/4000: Train Loss: 2842.7516003904393 Test Loss: 3005.6309982649773\n",
      "Epoch 673/4000: Train Loss: 2842.7444172597284 Test Loss: 3005.6209873628845\n",
      "Epoch 674/4000: Train Loss: 2842.737241683544 Test Loss: 3005.6109863758593\n",
      "Epoch 675/4000: Train Loss: 2842.7300736539396 Test Loss: 3005.6009952941217\n",
      "Epoch 676/4000: Train Loss: 2842.7229131629783 Test Loss: 3005.591014107895\n",
      "Epoch 677/4000: Train Loss: 2842.7157602027314 Test Loss: 3005.5810428074083\n",
      "Epoch 678/4000: Train Loss: 2842.708614765279 Test Loss: 3005.571081382905\n",
      "Epoch 679/4000: Train Loss: 2842.701476842709 Test Loss: 3005.5611298246345\n",
      "Epoch 680/4000: Train Loss: 2842.6943464271158 Test Loss: 3005.551188122851\n",
      "Epoch 681/4000: Train Loss: 2842.687223510606 Test Loss: 3005.54125626782\n",
      "Epoch 682/4000: Train Loss: 2842.6801080852915 Test Loss: 3005.5313342498175\n",
      "Epoch 683/4000: Train Loss: 2842.673000143294 Test Loss: 3005.52142205912\n",
      "Epoch 684/4000: Train Loss: 2842.6658996767414 Test Loss: 3005.51151968602\n",
      "Epoch 685/4000: Train Loss: 2842.658806677773 Test Loss: 3005.5016271208146\n",
      "Epoch 686/4000: Train Loss: 2842.6517211385344 Test Loss: 3005.4917443538097\n",
      "Epoch 687/4000: Train Loss: 2842.644643051179 Test Loss: 3005.4818713753207\n",
      "Epoch 688/4000: Train Loss: 2842.6375724078703 Test Loss: 3005.472008175666\n",
      "Epoch 689/4000: Train Loss: 2842.6305092007788 Test Loss: 3005.46215474518\n",
      "Epoch 690/4000: Train Loss: 2842.623453422083 Test Loss: 3005.4523110742007\n",
      "Epoch 691/4000: Train Loss: 2842.61640506397 Test Loss: 3005.4424771530735\n",
      "Epoch 692/4000: Train Loss: 2842.6093641186367 Test Loss: 3005.432652972154\n",
      "Epoch 693/4000: Train Loss: 2842.602330578285 Test Loss: 3005.4228385218053\n",
      "Epoch 694/4000: Train Loss: 2842.595304435127 Test Loss: 3005.4130337924\n",
      "Epoch 695/4000: Train Loss: 2842.588285681384 Test Loss: 3005.403238774315\n",
      "Epoch 696/4000: Train Loss: 2842.5812743092824 Test Loss: 3005.393453457943\n",
      "Epoch 697/4000: Train Loss: 2842.5742703110604 Test Loss: 3005.3836778336745\n",
      "Epoch 698/4000: Train Loss: 2842.5672736789606 Test Loss: 3005.373911891917\n",
      "Epoch 699/4000: Train Loss: 2842.560284405238 Test Loss: 3005.3641556230828\n",
      "Epoch 700/4000: Train Loss: 2842.553302482151 Test Loss: 3005.354409017591\n",
      "Epoch 701/4000: Train Loss: 2842.5463279019705 Test Loss: 3005.3446720658726\n",
      "Epoch 702/4000: Train Loss: 2842.5393606569733 Test Loss: 3005.3349447583637\n",
      "Epoch 703/4000: Train Loss: 2842.532400739444 Test Loss: 3005.3252270855087\n",
      "Epoch 704/4000: Train Loss: 2842.5254481416764 Test Loss: 3005.3155190377634\n",
      "Epoch 705/4000: Train Loss: 2842.5185028559717 Test Loss: 3005.3058206055866\n",
      "Epoch 706/4000: Train Loss: 2842.5115648746405 Test Loss: 3005.296131779451\n",
      "Epoch 707/4000: Train Loss: 2842.5046341899993 Test Loss: 3005.286452549834\n",
      "Epoch 708/4000: Train Loss: 2842.497710794374 Test Loss: 3005.2767829072204\n",
      "Epoch 709/4000: Train Loss: 2842.490794680099 Test Loss: 3005.2671228421077\n",
      "Epoch 710/4000: Train Loss: 2842.4838858395165 Test Loss: 3005.257472344995\n",
      "Epoch 711/4000: Train Loss: 2842.4769842649757 Test Loss: 3005.247831406396\n",
      "Epoch 712/4000: Train Loss: 2842.470089948835 Test Loss: 3005.238200016831\n",
      "Epoch 713/4000: Train Loss: 2842.46320288346 Test Loss: 3005.2285781668243\n",
      "Epoch 714/4000: Train Loss: 2842.4563230612252 Test Loss: 3005.218965846914\n",
      "Epoch 715/4000: Train Loss: 2842.449450474513 Test Loss: 3005.209363047643\n",
      "Epoch 716/4000: Train Loss: 2842.442585115713 Test Loss: 3005.199769759564\n",
      "Epoch 717/4000: Train Loss: 2842.4357269772236 Test Loss: 3005.190185973237\n",
      "Epoch 718/4000: Train Loss: 2842.4288760514505 Test Loss: 3005.18061167923\n",
      "Epoch 719/4000: Train Loss: 2842.422032330808 Test Loss: 3005.1710468681204\n",
      "Epoch 720/4000: Train Loss: 2842.415195807719 Test Loss: 3005.1614915304954\n",
      "Epoch 721/4000: Train Loss: 2842.408366474613 Test Loss: 3005.1519456569426\n",
      "Epoch 722/4000: Train Loss: 2842.4015443239277 Test Loss: 3005.1424092380685\n",
      "Epoch 723/4000: Train Loss: 2842.39472934811 Test Loss: 3005.132882264481\n",
      "Epoch 724/4000: Train Loss: 2842.3879215396123 Test Loss: 3005.1233647268\n",
      "Epoch 725/4000: Train Loss: 2842.3811208908983 Test Loss: 3005.1138566156483\n",
      "Epoch 726/4000: Train Loss: 2842.3743273944365 Test Loss: 3005.1043579216625\n",
      "Epoch 727/4000: Train Loss: 2842.367541042705 Test Loss: 3005.0948686354836\n",
      "Epoch 728/4000: Train Loss: 2842.3607618281894 Test Loss: 3005.085388747764\n",
      "Epoch 729/4000: Train Loss: 2842.3539897433843 Test Loss: 3005.075918249163\n",
      "Epoch 730/4000: Train Loss: 2842.347224780789 Test Loss: 3005.0664571303446\n",
      "Epoch 731/4000: Train Loss: 2842.3404669329148 Test Loss: 3005.057005381988\n",
      "Epoch 732/4000: Train Loss: 2842.3337161922777 Test Loss: 3005.0475629947746\n",
      "Epoch 733/4000: Train Loss: 2842.326972551404 Test Loss: 3005.0381299593982\n",
      "Epoch 734/4000: Train Loss: 2842.3202360028254 Test Loss: 3005.0287062665543\n",
      "Epoch 735/4000: Train Loss: 2842.313506539083 Test Loss: 3005.019291906959\n",
      "Epoch 736/4000: Train Loss: 2842.3067841527263 Test Loss: 3005.009886871321\n",
      "Epoch 737/4000: Train Loss: 2842.3000688363118 Test Loss: 3005.000491150371\n",
      "Epoch 738/4000: Train Loss: 2842.2933605824023 Test Loss: 3004.9911047348382\n",
      "Epoch 739/4000: Train Loss: 2842.286659383571 Test Loss: 3004.9817276154654\n",
      "Epoch 740/4000: Train Loss: 2842.2799652323974 Test Loss: 3004.972359783002\n",
      "Epoch 741/4000: Train Loss: 2842.273278121471 Test Loss: 3004.9630012282037\n",
      "Epoch 742/4000: Train Loss: 2842.266598043385 Test Loss: 3004.9536519418375\n",
      "Epoch 743/4000: Train Loss: 2842.259924990744 Test Loss: 3004.944311914681\n",
      "Epoch 744/4000: Train Loss: 2842.253258956159 Test Loss: 3004.9349811375096\n",
      "Epoch 745/4000: Train Loss: 2842.246599932249 Test Loss: 3004.9256596011187\n",
      "Epoch 746/4000: Train Loss: 2842.23994791164 Test Loss: 3004.9163472963046\n",
      "Epoch 747/4000: Train Loss: 2842.233302886967 Test Loss: 3004.907044213875\n",
      "Epoch 748/4000: Train Loss: 2842.2266648508726 Test Loss: 3004.897750344645\n",
      "Epoch 749/4000: Train Loss: 2842.2200337960053 Test Loss: 3004.8884656794376\n",
      "Epoch 750/4000: Train Loss: 2842.213409715025 Test Loss: 3004.879190209085\n",
      "Epoch 751/4000: Train Loss: 2842.206792600595 Test Loss: 3004.8699239244256\n",
      "Epoch 752/4000: Train Loss: 2842.2001824453896 Test Loss: 3004.8606668163075\n",
      "Epoch 753/4000: Train Loss: 2842.193579242089 Test Loss: 3004.851418875587\n",
      "Epoch 754/4000: Train Loss: 2842.186982983382 Test Loss: 3004.8421800931296\n",
      "Epoch 755/4000: Train Loss: 2842.180393661965 Test Loss: 3004.832950459806\n",
      "Epoch 756/4000: Train Loss: 2842.173811270541 Test Loss: 3004.823729966496\n",
      "Epoch 757/4000: Train Loss: 2842.167235801823 Test Loss: 3004.8145186040915\n",
      "Epoch 758/4000: Train Loss: 2842.160667248528 Test Loss: 3004.8053163634863\n",
      "Epoch 759/4000: Train Loss: 2842.1541056033857 Test Loss: 3004.796123235588\n",
      "Epoch 760/4000: Train Loss: 2842.1475508591284 Test Loss: 3004.786939211307\n",
      "Epoch 761/4000: Train Loss: 2842.1410030084994 Test Loss: 3004.777764281569\n",
      "Epoch 762/4000: Train Loss: 2842.134462044248 Test Loss: 3004.7685984373015\n",
      "Epoch 763/4000: Train Loss: 2842.127927959132 Test Loss: 3004.7594416694415\n",
      "Epoch 764/4000: Train Loss: 2842.1214007459157 Test Loss: 3004.750293968936\n",
      "Epoch 765/4000: Train Loss: 2842.114880397373 Test Loss: 3004.74115532674\n",
      "Epoch 766/4000: Train Loss: 2842.1083669062837 Test Loss: 3004.732025733814\n",
      "Epoch 767/4000: Train Loss: 2842.1018602654344 Test Loss: 3004.7229051811314\n",
      "Epoch 768/4000: Train Loss: 2842.095360467622 Test Loss: 3004.7137936596705\n",
      "Epoch 769/4000: Train Loss: 2842.0888675056494 Test Loss: 3004.704691160415\n",
      "Epoch 770/4000: Train Loss: 2842.082381372327 Test Loss: 3004.695597674363\n",
      "Epoch 771/4000: Train Loss: 2842.075902060473 Test Loss: 3004.6865131925174\n",
      "Epoch 772/4000: Train Loss: 2842.069429562913 Test Loss: 3004.677437705888\n",
      "Epoch 773/4000: Train Loss: 2842.0629638724804 Test Loss: 3004.668371205498\n",
      "Epoch 774/4000: Train Loss: 2842.056504982015 Test Loss: 3004.6593136823712\n",
      "Epoch 775/4000: Train Loss: 2842.050052884367 Test Loss: 3004.6502651275464\n",
      "Epoch 776/4000: Train Loss: 2842.043607572391 Test Loss: 3004.6412255320665\n",
      "Epoch 777/4000: Train Loss: 2842.0371690389507 Test Loss: 3004.632194886983\n",
      "Epoch 778/4000: Train Loss: 2842.030737276917 Test Loss: 3004.62317318336\n",
      "Epoch 779/4000: Train Loss: 2842.024312279168 Test Loss: 3004.614160412261\n",
      "Epoch 780/4000: Train Loss: 2842.01789403859 Test Loss: 3004.605156564764\n",
      "Epoch 781/4000: Train Loss: 2842.0114825480764 Test Loss: 3004.596161631957\n",
      "Epoch 782/4000: Train Loss: 2842.005077800528 Test Loss: 3004.5871756049296\n",
      "Epoch 783/4000: Train Loss: 2841.998679788852 Test Loss: 3004.5781984747828\n",
      "Epoch 784/4000: Train Loss: 2841.992288505965 Test Loss: 3004.5692302326297\n",
      "Epoch 785/4000: Train Loss: 2841.9859039447906 Test Loss: 3004.560270869584\n",
      "Epoch 786/4000: Train Loss: 2841.9795260982596 Test Loss: 3004.5513203767714\n",
      "Epoch 787/4000: Train Loss: 2841.9731549593093 Test Loss: 3004.542378745329\n",
      "Epoch 788/4000: Train Loss: 2841.9667905208853 Test Loss: 3004.533445966395\n",
      "Epoch 789/4000: Train Loss: 2841.9604327759407 Test Loss: 3004.52452203112\n",
      "Epoch 790/4000: Train Loss: 2841.954081717436 Test Loss: 3004.515606930663\n",
      "Epoch 791/4000: Train Loss: 2841.9477373383393 Test Loss: 3004.506700656189\n",
      "Epoch 792/4000: Train Loss: 2841.9413996316252 Test Loss: 3004.4978031988726\n",
      "Epoch 793/4000: Train Loss: 2841.935068590276 Test Loss: 3004.488914549898\n",
      "Epoch 794/4000: Train Loss: 2841.9287442072823 Test Loss: 3004.480034700452\n",
      "Epoch 795/4000: Train Loss: 2841.9224264756413 Test Loss: 3004.4711636417373\n",
      "Epoch 796/4000: Train Loss: 2841.9161153883565 Test Loss: 3004.4623013649584\n",
      "Epoch 797/4000: Train Loss: 2841.9098109384418 Test Loss: 3004.4534478613286\n",
      "Epoch 798/4000: Train Loss: 2841.9035131189153 Test Loss: 3004.4446031220727\n",
      "Epoch 799/4000: Train Loss: 2841.897221922804 Test Loss: 3004.435767138422\n",
      "Epoch 800/4000: Train Loss: 2841.890937343142 Test Loss: 3004.4269399016175\n",
      "Epoch 801/4000: Train Loss: 2841.884659372971 Test Loss: 3004.4181214029018\n",
      "Epoch 802/4000: Train Loss: 2841.8783880053393 Test Loss: 3004.4093116335307\n",
      "Epoch 803/4000: Train Loss: 2841.8721232333023 Test Loss: 3004.400510584771\n",
      "Epoch 804/4000: Train Loss: 2841.8658650499247 Test Loss: 3004.391718247893\n",
      "Epoch 805/4000: Train Loss: 2841.8596134482764 Test Loss: 3004.3829346141747\n",
      "Epoch 806/4000: Train Loss: 2841.8533684214353 Test Loss: 3004.3741596749046\n",
      "Epoch 807/4000: Train Loss: 2841.8471299624866 Test Loss: 3004.365393421379\n",
      "Epoch 808/4000: Train Loss: 2841.840898064523 Test Loss: 3004.3566358449\n",
      "Epoch 809/4000: Train Loss: 2841.8346727206435 Test Loss: 3004.3478869367805\n",
      "Epoch 810/4000: Train Loss: 2841.8284539239553 Test Loss: 3004.3391466883404\n",
      "Epoch 811/4000: Train Loss: 2841.8222416675735 Test Loss: 3004.330415090908\n",
      "Epoch 812/4000: Train Loss: 2841.8160359446188 Test Loss: 3004.3216921358176\n",
      "Epoch 813/4000: Train Loss: 2841.80983674822 Test Loss: 3004.3129778144144\n",
      "Epoch 814/4000: Train Loss: 2841.8036440715136 Test Loss: 3004.3042721180495\n",
      "Epoch 815/4000: Train Loss: 2841.7974579076417 Test Loss: 3004.2955750380856\n",
      "Epoch 816/4000: Train Loss: 2841.7912782497556 Test Loss: 3004.286886565888\n",
      "Epoch 817/4000: Train Loss: 2841.785105091012 Test Loss: 3004.2782066928335\n",
      "Epoch 818/4000: Train Loss: 2841.7789384245766 Test Loss: 3004.269535410308\n",
      "Epoch 819/4000: Train Loss: 2841.7727782436205 Test Loss: 3004.2608727097027\n",
      "Epoch 820/4000: Train Loss: 2841.766624541324 Test Loss: 3004.252218582418\n",
      "Epoch 821/4000: Train Loss: 2841.760477310872 Test Loss: 3004.243573019861\n",
      "Epoch 822/4000: Train Loss: 2841.754336545459 Test Loss: 3004.234936013449\n",
      "Epoch 823/4000: Train Loss: 2841.7482022382846 Test Loss: 3004.2263075546075\n",
      "Epoch 824/4000: Train Loss: 2841.7420743825583 Test Loss: 3004.217687634768\n",
      "Epoch 825/4000: Train Loss: 2841.735952971493 Test Loss: 3004.2090762453704\n",
      "Epoch 826/4000: Train Loss: 2841.7298379983126 Test Loss: 3004.2004733778617\n",
      "Epoch 827/4000: Train Loss: 2841.723729456245 Test Loss: 3004.1918790237023\n",
      "Epoch 828/4000: Train Loss: 2841.7176273385276 Test Loss: 3004.183293174354\n",
      "Epoch 829/4000: Train Loss: 2841.711531638403 Test Loss: 3004.174715821289\n",
      "Epoch 830/4000: Train Loss: 2841.705442349122 Test Loss: 3004.1661469559876\n",
      "Epoch 831/4000: Train Loss: 2841.699359463942 Test Loss: 3004.1575865699415\n",
      "Epoch 832/4000: Train Loss: 2841.6932829761286 Test Loss: 3004.149034654642\n",
      "Epoch 833/4000: Train Loss: 2841.6872128789523 Test Loss: 3004.1404912015964\n",
      "Epoch 834/4000: Train Loss: 2841.681149165693 Test Loss: 3004.131956202317\n",
      "Epoch 835/4000: Train Loss: 2841.675091829637 Test Loss: 3004.123429648324\n",
      "Epoch 836/4000: Train Loss: 2841.6690408640766 Test Loss: 3004.114911531144\n",
      "Epoch 837/4000: Train Loss: 2841.662996262311 Test Loss: 3004.106401842316\n",
      "Epoch 838/4000: Train Loss: 2841.6569580176492 Test Loss: 3004.097900573382\n",
      "Epoch 839/4000: Train Loss: 2841.650926123404 Test Loss: 3004.0894077158946\n",
      "Epoch 840/4000: Train Loss: 2841.6449005728978 Test Loss: 3004.0809232614142\n",
      "Epoch 841/4000: Train Loss: 2841.638881359457 Test Loss: 3004.0724472015104\n",
      "Epoch 842/4000: Train Loss: 2841.632868476419 Test Loss: 3004.063979527756\n",
      "Epoch 843/4000: Train Loss: 2841.626861917125 Test Loss: 3004.0555202317382\n",
      "Epoch 844/4000: Train Loss: 2841.620861674924 Test Loss: 3004.047069305046\n",
      "Epoch 845/4000: Train Loss: 2841.6148677431725 Test Loss: 3004.038626739282\n",
      "Epoch 846/4000: Train Loss: 2841.6088801152337 Test Loss: 3004.030192526052\n",
      "Epoch 847/4000: Train Loss: 2841.602898784478 Test Loss: 3004.0217666569733\n",
      "Epoch 848/4000: Train Loss: 2841.596923744283 Test Loss: 3004.0133491236684\n",
      "Epoch 849/4000: Train Loss: 2841.5909549880316 Test Loss: 3004.0049399177688\n",
      "Epoch 850/4000: Train Loss: 2841.584992509116 Test Loss: 3003.9965390309144\n",
      "Epoch 851/4000: Train Loss: 2841.5790363009346 Test Loss: 3003.988146454753\n",
      "Epoch 852/4000: Train Loss: 2841.573086356891 Test Loss: 3003.9797621809385\n",
      "Epoch 853/4000: Train Loss: 2841.567142670398 Test Loss: 3003.971386201135\n",
      "Epoch 854/4000: Train Loss: 2841.5612052348747 Test Loss: 3003.963018507013\n",
      "Epoch 855/4000: Train Loss: 2841.555274043747 Test Loss: 3003.9546590902537\n",
      "Epoch 856/4000: Train Loss: 2841.549349090446 Test Loss: 3003.9463079425404\n",
      "Epoch 857/4000: Train Loss: 2841.5434303684137 Test Loss: 3003.9379650555716\n",
      "Epoch 858/4000: Train Loss: 2841.537517871095 Test Loss: 3003.9296304210475\n",
      "Epoch 859/4000: Train Loss: 2841.531611591944 Test Loss: 3003.921304030677\n",
      "Epoch 860/4000: Train Loss: 2841.525711524421 Test Loss: 3003.9129858761835\n",
      "Epoch 861/4000: Train Loss: 2841.519817661992 Test Loss: 3003.9046759492894\n",
      "Epoch 862/4000: Train Loss: 2841.513929998133 Test Loss: 3003.896374241729\n",
      "Epoch 863/4000: Train Loss: 2841.508048526324 Test Loss: 3003.8880807452465\n",
      "Epoch 864/4000: Train Loss: 2841.502173240052 Test Loss: 3003.879795451591\n",
      "Epoch 865/4000: Train Loss: 2841.4963041328133 Test Loss: 3003.8715183525196\n",
      "Epoch 866/4000: Train Loss: 2841.4904411981074 Test Loss: 3003.8632494397966\n",
      "Epoch 867/4000: Train Loss: 2841.484584429445 Test Loss: 3003.854988705198\n",
      "Epoch 868/4000: Train Loss: 2841.4787338203387 Test Loss: 3003.8467361405023\n",
      "Epoch 869/4000: Train Loss: 2841.4728893643114 Test Loss: 3003.8384917375\n",
      "Epoch 870/4000: Train Loss: 2841.4670510548935 Test Loss: 3003.8302554879892\n",
      "Epoch 871/4000: Train Loss: 2841.461218885618 Test Loss: 3003.822027383773\n",
      "Epoch 872/4000: Train Loss: 2841.4553928500286 Test Loss: 3003.8138074166636\n",
      "Epoch 873/4000: Train Loss: 2841.4495729416744 Test Loss: 3003.805595578483\n",
      "Epoch 874/4000: Train Loss: 2841.443759154112 Test Loss: 3003.7973918610587\n",
      "Epoch 875/4000: Train Loss: 2841.4379514809025 Test Loss: 3003.789196256226\n",
      "Epoch 876/4000: Train Loss: 2841.4321499156163 Test Loss: 3003.7810087558296\n",
      "Epoch 877/4000: Train Loss: 2841.4263544518294 Test Loss: 3003.7728293517202\n",
      "Epoch 878/4000: Train Loss: 2841.4205650831254 Test Loss: 3003.764658035758\n",
      "Epoch 879/4000: Train Loss: 2841.414781803094 Test Loss: 3003.75649479981\n",
      "Epoch 880/4000: Train Loss: 2841.40900460533 Test Loss: 3003.74833963575\n",
      "Epoch 881/4000: Train Loss: 2841.403233483439 Test Loss: 3003.740192535462\n",
      "Epoch 882/4000: Train Loss: 2841.397468431031 Test Loss: 3003.7320534908376\n",
      "Epoch 883/4000: Train Loss: 2841.39170944172 Test Loss: 3003.7239224937716\n",
      "Epoch 884/4000: Train Loss: 2841.3859565091325 Test Loss: 3003.715799536173\n",
      "Epoch 885/4000: Train Loss: 2841.3802096268964 Test Loss: 3003.7076846099544\n",
      "Epoch 886/4000: Train Loss: 2841.3744687886497 Test Loss: 3003.69957770704\n",
      "Epoch 887/4000: Train Loss: 2841.368733988035 Test Loss: 3003.6914788193535\n",
      "Epoch 888/4000: Train Loss: 2841.363005218703 Test Loss: 3003.683387938837\n",
      "Epoch 889/4000: Train Loss: 2841.3572824743114 Test Loss: 3003.6753050574334\n",
      "Epoch 890/4000: Train Loss: 2841.3515657485223 Test Loss: 3003.667230167095\n",
      "Epoch 891/4000: Train Loss: 2841.3458550350065 Test Loss: 3003.659163259784\n",
      "Epoch 892/4000: Train Loss: 2841.3401503274413 Test Loss: 3003.6511043274654\n",
      "Epoch 893/4000: Train Loss: 2841.334451619509 Test Loss: 3003.643053362118\n",
      "Epoch 894/4000: Train Loss: 2841.328758904902 Test Loss: 3003.635010355724\n",
      "Epoch 895/4000: Train Loss: 2841.3230721773143 Test Loss: 3003.626975300272\n",
      "Epoch 896/4000: Train Loss: 2841.3173914304507 Test Loss: 3003.618948187765\n",
      "Epoch 897/4000: Train Loss: 2841.311716658022 Test Loss: 3003.6109290102067\n",
      "Epoch 898/4000: Train Loss: 2841.3060478537436 Test Loss: 3003.602917759613\n",
      "Epoch 899/4000: Train Loss: 2841.300385011339 Test Loss: 3003.5949144280053\n",
      "Epoch 900/4000: Train Loss: 2841.2947281245383 Test Loss: 3003.5869190074122\n",
      "Epoch 901/4000: Train Loss: 2841.289077187079 Test Loss: 3003.578931489874\n",
      "Epoch 902/4000: Train Loss: 2841.283432192702 Test Loss: 3003.5709518674335\n",
      "Epoch 903/4000: Train Loss: 2841.2777931351584 Test Loss: 3003.5629801321434\n",
      "Epoch 904/4000: Train Loss: 2841.2721600082036 Test Loss: 3003.555016276064\n",
      "Epoch 905/4000: Train Loss: 2841.266532805602 Test Loss: 3003.5470602912637\n",
      "Epoch 906/4000: Train Loss: 2841.260911521121 Test Loss: 3003.5391121698203\n",
      "Epoch 907/4000: Train Loss: 2841.255296148537 Test Loss: 3003.5311719038145\n",
      "Epoch 908/4000: Train Loss: 2841.249686681633 Test Loss: 3003.523239485338\n",
      "Epoch 909/4000: Train Loss: 2841.2440831141976 Test Loss: 3003.5153149064877\n",
      "Epoch 910/4000: Train Loss: 2841.2384854400266 Test Loss: 3003.5073981593737\n",
      "Epoch 911/4000: Train Loss: 2841.2328936529216 Test Loss: 3003.4994892361074\n",
      "Epoch 912/4000: Train Loss: 2841.2273077466916 Test Loss: 3003.4915881288116\n",
      "Epoch 913/4000: Train Loss: 2841.221727715151 Test Loss: 3003.483694829614\n",
      "Epoch 914/4000: Train Loss: 2841.2161535521223 Test Loss: 3003.475809330652\n",
      "Epoch 915/4000: Train Loss: 2841.2105852514323 Test Loss: 3003.4679316240713\n",
      "Epoch 916/4000: Train Loss: 2841.205022806917 Test Loss: 3003.460061702021\n",
      "Epoch 917/4000: Train Loss: 2841.1994662124157 Test Loss: 3003.452199556665\n",
      "Epoch 918/4000: Train Loss: 2841.193915461778 Test Loss: 3003.4443451801676\n",
      "Epoch 919/4000: Train Loss: 2841.1883705488563 Test Loss: 3003.4364985647035\n",
      "Epoch 920/4000: Train Loss: 2841.1828314675117 Test Loss: 3003.4286597024566\n",
      "Epoch 921/4000: Train Loss: 2841.177298211611 Test Loss: 3003.420828585615\n",
      "Epoch 922/4000: Train Loss: 2841.171770775028 Test Loss: 3003.413005206379\n",
      "Epoch 923/4000: Train Loss: 2841.1662491516413 Test Loss: 3003.405189556951\n",
      "Epoch 924/4000: Train Loss: 2841.1607333353386 Test Loss: 3003.397381629547\n",
      "Epoch 925/4000: Train Loss: 2841.1552233200114 Test Loss: 3003.389581416383\n",
      "Epoch 926/4000: Train Loss: 2841.1497190995597 Test Loss: 3003.381788909691\n",
      "Epoch 927/4000: Train Loss: 2841.1442206678885 Test Loss: 3003.374004101704\n",
      "Epoch 928/4000: Train Loss: 2841.138728018909 Test Loss: 3003.366226984665\n",
      "Epoch 929/4000: Train Loss: 2841.1332411465414 Test Loss: 3003.3584575508266\n",
      "Epoch 930/4000: Train Loss: 2841.127760044709 Test Loss: 3003.350695792444\n",
      "Epoch 931/4000: Train Loss: 2841.1222847073427 Test Loss: 3003.3429417017846\n",
      "Epoch 932/4000: Train Loss: 2841.1168151283814 Test Loss: 3003.3351952711223\n",
      "Epoch 933/4000: Train Loss: 2841.1113513017676 Test Loss: 3003.327456492735\n",
      "Epoch 934/4000: Train Loss: 2841.1058932214523 Test Loss: 3003.3197253589137\n",
      "Epoch 935/4000: Train Loss: 2841.1004408813915 Test Loss: 3003.3120018619525\n",
      "Epoch 936/4000: Train Loss: 2841.094994275549 Test Loss: 3003.3042859941543\n",
      "Epoch 937/4000: Train Loss: 2841.0895533978937 Test Loss: 3003.296577747831\n",
      "Epoch 938/4000: Train Loss: 2841.084118242401 Test Loss: 3003.2888771152993\n",
      "Epoch 939/4000: Train Loss: 2841.0786888030534 Test Loss: 3003.281184088887\n",
      "Epoch 940/4000: Train Loss: 2841.0732650738387 Test Loss: 3003.2734986609257\n",
      "Epoch 941/4000: Train Loss: 2841.0678470487514 Test Loss: 3003.265820823756\n",
      "Epoch 942/4000: Train Loss: 2841.062434721792 Test Loss: 3003.258150569726\n",
      "Epoch 943/4000: Train Loss: 2841.05702808697 Test Loss: 3003.2504878911923\n",
      "Epoch 944/4000: Train Loss: 2841.0516271382976 Test Loss: 3003.2428327805173\n",
      "Epoch 945/4000: Train Loss: 2841.0462318697937 Test Loss: 3003.2351852300712\n",
      "Epoch 946/4000: Train Loss: 2841.0408422754854 Test Loss: 3003.2275452322333\n",
      "Epoch 947/4000: Train Loss: 2841.035458349405 Test Loss: 3003.2199127793874\n",
      "Epoch 948/4000: Train Loss: 2841.0300800855916 Test Loss: 3003.212287863926\n",
      "Epoch 949/4000: Train Loss: 2841.02470747809 Test Loss: 3003.204670478251\n",
      "Epoch 950/4000: Train Loss: 2841.0193405209516 Test Loss: 3003.197060614772\n",
      "Epoch 951/4000: Train Loss: 2841.0139792082327 Test Loss: 3003.189458265898\n",
      "Epoch 952/4000: Train Loss: 2841.008623533998 Test Loss: 3003.181863424057\n",
      "Epoch 953/4000: Train Loss: 2841.0032734923184 Test Loss: 3003.174276081677\n",
      "Epoch 954/4000: Train Loss: 2840.9979290772685 Test Loss: 3003.166696231196\n",
      "Epoch 955/4000: Train Loss: 2840.9925902829314 Test Loss: 3003.1591238650594\n",
      "Epoch 956/4000: Train Loss: 2840.987257103396 Test Loss: 3003.1515589757173\n",
      "Epoch 957/4000: Train Loss: 2840.9819295327575 Test Loss: 3003.1440015556304\n",
      "Epoch 958/4000: Train Loss: 2840.976607565116 Test Loss: 3003.136451597267\n",
      "Epoch 959/4000: Train Loss: 2840.971291194579 Test Loss: 3003.1289090930995\n",
      "Epoch 960/4000: Train Loss: 2840.965980415261 Test Loss: 3003.121374035613\n",
      "Epoch 961/4000: Train Loss: 2840.9606752212812 Test Loss: 3003.1138464172914\n",
      "Epoch 962/4000: Train Loss: 2840.9553756067653 Test Loss: 3003.1063262306343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 963/4000: Train Loss: 2840.950081565845 Test Loss: 3003.098813468145\n",
      "Epoch 964/4000: Train Loss: 2840.9447930926594 Test Loss: 3003.0913081223366\n",
      "Epoch 965/4000: Train Loss: 2840.9395101813525 Test Loss: 3003.0838101857257\n",
      "Epoch 966/4000: Train Loss: 2840.9342328260745 Test Loss: 3003.0763196508383\n",
      "Epoch 967/4000: Train Loss: 2840.9289610209826 Test Loss: 3003.068836510209\n",
      "Epoch 968/4000: Train Loss: 2840.92369476024 Test Loss: 3003.061360756376\n",
      "Epoch 969/4000: Train Loss: 2840.9184340380148 Test Loss: 3003.0538923818885\n",
      "Epoch 970/4000: Train Loss: 2840.9131788484824 Test Loss: 3003.046431379303\n",
      "Epoch 971/4000: Train Loss: 2840.9079291858243 Test Loss: 3003.0389777411806\n",
      "Epoch 972/4000: Train Loss: 2840.902685044228 Test Loss: 3003.031531460093\n",
      "Epoch 973/4000: Train Loss: 2840.897446417887 Test Loss: 3003.024092528614\n",
      "Epoch 974/4000: Train Loss: 2840.8922133010005 Test Loss: 3003.0166609393314\n",
      "Epoch 975/4000: Train Loss: 2840.886985687774 Test Loss: 3003.0092366848357\n",
      "Epoch 976/4000: Train Loss: 2840.8817635724204 Test Loss: 3003.001819757725\n",
      "Epoch 977/4000: Train Loss: 2840.876546949156 Test Loss: 3002.9944101506067\n",
      "Epoch 978/4000: Train Loss: 2840.8713358122063 Test Loss: 3002.987007856096\n",
      "Epoch 979/4000: Train Loss: 2840.866130155801 Test Loss: 3002.9796128668113\n",
      "Epoch 980/4000: Train Loss: 2840.8609299741756 Test Loss: 3002.972225175383\n",
      "Epoch 981/4000: Train Loss: 2840.855735261572 Test Loss: 3002.964844774444\n",
      "Epoch 982/4000: Train Loss: 2840.850546012239 Test Loss: 3002.95747165664\n",
      "Epoch 983/4000: Train Loss: 2840.8453622204315 Test Loss: 3002.9501058146184\n",
      "Epoch 984/4000: Train Loss: 2840.840183880409 Test Loss: 3002.942747241037\n",
      "Epoch 985/4000: Train Loss: 2840.8350109864377 Test Loss: 3002.935395928561\n",
      "Epoch 986/4000: Train Loss: 2840.82984353279 Test Loss: 3002.9280518698615\n",
      "Epoch 987/4000: Train Loss: 2840.824681513745 Test Loss: 3002.920715057619\n",
      "Epoch 988/4000: Train Loss: 2840.819524923587 Test Loss: 3002.913385484517\n",
      "Epoch 989/4000: Train Loss: 2840.814373756606 Test Loss: 3002.9060631432517\n",
      "Epoch 990/4000: Train Loss: 2840.8092280070987 Test Loss: 3002.8987480265214\n",
      "Epoch 991/4000: Train Loss: 2840.804087669367 Test Loss: 3002.8914401270345\n",
      "Epoch 992/4000: Train Loss: 2840.79895273772 Test Loss: 3002.884139437507\n",
      "Epoch 993/4000: Train Loss: 2840.793823206472 Test Loss: 3002.8768459506605\n",
      "Epoch 994/4000: Train Loss: 2840.788699069943 Test Loss: 3002.8695596592233\n",
      "Epoch 995/4000: Train Loss: 2840.78358032246 Test Loss: 3002.8622805559335\n",
      "Epoch 996/4000: Train Loss: 2840.778466958355 Test Loss: 3002.855008633534\n",
      "Epoch 997/4000: Train Loss: 2840.773358971966 Test Loss: 3002.847743884777\n",
      "Epoch 998/4000: Train Loss: 2840.7682563576373 Test Loss: 3002.8404863024184\n",
      "Epoch 999/4000: Train Loss: 2840.7631591097197 Test Loss: 3002.8332358792254\n",
      "Epoch 1000/4000: Train Loss: 2840.758067222569 Test Loss: 3002.82599260797\n",
      "Epoch 1001/4000: Train Loss: 2840.7529806905472 Test Loss: 3002.8187564814302\n",
      "Epoch 1002/4000: Train Loss: 2840.747899508023 Test Loss: 3002.8115274923957\n",
      "Epoch 1003/4000: Train Loss: 2840.742823669369 Test Loss: 3002.8043056336573\n",
      "Epoch 1004/4000: Train Loss: 2840.7377531689654 Test Loss: 3002.7970908980174\n",
      "Epoch 1005/4000: Train Loss: 2840.7326880011997 Test Loss: 3002.789883278283\n",
      "Epoch 1006/4000: Train Loss: 2840.7276281604604 Test Loss: 3002.7826827672725\n",
      "Epoch 1007/4000: Train Loss: 2840.7225736411483 Test Loss: 3002.775489357805\n",
      "Epoch 1008/4000: Train Loss: 2840.7175244376645 Test Loss: 3002.768303042712\n",
      "Epoch 1009/4000: Train Loss: 2840.712480544419 Test Loss: 3002.7611238148265\n",
      "Epoch 1010/4000: Train Loss: 2840.707441955828 Test Loss: 3002.7539516669976\n",
      "Epoch 1011/4000: Train Loss: 2840.702408666312 Test Loss: 3002.746786592072\n",
      "Epoch 1012/4000: Train Loss: 2840.697380670297 Test Loss: 3002.7396285829086\n",
      "Epoch 1013/4000: Train Loss: 2840.6923579622176 Test Loss: 3002.732477632374\n",
      "Epoch 1014/4000: Train Loss: 2840.687340536511 Test Loss: 3002.7253337333373\n",
      "Epoch 1015/4000: Train Loss: 2840.6823283876224 Test Loss: 3002.7181968786795\n",
      "Epoch 1016/4000: Train Loss: 2840.6773215100015 Test Loss: 3002.7110670612856\n",
      "Epoch 1017/4000: Train Loss: 2840.6723198981053 Test Loss: 3002.7039442740506\n",
      "Epoch 1018/4000: Train Loss: 2840.6673235463954 Test Loss: 3002.6968285098724\n",
      "Epoch 1019/4000: Train Loss: 2840.6623324493403 Test Loss: 3002.6897197616604\n",
      "Epoch 1020/4000: Train Loss: 2840.6573466014124 Test Loss: 3002.682618022329\n",
      "Epoch 1021/4000: Train Loss: 2840.652365997093 Test Loss: 3002.6755232847963\n",
      "Epoch 1022/4000: Train Loss: 2840.6473906308656 Test Loss: 3002.668435541996\n",
      "Epoch 1023/4000: Train Loss: 2840.6424204972222 Test Loss: 3002.6613547868596\n",
      "Epoch 1024/4000: Train Loss: 2840.63745559066 Test Loss: 3002.6542810123296\n",
      "Epoch 1025/4000: Train Loss: 2840.6324959056806 Test Loss: 3002.647214211358\n",
      "Epoch 1026/4000: Train Loss: 2840.6275414367933 Test Loss: 3002.640154376899\n",
      "Epoch 1027/4000: Train Loss: 2840.6225921785126 Test Loss: 3002.633101501917\n",
      "Epoch 1028/4000: Train Loss: 2840.6176481253583 Test Loss: 3002.6260555793833\n",
      "Epoch 1029/4000: Train Loss: 2840.612709271855 Test Loss: 3002.619016602274\n",
      "Epoch 1030/4000: Train Loss: 2840.607775612535 Test Loss: 3002.611984563573\n",
      "Epoch 1031/4000: Train Loss: 2840.6028471419368 Test Loss: 3002.6049594562746\n",
      "Epoch 1032/4000: Train Loss: 2840.5979238546015 Test Loss: 3002.597941273374\n",
      "Epoch 1033/4000: Train Loss: 2840.5930057450796 Test Loss: 3002.590930007879\n",
      "Epoch 1034/4000: Train Loss: 2840.5880928079237 Test Loss: 3002.5839256528\n",
      "Epoch 1035/4000: Train Loss: 2840.5831850376953 Test Loss: 3002.576928201159\n",
      "Epoch 1036/4000: Train Loss: 2840.5782824289604 Test Loss: 3002.5699376459793\n",
      "Epoch 1037/4000: Train Loss: 2840.57338497629 Test Loss: 3002.562953980295\n",
      "Epoch 1038/4000: Train Loss: 2840.568492674262 Test Loss: 3002.5559771971475\n",
      "Epoch 1039/4000: Train Loss: 2840.563605517459 Test Loss: 3002.5490072895827\n",
      "Epoch 1040/4000: Train Loss: 2840.55872350047 Test Loss: 3002.5420442506547\n",
      "Epoch 1041/4000: Train Loss: 2840.5538466178896 Test Loss: 3002.535088073425\n",
      "Epoch 1042/4000: Train Loss: 2840.5489748643186 Test Loss: 3002.5281387509613\n",
      "Epoch 1043/4000: Train Loss: 2840.5441082343614 Test Loss: 3002.5211962763365\n",
      "Epoch 1044/4000: Train Loss: 2840.53924672263 Test Loss: 3002.5142606426357\n",
      "Epoch 1045/4000: Train Loss: 2840.5343903237417 Test Loss: 3002.507331842946\n",
      "Epoch 1046/4000: Train Loss: 2840.52953903232 Test Loss: 3002.5004098703616\n",
      "Epoch 1047/4000: Train Loss: 2840.524692842992 Test Loss: 3002.493494717985\n",
      "Epoch 1048/4000: Train Loss: 2840.519851750393 Test Loss: 3002.486586378928\n",
      "Epoch 1049/4000: Train Loss: 2840.515015749162 Test Loss: 3002.4796848463034\n",
      "Epoch 1050/4000: Train Loss: 2840.5101848339445 Test Loss: 3002.472790113235\n",
      "Epoch 1051/4000: Train Loss: 2840.5053589993927 Test Loss: 3002.465902172855\n",
      "Epoch 1052/4000: Train Loss: 2840.5005382401614 Test Loss: 3002.459021018298\n",
      "Epoch 1053/4000: Train Loss: 2840.4957225509143 Test Loss: 3002.4521466427072\n",
      "Epoch 1054/4000: Train Loss: 2840.490911926319 Test Loss: 3002.445279039234\n",
      "Epoch 1055/4000: Train Loss: 2840.486106361049 Test Loss: 3002.438418201035\n",
      "Epoch 1056/4000: Train Loss: 2840.4813058497825 Test Loss: 3002.431564121274\n",
      "Epoch 1057/4000: Train Loss: 2840.476510387206 Test Loss: 3002.424716793124\n",
      "Epoch 1058/4000: Train Loss: 2840.4717199680076 Test Loss: 3002.4178762097613\n",
      "Epoch 1059/4000: Train Loss: 2840.466934586885 Test Loss: 3002.4110423643683\n",
      "Epoch 1060/4000: Train Loss: 2840.462154238539 Test Loss: 3002.4042152501397\n",
      "Epoch 1061/4000: Train Loss: 2840.457378917676 Test Loss: 3002.3973948602707\n",
      "Epoch 1062/4000: Train Loss: 2840.4526086190103 Test Loss: 3002.3905811879704\n",
      "Epoch 1063/4000: Train Loss: 2840.447843337258 Test Loss: 3002.383774226447\n",
      "Epoch 1064/4000: Train Loss: 2840.4430830671445 Test Loss: 3002.3769739689183\n",
      "Epoch 1065/4000: Train Loss: 2840.4383278033984 Test Loss: 3002.3701804086136\n",
      "Epoch 1066/4000: Train Loss: 2840.4335775407535 Test Loss: 3002.3633935387616\n",
      "Epoch 1067/4000: Train Loss: 2840.428832273952 Test Loss: 3002.3566133526024\n",
      "Epoch 1068/4000: Train Loss: 2840.4240919977387 Test Loss: 3002.3498398433817\n",
      "Epoch 1069/4000: Train Loss: 2840.419356706865 Test Loss: 3002.343073004352\n",
      "Epoch 1070/4000: Train Loss: 2840.414626396088 Test Loss: 3002.336312828771\n",
      "Epoch 1071/4000: Train Loss: 2840.4099010601703 Test Loss: 3002.3295593099074\n",
      "Epoch 1072/4000: Train Loss: 2840.4051806938796 Test Loss: 3002.322812441031\n",
      "Epoch 1073/4000: Train Loss: 2840.4004652919893 Test Loss: 3002.3160722154225\n",
      "Epoch 1074/4000: Train Loss: 2840.395754849279 Test Loss: 3002.3093386263677\n",
      "Epoch 1075/4000: Train Loss: 2840.3910493605326 Test Loss: 3002.3026116671595\n",
      "Epoch 1076/4000: Train Loss: 2840.3863488205398 Test Loss: 3002.295891331098\n",
      "Epoch 1077/4000: Train Loss: 2840.381653224096 Test Loss: 3002.28917761149\n",
      "Epoch 1078/4000: Train Loss: 2840.3769625660025 Test Loss: 3002.282470501645\n",
      "Epoch 1079/4000: Train Loss: 2840.3722768410657 Test Loss: 3002.2757699948866\n",
      "Epoch 1080/4000: Train Loss: 2840.367596044097 Test Loss: 3002.2690760845394\n",
      "Epoch 1081/4000: Train Loss: 2840.362920169913 Test Loss: 3002.2623887639365\n",
      "Epoch 1082/4000: Train Loss: 2840.3582492133382 Test Loss: 3002.2557080264182\n",
      "Epoch 1083/4000: Train Loss: 2840.3535831692 Test Loss: 3002.2490338653306\n",
      "Epoch 1084/4000: Train Loss: 2840.3489220323313 Test Loss: 3002.2423662740266\n",
      "Epoch 1085/4000: Train Loss: 2840.3442657975716 Test Loss: 3002.235705245865\n",
      "Epoch 1086/4000: Train Loss: 2840.339614459765 Test Loss: 3002.2290507742127\n",
      "Epoch 1087/4000: Train Loss: 2840.3349680137626 Test Loss: 3002.2224028524456\n",
      "Epoch 1088/4000: Train Loss: 2840.3303264544184 Test Loss: 3002.21576147394\n",
      "Epoch 1089/4000: Train Loss: 2840.325689776593 Test Loss: 3002.2091266320845\n",
      "Epoch 1090/4000: Train Loss: 2840.3210579751535 Test Loss: 3002.20249832027\n",
      "Epoch 1091/4000: Train Loss: 2840.316431044971 Test Loss: 3002.195876531896\n",
      "Epoch 1092/4000: Train Loss: 2840.3118089809227 Test Loss: 3002.189261260372\n",
      "Epoch 1093/4000: Train Loss: 2840.30719177789 Test Loss: 3002.1826524991066\n",
      "Epoch 1094/4000: Train Loss: 2840.3025794307614 Test Loss: 3002.176050241523\n",
      "Epoch 1095/4000: Train Loss: 2840.29797193443 Test Loss: 3002.169454481045\n",
      "Epoch 1096/4000: Train Loss: 2840.2933692837933 Test Loss: 3002.1628652111062\n",
      "Epoch 1097/4000: Train Loss: 2840.288771473756 Test Loss: 3002.1562824251464\n",
      "Epoch 1098/4000: Train Loss: 2840.2841784992265 Test Loss: 3002.1497061166097\n",
      "Epoch 1099/4000: Train Loss: 2840.27959035512 Test Loss: 3002.1431362789494\n",
      "Epoch 1100/4000: Train Loss: 2840.2750070363563 Test Loss: 3002.136572905625\n",
      "Epoch 1101/4000: Train Loss: 2840.2704285378604 Test Loss: 3002.130015990102\n",
      "Epoch 1102/4000: Train Loss: 2840.265854854563 Test Loss: 3002.1234655258536\n",
      "Epoch 1103/4000: Train Loss: 2840.261285981399 Test Loss: 3002.116921506356\n",
      "Epoch 1104/4000: Train Loss: 2840.2567219133107 Test Loss: 3002.1103839250964\n",
      "Epoch 1105/4000: Train Loss: 2840.2521626452444 Test Loss: 3002.103852775565\n",
      "Epoch 1106/4000: Train Loss: 2840.2476081721516 Test Loss: 3002.0973280512626\n",
      "Epoch 1107/4000: Train Loss: 2840.24305848899 Test Loss: 3002.0908097456927\n",
      "Epoch 1108/4000: Train Loss: 2840.238513590721 Test Loss: 3002.0842978523656\n",
      "Epoch 1109/4000: Train Loss: 2840.2339734723137 Test Loss: 3002.077792364802\n",
      "Epoch 1110/4000: Train Loss: 2840.22943812874 Test Loss: 3002.0712932765255\n",
      "Epoch 1111/4000: Train Loss: 2840.2249075549785 Test Loss: 3002.0648005810654\n",
      "Epoch 1112/4000: Train Loss: 2840.2203817460136 Test Loss: 3002.0583142719606\n",
      "Epoch 1113/4000: Train Loss: 2840.215860696833 Test Loss: 3002.0518343427552\n",
      "Epoch 1114/4000: Train Loss: 2840.211344402431 Test Loss: 3002.045360787001\n",
      "Epoch 1115/4000: Train Loss: 2840.2068328578075 Test Loss: 3002.0388935982514\n",
      "Epoch 1116/4000: Train Loss: 2840.2023260579676 Test Loss: 3002.032432770073\n",
      "Epoch 1117/4000: Train Loss: 2840.19782399792 Test Loss: 3002.025978296036\n",
      "Epoch 1118/4000: Train Loss: 2840.193326672681 Test Loss: 3002.0195301697127\n",
      "Epoch 1119/4000: Train Loss: 2840.18883407727 Test Loss: 3002.0130883846914\n",
      "Epoch 1120/4000: Train Loss: 2840.1843462067127 Test Loss: 3002.0066529345586\n",
      "Epoch 1121/4000: Train Loss: 2840.1798630560415 Test Loss: 3002.0002238129105\n",
      "Epoch 1122/4000: Train Loss: 2840.1753846202905 Test Loss: 3001.99380101335\n",
      "Epoch 1123/4000: Train Loss: 2840.170910894502 Test Loss: 3001.987384529486\n",
      "Epoch 1124/4000: Train Loss: 2840.166441873723 Test Loss: 3001.9809743549317\n",
      "Epoch 1125/4000: Train Loss: 2840.161977553005 Test Loss: 3001.9745704833113\n",
      "Epoch 1126/4000: Train Loss: 2840.157517927404 Test Loss: 3001.968172908251\n",
      "Epoch 1127/4000: Train Loss: 2840.153062991983 Test Loss: 3001.9617816233867\n",
      "Epoch 1128/4000: Train Loss: 2840.1486127418084 Test Loss: 3001.9553966223584\n",
      "Epoch 1129/4000: Train Loss: 2840.1441671719545 Test Loss: 3001.9490178988126\n",
      "Epoch 1130/4000: Train Loss: 2840.1397262774976 Test Loss: 3001.9426454464046\n",
      "Epoch 1131/4000: Train Loss: 2840.135290053521 Test Loss: 3001.936279258795\n",
      "Epoch 1132/4000: Train Loss: 2840.1308584951134 Test Loss: 3001.929919329647\n",
      "Epoch 1133/4000: Train Loss: 2840.1264315973663 Test Loss: 3001.923565652638\n",
      "Epoch 1134/4000: Train Loss: 2840.12200935538 Test Loss: 3001.917218221443\n",
      "Epoch 1135/4000: Train Loss: 2840.117591764257 Test Loss: 3001.9108770297503\n",
      "Epoch 1136/4000: Train Loss: 2840.1131788191065 Test Loss: 3001.904542071252\n",
      "Epoch 1137/4000: Train Loss: 2840.108770515041 Test Loss: 3001.8982133396444\n",
      "Epoch 1138/4000: Train Loss: 2840.1043668471816 Test Loss: 3001.891890828633\n",
      "Epoch 1139/4000: Train Loss: 2840.099967810651 Test Loss: 3001.8855745319306\n",
      "Epoch 1140/4000: Train Loss: 2840.0955734005784 Test Loss: 3001.879264443252\n",
      "Epoch 1141/4000: Train Loss: 2840.091183612099 Test Loss: 3001.8729605563244\n",
      "Epoch 1142/4000: Train Loss: 2840.0867984403517 Test Loss: 3001.8666628648743\n",
      "Epoch 1143/4000: Train Loss: 2840.082417880481 Test Loss: 3001.86037136264\n",
      "Epoch 1144/4000: Train Loss: 2840.078041927637 Test Loss: 3001.854086043362\n",
      "Epoch 1145/4000: Train Loss: 2840.0736705769746 Test Loss: 3001.8478069007933\n",
      "Epoch 1146/4000: Train Loss: 2840.0693038236527 Test Loss: 3001.841533928687\n",
      "Epoch 1147/4000: Train Loss: 2840.064941662837 Test Loss: 3001.835267120805\n",
      "Epoch 1148/4000: Train Loss: 2840.060584089698 Test Loss: 3001.829006470915\n",
      "Epoch 1149/4000: Train Loss: 2840.05623109941 Test Loss: 3001.822751972792\n",
      "Epoch 1150/4000: Train Loss: 2840.0518826871544 Test Loss: 3001.8165036202163\n",
      "Epoch 1151/4000: Train Loss: 2840.047538848115 Test Loss: 3001.8102614069735\n",
      "Epoch 1152/4000: Train Loss: 2840.043199577483 Test Loss: 3001.804025326859\n",
      "Epoch 1153/4000: Train Loss: 2840.0388648704534 Test Loss: 3001.7977953736695\n",
      "Epoch 1154/4000: Train Loss: 2840.0345347222274 Test Loss: 3001.791571541214\n",
      "Epoch 1155/4000: Train Loss: 2840.030209128009 Test Loss: 3001.785353823302\n",
      "Epoch 1156/4000: Train Loss: 2840.025888083011 Test Loss: 3001.779142213753\n",
      "Epoch 1157/4000: Train Loss: 2840.0215715824475 Test Loss: 3001.772936706388\n",
      "Epoch 1158/4000: Train Loss: 2840.017259621539 Test Loss: 3001.7667372950427\n",
      "Epoch 1159/4000: Train Loss: 2840.012952195512 Test Loss: 3001.760543973551\n",
      "Epoch 1160/4000: Train Loss: 2840.0086492995956 Test Loss: 3001.7543567357584\n",
      "Epoch 1161/4000: Train Loss: 2840.004350929028 Test Loss: 3001.7481755755102\n",
      "Epoch 1162/4000: Train Loss: 2840.0000570790476 Test Loss: 3001.7420004866653\n",
      "Epoch 1163/4000: Train Loss: 2839.9957677449024 Test Loss: 3001.735831463086\n",
      "Epoch 1164/4000: Train Loss: 2839.99148292184 Test Loss: 3001.7296684986377\n",
      "Epoch 1165/4000: Train Loss: 2839.9872026051185 Test Loss: 3001.723511587196\n",
      "Epoch 1166/4000: Train Loss: 2839.982926789998 Test Loss: 3001.717360722641\n",
      "Epoch 1167/4000: Train Loss: 2839.9786554717443 Test Loss: 3001.711215898859\n",
      "Epoch 1168/4000: Train Loss: 2839.9743886456276 Test Loss: 3001.7050771097433\n",
      "Epoch 1169/4000: Train Loss: 2839.9701263069246 Test Loss: 3001.6989443491925\n",
      "Epoch 1170/4000: Train Loss: 2839.9658684509136 Test Loss: 3001.6928176111137\n",
      "Epoch 1171/4000: Train Loss: 2839.961615072882 Test Loss: 3001.686696889415\n",
      "Epoch 1172/4000: Train Loss: 2839.9573661681206 Test Loss: 3001.6805821780144\n",
      "Epoch 1173/4000: Train Loss: 2839.9531217319236 Test Loss: 3001.6744734708377\n",
      "Epoch 1174/4000: Train Loss: 2839.9488817595925 Test Loss: 3001.6683707618135\n",
      "Epoch 1175/4000: Train Loss: 2839.9446462464316 Test Loss: 3001.662274044878\n",
      "Epoch 1176/4000: Train Loss: 2839.940415187752 Test Loss: 3001.656183313973\n",
      "Epoch 1177/4000: Train Loss: 2839.936188578869 Test Loss: 3001.650098563045\n",
      "Epoch 1178/4000: Train Loss: 2839.931966415102 Test Loss: 3001.6440197860506\n",
      "Epoch 1179/4000: Train Loss: 2839.927748691777 Test Loss: 3001.63794697695\n",
      "Epoch 1180/4000: Train Loss: 2839.923535404223 Test Loss: 3001.63188012971\n",
      "Epoch 1181/4000: Train Loss: 2839.919326547776 Test Loss: 3001.6258192383007\n",
      "Epoch 1182/4000: Train Loss: 2839.915122117775 Test Loss: 3001.6197642967045\n",
      "Epoch 1183/4000: Train Loss: 2839.9109221095646 Test Loss: 3001.613715298903\n",
      "Epoch 1184/4000: Train Loss: 2839.9067265184954 Test Loss: 3001.607672238888\n",
      "Epoch 1185/4000: Train Loss: 2839.9025353399206 Test Loss: 3001.6016351106596\n",
      "Epoch 1186/4000: Train Loss: 2839.898348569201 Test Loss: 3001.595603908217\n",
      "Epoch 1187/4000: Train Loss: 2839.8941662017 Test Loss: 3001.589578625571\n",
      "Epoch 1188/4000: Train Loss: 2839.8899882327864 Test Loss: 3001.5835592567364\n",
      "Epoch 1189/4000: Train Loss: 2839.885814657835 Test Loss: 3001.5775457957357\n",
      "Epoch 1190/4000: Train Loss: 2839.881645472224 Test Loss: 3001.571538236595\n",
      "Epoch 1191/4000: Train Loss: 2839.877480671338 Test Loss: 3001.565536573349\n",
      "Epoch 1192/4000: Train Loss: 2839.8733202505646 Test Loss: 3001.559540800035\n",
      "Epoch 1193/4000: Train Loss: 2839.869164205298 Test Loss: 3001.553550910702\n",
      "Epoch 1194/4000: Train Loss: 2839.865012530936 Test Loss: 3001.547566899399\n",
      "Epoch 1195/4000: Train Loss: 2839.8608652228822 Test Loss: 3001.541588760184\n",
      "Epoch 1196/4000: Train Loss: 2839.8567222765446 Test Loss: 3001.5356164871196\n",
      "Epoch 1197/4000: Train Loss: 2839.8525836873355 Test Loss: 3001.529650074278\n",
      "Epoch 1198/4000: Train Loss: 2839.848449450672 Test Loss: 3001.523689515733\n",
      "Epoch 1199/4000: Train Loss: 2839.844319561978 Test Loss: 3001.5177348055677\n",
      "Epoch 1200/4000: Train Loss: 2839.8401940166796 Test Loss: 3001.5117859378674\n",
      "Epoch 1201/4000: Train Loss: 2839.8360728102093 Test Loss: 3001.505842906729\n",
      "Epoch 1202/4000: Train Loss: 2839.8319559380043 Test Loss: 3001.4999057062505\n",
      "Epoch 1203/4000: Train Loss: 2839.8278433955056 Test Loss: 3001.4939743305367\n",
      "Epoch 1204/4000: Train Loss: 2839.82373517816 Test Loss: 3001.4880487736987\n",
      "Epoch 1205/4000: Train Loss: 2839.8196312814184 Test Loss: 3001.4821290298573\n",
      "Epoch 1206/4000: Train Loss: 2839.815531700737 Test Loss: 3001.4762150931338\n",
      "Epoch 1207/4000: Train Loss: 2839.811436431577 Test Loss: 3001.4703069576585\n",
      "Epoch 1208/4000: Train Loss: 2839.8073454694036 Test Loss: 3001.4644046175663\n",
      "Epoch 1209/4000: Train Loss: 2839.803258809687 Test Loss: 3001.4585080669995\n",
      "Epoch 1210/4000: Train Loss: 2839.799176447903 Test Loss: 3001.4526173001036\n",
      "Epoch 1211/4000: Train Loss: 2839.795098379531 Test Loss: 3001.4467323110334\n",
      "Epoch 1212/4000: Train Loss: 2839.791024600055 Test Loss: 3001.4408530939495\n",
      "Epoch 1213/4000: Train Loss: 2839.786955104966 Test Loss: 3001.4349796430142\n",
      "Epoch 1214/4000: Train Loss: 2839.7828898897565 Test Loss: 3001.4291119524005\n",
      "Epoch 1215/4000: Train Loss: 2839.778828949926 Test Loss: 3001.423250016285\n",
      "Epoch 1216/4000: Train Loss: 2839.774772280978 Test Loss: 3001.4173938288513\n",
      "Epoch 1217/4000: Train Loss: 2839.7707198784215 Test Loss: 3001.4115433842867\n",
      "Epoch 1218/4000: Train Loss: 2839.766671737768 Test Loss: 3001.4056986767873\n",
      "Epoch 1219/4000: Train Loss: 2839.7626278545367 Test Loss: 3001.3998597005534\n",
      "Epoch 1220/4000: Train Loss: 2839.7585882242497 Test Loss: 3001.39402644979\n",
      "Epoch 1221/4000: Train Loss: 2839.754552842434 Test Loss: 3001.388198918712\n",
      "Epoch 1222/4000: Train Loss: 2839.7505217046214 Test Loss: 3001.3823771015363\n",
      "Epoch 1223/4000: Train Loss: 2839.746494806349 Test Loss: 3001.376560992487\n",
      "Epoch 1224/4000: Train Loss: 2839.742472143157 Test Loss: 3001.370750585796\n",
      "Epoch 1225/4000: Train Loss: 2839.7384537105922 Test Loss: 3001.364945875697\n",
      "Epoch 1226/4000: Train Loss: 2839.7344395042064 Test Loss: 3001.359146856433\n",
      "Epoch 1227/4000: Train Loss: 2839.730429519552 Test Loss: 3001.3533535222514\n",
      "Epoch 1228/4000: Train Loss: 2839.726423752192 Test Loss: 3001.3475658674042\n",
      "Epoch 1229/4000: Train Loss: 2839.7224221976894 Test Loss: 3001.341783886153\n",
      "Epoch 1230/4000: Train Loss: 2839.7184248516137 Test Loss: 3001.3360075727596\n",
      "Epoch 1231/4000: Train Loss: 2839.7144317095394 Test Loss: 3001.3302369215007\n",
      "Epoch 1232/4000: Train Loss: 2839.7104427670447 Test Loss: 3001.3244719266477\n",
      "Epoch 1233/4000: Train Loss: 2839.706458019713 Test Loss: 3001.318712582486\n",
      "Epoch 1234/4000: Train Loss: 2839.7024774631327 Test Loss: 3001.3129588833026\n",
      "Epoch 1235/4000: Train Loss: 2839.698501092896 Test Loss: 3001.3072108233923\n",
      "Epoch 1236/4000: Train Loss: 2839.6945289046 Test Loss: 3001.3014683970537\n",
      "Epoch 1237/4000: Train Loss: 2839.690560893847 Test Loss: 3001.2957315985964\n",
      "Epoch 1238/4000: Train Loss: 2839.6865970562426 Test Loss: 3001.290000422328\n",
      "Epoch 1239/4000: Train Loss: 2839.682637387399 Test Loss: 3001.284274862568\n",
      "Epoch 1240/4000: Train Loss: 2839.678681882932 Test Loss: 3001.2785549136383\n",
      "Epoch 1241/4000: Train Loss: 2839.6747305384606 Test Loss: 3001.2728405698695\n",
      "Epoch 1242/4000: Train Loss: 2839.6707833496102 Test Loss: 3001.267131825594\n",
      "Epoch 1243/4000: Train Loss: 2839.6668403120116 Test Loss: 3001.2614286751545\n",
      "Epoch 1244/4000: Train Loss: 2839.6629014212976 Test Loss: 3001.2557311128976\n",
      "Epoch 1245/4000: Train Loss: 2839.658966673107 Test Loss: 3001.2500391331714\n",
      "Epoch 1246/4000: Train Loss: 2839.655036063084 Test Loss: 3001.2443527303394\n",
      "Epoch 1247/4000: Train Loss: 2839.651109586876 Test Loss: 3001.2386718987586\n",
      "Epoch 1248/4000: Train Loss: 2839.647187240135 Test Loss: 3001.232996632803\n",
      "Epoch 1249/4000: Train Loss: 2839.6432690185184 Test Loss: 3001.2273269268458\n",
      "Epoch 1250/4000: Train Loss: 2839.639354917688 Test Loss: 3001.2216627752687\n",
      "Epoch 1251/4000: Train Loss: 2839.63544493331 Test Loss: 3001.216004172456\n",
      "Epoch 1252/4000: Train Loss: 2839.631539061055 Test Loss: 3001.2103511128016\n",
      "Epoch 1253/4000: Train Loss: 2839.627637296598 Test Loss: 3001.204703590703\n",
      "Epoch 1254/4000: Train Loss: 2839.623739635619 Test Loss: 3001.199061600562\n",
      "Epoch 1255/4000: Train Loss: 2839.6198460738033 Test Loss: 3001.1934251367898\n",
      "Epoch 1256/4000: Train Loss: 2839.615956606838 Test Loss: 3001.1877941938005\n",
      "Epoch 1257/4000: Train Loss: 2839.612071230418 Test Loss: 3001.182168766015\n",
      "Epoch 1258/4000: Train Loss: 2839.6081899402407 Test Loss: 3001.1765488478595\n",
      "Epoch 1259/4000: Train Loss: 2839.6043127320095 Test Loss: 3001.170934433765\n",
      "Epoch 1260/4000: Train Loss: 2839.60043960143 Test Loss: 3001.16532551817\n",
      "Epoch 1261/4000: Train Loss: 2839.5965705442145 Test Loss: 3001.159722095517\n",
      "Epoch 1262/4000: Train Loss: 2839.592705556079 Test Loss: 3001.1541241602563\n",
      "Epoch 1263/4000: Train Loss: 2839.588844632744 Test Loss: 3001.1485317068395\n",
      "Epoch 1264/4000: Train Loss: 2839.5849877699347 Test Loss: 3001.14294472973\n",
      "Epoch 1265/4000: Train Loss: 2839.58113496338 Test Loss: 3001.137363223393\n",
      "Epoch 1266/4000: Train Loss: 2839.577286208815 Test Loss: 3001.131787182298\n",
      "Epoch 1267/4000: Train Loss: 2839.573441501978 Test Loss: 3001.126216600925\n",
      "Epoch 1268/4000: Train Loss: 2839.5696008386108 Test Loss: 3001.120651473754\n",
      "Epoch 1269/4000: Train Loss: 2839.5657642144624 Test Loss: 3001.115091795275\n",
      "Epoch 1270/4000: Train Loss: 2839.561931625284 Test Loss: 3001.1095375599803\n",
      "Epoch 1271/4000: Train Loss: 2839.5581030668322 Test Loss: 3001.103988762372\n",
      "Epoch 1272/4000: Train Loss: 2839.554278534868 Test Loss: 3001.0984453969522\n",
      "Epoch 1273/4000: Train Loss: 2839.5504580251563 Test Loss: 3001.0929074582336\n",
      "Epoch 1274/4000: Train Loss: 2839.5466415334677 Test Loss: 3001.087374940733\n",
      "Epoch 1275/4000: Train Loss: 2839.542829055576 Test Loss: 3001.081847838972\n",
      "Epoch 1276/4000: Train Loss: 2839.5390205872595 Test Loss: 3001.076326147477\n",
      "Epoch 1277/4000: Train Loss: 2839.5352161243013 Test Loss: 3001.07080986078\n",
      "Epoch 1278/4000: Train Loss: 2839.5314156624904 Test Loss: 3001.065298973424\n",
      "Epoch 1279/4000: Train Loss: 2839.5276191976177 Test Loss: 3001.0597934799493\n",
      "Epoch 1280/4000: Train Loss: 2839.523826725479 Test Loss: 3001.0542933749066\n",
      "Epoch 1281/4000: Train Loss: 2839.5200382418766 Test Loss: 3001.048798652853\n",
      "Epoch 1282/4000: Train Loss: 2839.5162537426145 Test Loss: 3001.0433093083475\n",
      "Epoch 1283/4000: Train Loss: 2839.5124732235035 Test Loss: 3001.0378253359554\n",
      "Epoch 1284/4000: Train Loss: 2839.508696680357 Test Loss: 3001.032346730253\n",
      "Epoch 1285/4000: Train Loss: 2839.5049241089923 Test Loss: 3001.026873485813\n",
      "Epoch 1286/4000: Train Loss: 2839.501155505235 Test Loss: 3001.0214055972206\n",
      "Epoch 1287/4000: Train Loss: 2839.4973908649104 Test Loss: 3001.0159430590647\n",
      "Epoch 1288/4000: Train Loss: 2839.4936301838507 Test Loss: 3001.0104858659392\n",
      "Epoch 1289/4000: Train Loss: 2839.4898734578924 Test Loss: 3001.005034012442\n",
      "Epoch 1290/4000: Train Loss: 2839.486120682875 Test Loss: 3000.999587493181\n",
      "Epoch 1291/4000: Train Loss: 2839.482371854644 Test Loss: 3000.9941463027662\n",
      "Epoch 1292/4000: Train Loss: 2839.478626969048 Test Loss: 3000.98871043581\n",
      "Epoch 1293/4000: Train Loss: 2839.4748860219415 Test Loss: 3000.9832798869384\n",
      "Epoch 1294/4000: Train Loss: 2839.4711490091813 Test Loss: 3000.9778546507755\n",
      "Epoch 1295/4000: Train Loss: 2839.4674159266306 Test Loss: 3000.9724347219553\n",
      "Epoch 1296/4000: Train Loss: 2839.463686770155 Test Loss: 3000.967020095117\n",
      "Epoch 1297/4000: Train Loss: 2839.4599615356265 Test Loss: 3000.961610764902\n",
      "Epoch 1298/4000: Train Loss: 2839.456240218919 Test Loss: 3000.956206725959\n",
      "Epoch 1299/4000: Train Loss: 2839.452522815914 Test Loss: 3000.9508079729435\n",
      "Epoch 1300/4000: Train Loss: 2839.4488093224945 Test Loss: 3000.945414500516\n",
      "Epoch 1301/4000: Train Loss: 2839.4450997345484 Test Loss: 3000.9400263033394\n",
      "Epoch 1302/4000: Train Loss: 2839.441394047969 Test Loss: 3000.9346433760875\n",
      "Epoch 1303/4000: Train Loss: 2839.437692258652 Test Loss: 3000.9292657134324\n",
      "Epoch 1304/4000: Train Loss: 2839.433994362501 Test Loss: 3000.923893310059\n",
      "Epoch 1305/4000: Train Loss: 2839.430300355419 Test Loss: 3000.9185261606535\n",
      "Epoch 1306/4000: Train Loss: 2839.426610233318 Test Loss: 3000.913164259909\n",
      "Epoch 1307/4000: Train Loss: 2839.422923992111 Test Loss: 3000.9078076025203\n",
      "Epoch 1308/4000: Train Loss: 2839.4192416277165 Test Loss: 3000.902456183195\n",
      "Epoch 1309/4000: Train Loss: 2839.4155631360572 Test Loss: 3000.8971099966393\n",
      "Epoch 1310/4000: Train Loss: 2839.4118885130606 Test Loss: 3000.8917690375656\n",
      "Epoch 1311/4000: Train Loss: 2839.4082177546584 Test Loss: 3000.8864333006977\n",
      "Epoch 1312/4000: Train Loss: 2839.4045508567856 Test Loss: 3000.8811027807574\n",
      "Epoch 1313/4000: Train Loss: 2839.400887815381 Test Loss: 3000.875777472474\n",
      "Epoch 1314/4000: Train Loss: 2839.3972286263906 Test Loss: 3000.8704573705863\n",
      "Epoch 1315/4000: Train Loss: 2839.3935732857626 Test Loss: 3000.865142469832\n",
      "Epoch 1316/4000: Train Loss: 2839.3899217894486 Test Loss: 3000.8598327649597\n",
      "Epoch 1317/4000: Train Loss: 2839.3862741334065 Test Loss: 3000.85452825072\n",
      "Epoch 1318/4000: Train Loss: 2839.382630313597 Test Loss: 3000.849228921871\n",
      "Epoch 1319/4000: Train Loss: 2839.3789903259853 Test Loss: 3000.8439347731737\n",
      "Epoch 1320/4000: Train Loss: 2839.3753541665424 Test Loss: 3000.8386457993975\n",
      "Epoch 1321/4000: Train Loss: 2839.37172183124 Test Loss: 3000.8333619953137\n",
      "Epoch 1322/4000: Train Loss: 2839.368093316059 Test Loss: 3000.828083355702\n",
      "Epoch 1323/4000: Train Loss: 2839.364468616979 Test Loss: 3000.8228098753443\n",
      "Epoch 1324/4000: Train Loss: 2839.360847729989 Test Loss: 3000.8175415490327\n",
      "Epoch 1325/4000: Train Loss: 2839.357230651078 Test Loss: 3000.8122783715585\n",
      "Epoch 1326/4000: Train Loss: 2839.3536173762427 Test Loss: 3000.8070203377238\n",
      "Epoch 1327/4000: Train Loss: 2839.350007901481 Test Loss: 3000.801767442333\n",
      "Epoch 1328/4000: Train Loss: 2839.346402222797 Test Loss: 3000.796519680195\n",
      "Epoch 1329/4000: Train Loss: 2839.3428003361983 Test Loss: 3000.791277046127\n",
      "Epoch 1330/4000: Train Loss: 2839.339202237696 Test Loss: 3000.786039534949\n",
      "Epoch 1331/4000: Train Loss: 2839.3356079233076 Test Loss: 3000.7808071414875\n",
      "Epoch 1332/4000: Train Loss: 2839.3320173890525 Test Loss: 3000.7755798605745\n",
      "Epoch 1333/4000: Train Loss: 2839.328430630955 Test Loss: 3000.770357687046\n",
      "Epoch 1334/4000: Train Loss: 2839.324847645044 Test Loss: 3000.7651406157433\n",
      "Epoch 1335/4000: Train Loss: 2839.321268427353 Test Loss: 3000.7599286415157\n",
      "Epoch 1336/4000: Train Loss: 2839.317692973918 Test Loss: 3000.7547217592155\n",
      "Epoch 1337/4000: Train Loss: 2839.31412128078 Test Loss: 3000.749519963698\n",
      "Epoch 1338/4000: Train Loss: 2839.310553343985 Test Loss: 3000.74432324983\n",
      "Epoch 1339/4000: Train Loss: 2839.306989159582 Test Loss: 3000.739131612476\n",
      "Epoch 1340/4000: Train Loss: 2839.3034287236246 Test Loss: 3000.733945046511\n",
      "Epoch 1341/4000: Train Loss: 2839.2998720321702 Test Loss: 3000.728763546815\n",
      "Epoch 1342/4000: Train Loss: 2839.2963190812825 Test Loss: 3000.7235871082703\n",
      "Epoch 1343/4000: Train Loss: 2839.292769867026 Test Loss: 3000.71841572577\n",
      "Epoch 1344/4000: Train Loss: 2839.289224385471 Test Loss: 3000.7132493942013\n",
      "Epoch 1345/4000: Train Loss: 2839.285682632692 Test Loss: 3000.708088108471\n",
      "Epoch 1346/4000: Train Loss: 2839.282144604768 Test Loss: 3000.702931863481\n",
      "Epoch 1347/4000: Train Loss: 2839.2786102977802 Test Loss: 3000.697780654139\n",
      "Epoch 1348/4000: Train Loss: 2839.2750797078165 Test Loss: 3000.692634475366\n",
      "Epoch 1349/4000: Train Loss: 2839.2715528309677 Test Loss: 3000.687493322077\n",
      "Epoch 1350/4000: Train Loss: 2839.2680296633275 Test Loss: 3000.6823571892\n",
      "Epoch 1351/4000: Train Loss: 2839.264510200997 Test Loss: 3000.677226071667\n",
      "Epoch 1352/4000: Train Loss: 2839.2609944400774 Test Loss: 3000.6720999644112\n",
      "Epoch 1353/4000: Train Loss: 2839.2574823766768 Test Loss: 3000.666978862375\n",
      "Epoch 1354/4000: Train Loss: 2839.2539740069064 Test Loss: 3000.661862760506\n",
      "Epoch 1355/4000: Train Loss: 2839.250469326882 Test Loss: 3000.6567516537552\n",
      "Epoch 1356/4000: Train Loss: 2839.246968332722 Test Loss: 3000.6516455370797\n",
      "Epoch 1357/4000: Train Loss: 2839.243471020552 Test Loss: 3000.6465444054406\n",
      "Epoch 1358/4000: Train Loss: 2839.2399773864972 Test Loss: 3000.6414482538034\n",
      "Epoch 1359/4000: Train Loss: 2839.236487426691 Test Loss: 3000.636357077142\n",
      "Epoch 1360/4000: Train Loss: 2839.2330011372683 Test Loss: 3000.6312708704354\n",
      "Epoch 1361/4000: Train Loss: 2839.2295185143703 Test Loss: 3000.626189628662\n",
      "Epoch 1362/4000: Train Loss: 2839.2260395541393 Test Loss: 3000.6211133468123\n",
      "Epoch 1363/4000: Train Loss: 2839.222564252724 Test Loss: 3000.616042019878\n",
      "Epoch 1364/4000: Train Loss: 2839.219092606277 Test Loss: 3000.6109756428564\n",
      "Epoch 1365/4000: Train Loss: 2839.215624610953 Test Loss: 3000.6059142107524\n",
      "Epoch 1366/4000: Train Loss: 2839.212160262913 Test Loss: 3000.6008577185717\n",
      "Epoch 1367/4000: Train Loss: 2839.2086995583213 Test Loss: 3000.5958061613264\n",
      "Epoch 1368/4000: Train Loss: 2839.205242493346 Test Loss: 3000.5907595340404\n",
      "Epoch 1369/4000: Train Loss: 2839.201789064159 Test Loss: 3000.585717831729\n",
      "Epoch 1370/4000: Train Loss: 2839.1983392669363 Test Loss: 3000.5806810494273\n",
      "Epoch 1371/4000: Train Loss: 2839.194893097859 Test Loss: 3000.5756491821658\n",
      "Epoch 1372/4000: Train Loss: 2839.1914505531113 Test Loss: 3000.570622224982\n",
      "Epoch 1373/4000: Train Loss: 2839.1880116288808 Test Loss: 3000.565600172924\n",
      "Epoch 1374/4000: Train Loss: 2839.18457632136 Test Loss: 3000.560583021034\n",
      "Epoch 1375/4000: Train Loss: 2839.181144626746 Test Loss: 3000.5555707643716\n",
      "Epoch 1376/4000: Train Loss: 2839.177716541238 Test Loss: 3000.550563397992\n",
      "Epoch 1377/4000: Train Loss: 2839.174292061041 Test Loss: 3000.5455609169608\n",
      "Epoch 1378/4000: Train Loss: 2839.1708711823635 Test Loss: 3000.540563316345\n",
      "Epoch 1379/4000: Train Loss: 2839.1674539014166 Test Loss: 3000.535570591221\n",
      "Epoch 1380/4000: Train Loss: 2839.1640402144185 Test Loss: 3000.530582736667\n",
      "Epoch 1381/4000: Train Loss: 2839.1606301175875 Test Loss: 3000.5255997477648\n",
      "Epoch 1382/4000: Train Loss: 2839.157223607149 Test Loss: 3000.520621619605\n",
      "Epoch 1383/4000: Train Loss: 2839.1538206793307 Test Loss: 3000.5156483472842\n",
      "Epoch 1384/4000: Train Loss: 2839.1504213303647 Test Loss: 3000.5106799258983\n",
      "Epoch 1385/4000: Train Loss: 2839.1470255564877 Test Loss: 3000.5057163505517\n",
      "Epoch 1386/4000: Train Loss: 2839.1436333539395 Test Loss: 3000.5007576163534\n",
      "Epoch 1387/4000: Train Loss: 2839.140244718964 Test Loss: 3000.495803718418\n",
      "Epoch 1388/4000: Train Loss: 2839.136859647809 Test Loss: 3000.4908546518664\n",
      "Epoch 1389/4000: Train Loss: 2839.1334781367273 Test Loss: 3000.48591041182\n",
      "Epoch 1390/4000: Train Loss: 2839.130100181974 Test Loss: 3000.480970993409\n",
      "Epoch 1391/4000: Train Loss: 2839.1267257798086 Test Loss: 3000.4760363917667\n",
      "Epoch 1392/4000: Train Loss: 2839.1233549264957 Test Loss: 3000.4711066020327\n",
      "Epoch 1393/4000: Train Loss: 2839.119987618303 Test Loss: 3000.46618161935\n",
      "Epoch 1394/4000: Train Loss: 2839.116623851501 Test Loss: 3000.46126143887\n",
      "Epoch 1395/4000: Train Loss: 2839.1132636223665 Test Loss: 3000.4563460557456\n",
      "Epoch 1396/4000: Train Loss: 2839.1099069271786 Test Loss: 3000.4514354651337\n",
      "Epoch 1397/4000: Train Loss: 2839.10655376222 Test Loss: 3000.446529662201\n",
      "Epoch 1398/4000: Train Loss: 2839.1032041237786 Test Loss: 3000.441628642113\n",
      "Epoch 1399/4000: Train Loss: 2839.099858008146 Test Loss: 3000.436732400047\n",
      "Epoch 1400/4000: Train Loss: 2839.096515411615 Test Loss: 3000.431840931178\n",
      "Epoch 1401/4000: Train Loss: 2839.093176330488 Test Loss: 3000.426954230693\n",
      "Epoch 1402/4000: Train Loss: 2839.0898407610653 Test Loss: 3000.422072293779\n",
      "Epoch 1403/4000: Train Loss: 2839.0865086996546 Test Loss: 3000.41719511563\n",
      "Epoch 1404/4000: Train Loss: 2839.0831801425657 Test Loss: 3000.412322691443\n",
      "Epoch 1405/4000: Train Loss: 2839.079855086115 Test Loss: 3000.407455016422\n",
      "Epoch 1406/4000: Train Loss: 2839.076533526619 Test Loss: 3000.402592085775\n",
      "Epoch 1407/4000: Train Loss: 2839.0732154604007 Test Loss: 3000.397733894717\n",
      "Epoch 1408/4000: Train Loss: 2839.069900883786 Test Loss: 3000.3928804384636\n",
      "Epoch 1409/4000: Train Loss: 2839.0665897931053 Test Loss: 3000.3880317122366\n",
      "Epoch 1410/4000: Train Loss: 2839.0632821846916 Test Loss: 3000.3831877112666\n",
      "Epoch 1411/4000: Train Loss: 2839.0599780548837 Test Loss: 3000.3783484307864\n",
      "Epoch 1412/4000: Train Loss: 2839.056677400023 Test Loss: 3000.3735138660313\n",
      "Epoch 1413/4000: Train Loss: 2839.053380216454 Test Loss: 3000.368684012245\n",
      "Epoch 1414/4000: Train Loss: 2839.050086500527 Test Loss: 3000.3638588646736\n",
      "Epoch 1415/4000: Train Loss: 2839.046796248594 Test Loss: 3000.35903841857\n",
      "Epoch 1416/4000: Train Loss: 2839.043509457013 Test Loss: 3000.354222669191\n",
      "Epoch 1417/4000: Train Loss: 2839.0402261221434 Test Loss: 3000.3494116118\n",
      "Epoch 1418/4000: Train Loss: 2839.036946240352 Test Loss: 3000.3446052416625\n",
      "Epoch 1419/4000: Train Loss: 2839.033669808005 Test Loss: 3000.339803554048\n",
      "Epoch 1420/4000: Train Loss: 2839.0303968214757 Test Loss: 3000.335006544234\n",
      "Epoch 1421/4000: Train Loss: 2839.0271272771397 Test Loss: 3000.3302142075045\n",
      "Epoch 1422/4000: Train Loss: 2839.0238611713776 Test Loss: 3000.325426539143\n",
      "Epoch 1423/4000: Train Loss: 2839.020598500572 Test Loss: 3000.32064353444\n",
      "Epoch 1424/4000: Train Loss: 2839.017339261111 Test Loss: 3000.3158651886924\n",
      "Epoch 1425/4000: Train Loss: 2839.014083449386 Test Loss: 3000.3110914971994\n",
      "Epoch 1426/4000: Train Loss: 2839.0108310617916 Test Loss: 3000.306322455267\n",
      "Epoch 1427/4000: Train Loss: 2839.0075820947272 Test Loss: 3000.3015580582064\n",
      "Epoch 1428/4000: Train Loss: 2839.0043365445945 Test Loss: 3000.29679830133\n",
      "Epoch 1429/4000: Train Loss: 2839.0010944078012 Test Loss: 3000.292043179961\n",
      "Epoch 1430/4000: Train Loss: 2838.9978556807564 Test Loss: 3000.2872926894206\n",
      "Epoch 1431/4000: Train Loss: 2838.9946203598747 Test Loss: 3000.282546825039\n",
      "Epoch 1432/4000: Train Loss: 2838.9913884415732 Test Loss: 3000.27780558215\n",
      "Epoch 1433/4000: Train Loss: 2838.9881599222745 Test Loss: 3000.2730689560935\n",
      "Epoch 1434/4000: Train Loss: 2838.984934798403 Test Loss: 3000.268336942213\n",
      "Epoch 1435/4000: Train Loss: 2838.981713066388 Test Loss: 3000.2636095358557\n",
      "Epoch 1436/4000: Train Loss: 2838.978494722662 Test Loss: 3000.2588867323757\n",
      "Epoch 1437/4000: Train Loss: 2838.975279763662 Test Loss: 3000.25416852713\n",
      "Epoch 1438/4000: Train Loss: 2838.972068185828 Test Loss: 3000.249454915482\n",
      "Epoch 1439/4000: Train Loss: 2838.968859985604 Test Loss: 3000.244745892799\n",
      "Epoch 1440/4000: Train Loss: 2838.965655159439 Test Loss: 3000.240041454453\n",
      "Epoch 1441/4000: Train Loss: 2838.9624537037816 Test Loss: 3000.235341595821\n",
      "Epoch 1442/4000: Train Loss: 2838.9592556150897 Test Loss: 3000.2306463122827\n",
      "Epoch 1443/4000: Train Loss: 2838.956060889822 Test Loss: 3000.225955599228\n",
      "Epoch 1444/4000: Train Loss: 2838.95286952444 Test Loss: 3000.2212694520467\n",
      "Epoch 1445/4000: Train Loss: 2838.9496815154102 Test Loss: 3000.2165878661326\n",
      "Epoch 1446/4000: Train Loss: 2838.946496859204 Test Loss: 3000.211910836887\n",
      "Epoch 1447/4000: Train Loss: 2838.9433155522943 Test Loss: 3000.207238359717\n",
      "Epoch 1448/4000: Train Loss: 2838.940137591159 Test Loss: 3000.2025704300295\n",
      "Epoch 1449/4000: Train Loss: 2838.936962972279 Test Loss: 3000.1979070432412\n",
      "Epoch 1450/4000: Train Loss: 2838.9337916921395 Test Loss: 3000.19324819477\n",
      "Epoch 1451/4000: Train Loss: 2838.930623747229 Test Loss: 3000.188593880043\n",
      "Epoch 1452/4000: Train Loss: 2838.9274591340404 Test Loss: 3000.183944094485\n",
      "Epoch 1453/4000: Train Loss: 2838.924297849069 Test Loss: 3000.179298833531\n",
      "Epoch 1454/4000: Train Loss: 2838.921139888815 Test Loss: 3000.1746580926188\n",
      "Epoch 1455/4000: Train Loss: 2838.9179852497814 Test Loss: 3000.170021867191\n",
      "Epoch 1456/4000: Train Loss: 2838.914833928475 Test Loss: 3000.165390152696\n",
      "Epoch 1457/4000: Train Loss: 2838.9116859214078 Test Loss: 3000.1607629445834\n",
      "Epoch 1458/4000: Train Loss: 2838.9085412250934 Test Loss: 3000.1561402383113\n",
      "Epoch 1459/4000: Train Loss: 2838.9053998360496 Test Loss: 3000.1515220293422\n",
      "Epoch 1460/4000: Train Loss: 2838.902261750799 Test Loss: 3000.14690831314\n",
      "Epoch 1461/4000: Train Loss: 2838.8991269658663 Test Loss: 3000.142299085178\n",
      "Epoch 1462/4000: Train Loss: 2838.8959954777806 Test Loss: 3000.137694340929\n",
      "Epoch 1463/4000: Train Loss: 2838.8928672830752 Test Loss: 3000.1330940758717\n",
      "Epoch 1464/4000: Train Loss: 2838.8897423782855 Test Loss: 3000.1284982854945\n",
      "Epoch 1465/4000: Train Loss: 2838.886620759952 Test Loss: 3000.1239069652834\n",
      "Epoch 1466/4000: Train Loss: 2838.883502424619 Test Loss: 3000.119320110735\n",
      "Epoch 1467/4000: Train Loss: 2838.880387368833 Test Loss: 3000.1147377173434\n",
      "Epoch 1468/4000: Train Loss: 2838.877275589145 Test Loss: 3000.1101597806155\n",
      "Epoch 1469/4000: Train Loss: 2838.874167082109 Test Loss: 3000.1055862960566\n",
      "Epoch 1470/4000: Train Loss: 2838.8710618442847 Test Loss: 3000.1010172591814\n",
      "Epoch 1471/4000: Train Loss: 2838.867959872232 Test Loss: 3000.096452665503\n",
      "Epoch 1472/4000: Train Loss: 2838.864861162518 Test Loss: 3000.091892510547\n",
      "Epoch 1473/4000: Train Loss: 2838.861765711711 Test Loss: 3000.087336789836\n",
      "Epoch 1474/4000: Train Loss: 2838.858673516383 Test Loss: 3000.082785498902\n",
      "Epoch 1475/4000: Train Loss: 2838.855584573111 Test Loss: 3000.078238633279\n",
      "Epoch 1476/4000: Train Loss: 2838.8524988784748 Test Loss: 3000.073696188508\n",
      "Epoch 1477/4000: Train Loss: 2838.8494164290573 Test Loss: 3000.0691581601322\n",
      "Epoch 1478/4000: Train Loss: 2838.846337221446 Test Loss: 3000.064624543703\n",
      "Epoch 1479/4000: Train Loss: 2838.843261252231 Test Loss: 3000.0600953347694\n",
      "Epoch 1480/4000: Train Loss: 2838.8401885180074 Test Loss: 3000.055570528893\n",
      "Epoch 1481/4000: Train Loss: 2838.837119015372 Test Loss: 3000.0510501216336\n",
      "Epoch 1482/4000: Train Loss: 2838.8340527409264 Test Loss: 3000.046534108561\n",
      "Epoch 1483/4000: Train Loss: 2838.8309896912756 Test Loss: 3000.0420224852455\n",
      "Epoch 1484/4000: Train Loss: 2838.8279298630287 Test Loss: 3000.0375152472616\n",
      "Epoch 1485/4000: Train Loss: 2838.8248732527964 Test Loss: 3000.033012390194\n",
      "Epoch 1486/4000: Train Loss: 2838.8218198571954 Test Loss: 3000.0285139096222\n",
      "Epoch 1487/4000: Train Loss: 2838.8187696728446 Test Loss: 3000.02401980114\n",
      "Epoch 1488/4000: Train Loss: 2838.815722696367 Test Loss: 3000.019530060342\n",
      "Epoch 1489/4000: Train Loss: 2838.8126789243884 Test Loss: 3000.0150446828247\n",
      "Epoch 1490/4000: Train Loss: 2838.8096383535385 Test Loss: 3000.0105636641924\n",
      "Epoch 1491/4000: Train Loss: 2838.8066009804506 Test Loss: 3000.0060870000525\n",
      "Epoch 1492/4000: Train Loss: 2838.8035668017633 Test Loss: 3000.0016146860175\n",
      "Epoch 1493/4000: Train Loss: 2838.800535814115 Test Loss: 2999.997146717703\n",
      "Epoch 1494/4000: Train Loss: 2838.797508014151 Test Loss: 2999.9926830907307\n",
      "Epoch 1495/4000: Train Loss: 2838.794483398518 Test Loss: 2999.9882238007276\n",
      "Epoch 1496/4000: Train Loss: 2838.791461963867 Test Loss: 2999.983768843322\n",
      "Epoch 1497/4000: Train Loss: 2838.788443706853 Test Loss: 2999.979318214151\n",
      "Epoch 1498/4000: Train Loss: 2838.785428624134 Test Loss: 2999.9748719088516\n",
      "Epoch 1499/4000: Train Loss: 2838.782416712372 Test Loss: 2999.9704299230675\n",
      "Epoch 1500/4000: Train Loss: 2838.779407968231 Test Loss: 2999.9659922524484\n",
      "Epoch 1501/4000: Train Loss: 2838.7764023883815 Test Loss: 2999.961558892646\n",
      "Epoch 1502/4000: Train Loss: 2838.7733999694933 Test Loss: 2999.9571298393157\n",
      "Epoch 1503/4000: Train Loss: 2838.770400708244 Test Loss: 2999.95270508812\n",
      "Epoch 1504/4000: Train Loss: 2838.7674046013112 Test Loss: 2999.948284634727\n",
      "Epoch 1505/4000: Train Loss: 2838.764411645379 Test Loss: 2999.943868474803\n",
      "Epoch 1506/4000: Train Loss: 2838.7614218371314 Test Loss: 2999.939456604027\n",
      "Epoch 1507/4000: Train Loss: 2838.75843517326 Test Loss: 2999.9350490180723\n",
      "Epoch 1508/4000: Train Loss: 2838.7554516504574 Test Loss: 2999.93064571263\n",
      "Epoch 1509/4000: Train Loss: 2838.7524712654194 Test Loss: 2999.9262466833816\n",
      "Epoch 1510/4000: Train Loss: 2838.7494940148467 Test Loss: 2999.921851926023\n",
      "Epoch 1511/4000: Train Loss: 2838.746519895443 Test Loss: 2999.91746143625\n",
      "Epoch 1512/4000: Train Loss: 2838.7435489039144 Test Loss: 2999.9130752097653\n",
      "Epoch 1513/4000: Train Loss: 2838.740581036972 Test Loss: 2999.9086932422724\n",
      "Epoch 1514/4000: Train Loss: 2838.7376162913292 Test Loss: 2999.9043155294826\n",
      "Epoch 1515/4000: Train Loss: 2838.7346546637036 Test Loss: 2999.8999420671103\n",
      "Epoch 1516/4000: Train Loss: 2838.731696150816 Test Loss: 2999.8955728508736\n",
      "Epoch 1517/4000: Train Loss: 2838.728740749391 Test Loss: 2999.891207876496\n",
      "Epoch 1518/4000: Train Loss: 2838.725788456156 Test Loss: 2999.886847139707\n",
      "Epoch 1519/4000: Train Loss: 2838.7228392678408 Test Loss: 2999.8824906362356\n",
      "Epoch 1520/4000: Train Loss: 2838.7198931811818 Test Loss: 2999.8781383618198\n",
      "Epoch 1521/4000: Train Loss: 2838.7169501929166 Test Loss: 2999.8737903121996\n",
      "Epoch 1522/4000: Train Loss: 2838.714010299787 Test Loss: 2999.869446483122\n",
      "Epoch 1523/4000: Train Loss: 2838.7110734985363 Test Loss: 2999.8651068703334\n",
      "Epoch 1524/4000: Train Loss: 2838.7081397859138 Test Loss: 2999.8607714695913\n",
      "Epoch 1525/4000: Train Loss: 2838.705209158671 Test Loss: 2999.85644027665\n",
      "Epoch 1526/4000: Train Loss: 2838.7022816135636 Test Loss: 2999.8521132872747\n",
      "Epoch 1527/4000: Train Loss: 2838.6993571473495 Test Loss: 2999.8477904972324\n",
      "Epoch 1528/4000: Train Loss: 2838.696435756791 Test Loss: 2999.8434719022916\n",
      "Epoch 1529/4000: Train Loss: 2838.6935174386526 Test Loss: 2999.83915749823\n",
      "Epoch 1530/4000: Train Loss: 2838.6906021897044 Test Loss: 2999.8348472808275\n",
      "Epoch 1531/4000: Train Loss: 2838.6876900067173 Test Loss: 2999.8305412458676\n",
      "Epoch 1532/4000: Train Loss: 2838.6847808864673 Test Loss: 2999.8262393891387\n",
      "Epoch 1533/4000: Train Loss: 2838.6818748257333 Test Loss: 2999.8219417064347\n",
      "Epoch 1534/4000: Train Loss: 2838.6789718212985 Test Loss: 2999.817648193551\n",
      "Epoch 1535/4000: Train Loss: 2838.676071869947 Test Loss: 2999.8133588462915\n",
      "Epoch 1536/4000: Train Loss: 2838.6731749684686 Test Loss: 2999.809073660458\n",
      "Epoch 1537/4000: Train Loss: 2838.6702811136565 Test Loss: 2999.8047926318654\n",
      "Epoch 1538/4000: Train Loss: 2838.6673903023056 Test Loss: 2999.800515756325\n",
      "Epoch 1539/4000: Train Loss: 2838.6645025312146 Test Loss: 2999.7962430296557\n",
      "Epoch 1540/4000: Train Loss: 2838.6616177971873 Test Loss: 2999.791974447681\n",
      "Epoch 1541/4000: Train Loss: 2838.6587360970298 Test Loss: 2999.787710006226\n",
      "Epoch 1542/4000: Train Loss: 2838.65585742755 Test Loss: 2999.783449701126\n",
      "Epoch 1543/4000: Train Loss: 2838.652981785561 Test Loss: 2999.7791935282144\n",
      "Epoch 1544/4000: Train Loss: 2838.6501091678797 Test Loss: 2999.774941483332\n",
      "Epoch 1545/4000: Train Loss: 2838.647239571325 Test Loss: 2999.7706935623214\n",
      "Epoch 1546/4000: Train Loss: 2838.644372992718 Test Loss: 2999.7664497610326\n",
      "Epoch 1547/4000: Train Loss: 2838.641509428887 Test Loss: 2999.7622100753183\n",
      "Epoch 1548/4000: Train Loss: 2838.6386488766607 Test Loss: 2999.757974501034\n",
      "Epoch 1549/4000: Train Loss: 2838.6357913328716 Test Loss: 2999.7537430340444\n",
      "Epoch 1550/4000: Train Loss: 2838.632936794355 Test Loss: 2999.7495156702125\n",
      "Epoch 1551/4000: Train Loss: 2838.630085257951 Test Loss: 2999.7452924054073\n",
      "Epoch 1552/4000: Train Loss: 2838.627236720503 Test Loss: 2999.741073235505\n",
      "Epoch 1553/4000: Train Loss: 2838.6243911788556 Test Loss: 2999.736858156382\n",
      "Epoch 1554/4000: Train Loss: 2838.6215486298593 Test Loss: 2999.732647163922\n",
      "Epoch 1555/4000: Train Loss: 2838.618709070366 Test Loss: 2999.7284402540104\n",
      "Epoch 1556/4000: Train Loss: 2838.615872497232 Test Loss: 2999.7242374225398\n",
      "Epoch 1557/4000: Train Loss: 2838.6130389073155 Test Loss: 2999.720038665404\n",
      "Epoch 1558/4000: Train Loss: 2838.610208297481 Test Loss: 2999.7158439785017\n",
      "Epoch 1559/4000: Train Loss: 2838.607380664593 Test Loss: 2999.7116533577373\n",
      "Epoch 1560/4000: Train Loss: 2838.6045560055213 Test Loss: 2999.707466799019\n",
      "Epoch 1561/4000: Train Loss: 2838.6017343171375 Test Loss: 2999.703284298258\n",
      "Epoch 1562/4000: Train Loss: 2838.598915596318 Test Loss: 2999.6991058513713\n",
      "Epoch 1563/4000: Train Loss: 2838.596099839942 Test Loss: 2999.6949314542785\n",
      "Epoch 1564/4000: Train Loss: 2838.593287044891 Test Loss: 2999.6907611029023\n",
      "Epoch 1565/4000: Train Loss: 2838.590477208051 Test Loss: 2999.6865947931747\n",
      "Epoch 1566/4000: Train Loss: 2838.5876703263107 Test Loss: 2999.682432521025\n",
      "Epoch 1567/4000: Train Loss: 2838.5848663965635 Test Loss: 2999.678274282392\n",
      "Epoch 1568/4000: Train Loss: 2838.582065415703 Test Loss: 2999.674120073218\n",
      "Epoch 1569/4000: Train Loss: 2838.579267380628 Test Loss: 2999.6699698894463\n",
      "Epoch 1570/4000: Train Loss: 2838.576472288242 Test Loss: 2999.665823727027\n",
      "Epoch 1571/4000: Train Loss: 2838.5736801354487 Test Loss: 2999.6616815819148\n",
      "Epoch 1572/4000: Train Loss: 2838.570890919157 Test Loss: 2999.6575434500646\n",
      "Epoch 1573/4000: Train Loss: 2838.568104636279 Test Loss: 2999.6534093274413\n",
      "Epoch 1574/4000: Train Loss: 2838.5653212837287 Test Loss: 2999.6492792100084\n",
      "Epoch 1575/4000: Train Loss: 2838.5625408584247 Test Loss: 2999.645153093738\n",
      "Epoch 1576/4000: Train Loss: 2838.5597633572893 Test Loss: 2999.641030974604\n",
      "Epoch 1577/4000: Train Loss: 2838.556988777246 Test Loss: 2999.636912848585\n",
      "Epoch 1578/4000: Train Loss: 2838.5542171152233 Test Loss: 2999.632798711662\n",
      "Epoch 1579/4000: Train Loss: 2838.5514483681523 Test Loss: 2999.6286885598224\n",
      "Epoch 1580/4000: Train Loss: 2838.5486825329663 Test Loss: 2999.6245823890577\n",
      "Epoch 1581/4000: Train Loss: 2838.545919606604 Test Loss: 2999.620480195363\n",
      "Epoch 1582/4000: Train Loss: 2838.543159586007 Test Loss: 2999.6163819747358\n",
      "Epoch 1583/4000: Train Loss: 2838.5404024681166 Test Loss: 2999.6122877231815\n",
      "Epoch 1584/4000: Train Loss: 2838.5376482498827 Test Loss: 2999.6081974367044\n",
      "Epoch 1585/4000: Train Loss: 2838.534896928255 Test Loss: 2999.6041111113163\n",
      "Epoch 1586/4000: Train Loss: 2838.5321485001864 Test Loss: 2999.600028743034\n",
      "Epoch 1587/4000: Train Loss: 2838.529402962634 Test Loss: 2999.5959503278764\n",
      "Epoch 1588/4000: Train Loss: 2838.5266603125583 Test Loss: 2999.5918758618673\n",
      "Epoch 1589/4000: Train Loss: 2838.5239205469215 Test Loss: 2999.5878053410343\n",
      "Epoch 1590/4000: Train Loss: 2838.5211836626927 Test Loss: 2999.5837387614083\n",
      "Epoch 1591/4000: Train Loss: 2838.5184496568386 Test Loss: 2999.5796761190254\n",
      "Epoch 1592/4000: Train Loss: 2838.5157185263333 Test Loss: 2999.5756174099242\n",
      "Epoch 1593/4000: Train Loss: 2838.512990268152 Test Loss: 2999.571562630152\n",
      "Epoch 1594/4000: Train Loss: 2838.510264879275 Test Loss: 2999.5675117757532\n",
      "Epoch 1595/4000: Train Loss: 2838.5075423566836 Test Loss: 2999.5634648427817\n",
      "Epoch 1596/4000: Train Loss: 2838.5048226973645 Test Loss: 2999.5594218272918\n",
      "Epoch 1597/4000: Train Loss: 2838.502105898305 Test Loss: 2999.5553827253466\n",
      "Epoch 1598/4000: Train Loss: 2838.4993919564995 Test Loss: 2999.551347533006\n",
      "Epoch 1599/4000: Train Loss: 2838.49668086894 Test Loss: 2999.5473162463413\n",
      "Epoch 1600/4000: Train Loss: 2838.4939726326256 Test Loss: 2999.5432888614246\n",
      "Epoch 1601/4000: Train Loss: 2838.491267244559 Test Loss: 2999.53926537433\n",
      "Epoch 1602/4000: Train Loss: 2838.4885647017427 Test Loss: 2999.535245781139\n",
      "Epoch 1603/4000: Train Loss: 2838.485865001186 Test Loss: 2999.531230077934\n",
      "Epoch 1604/4000: Train Loss: 2838.483168139899 Test Loss: 2999.5272182608082\n",
      "Epoch 1605/4000: Train Loss: 2838.4804741148964 Test Loss: 2999.523210325848\n",
      "Epoch 1606/4000: Train Loss: 2838.4777829231934 Test Loss: 2999.519206269154\n",
      "Epoch 1607/4000: Train Loss: 2838.475094561812 Test Loss: 2999.5152060868245\n",
      "Epoch 1608/4000: Train Loss: 2838.472409027775 Test Loss: 2999.5112097749634\n",
      "Epoch 1609/4000: Train Loss: 2838.4697263181088 Test Loss: 2999.5072173296794\n",
      "Epoch 1610/4000: Train Loss: 2838.4670464298433 Test Loss: 2999.5032287470854\n",
      "Epoch 1611/4000: Train Loss: 2838.4643693600106 Test Loss: 2999.4992440232963\n",
      "Epoch 1612/4000: Train Loss: 2838.4616951056473 Test Loss: 2999.4952631544325\n",
      "Epoch 1613/4000: Train Loss: 2838.4590236637923 Test Loss: 2999.491286136619\n",
      "Epoch 1614/4000: Train Loss: 2838.4563550314874 Test Loss: 2999.4873129659845\n",
      "Epoch 1615/4000: Train Loss: 2838.4536892057777 Test Loss: 2999.48334363866\n",
      "Epoch 1616/4000: Train Loss: 2838.4510261837113 Test Loss: 2999.47937815078\n",
      "Epoch 1617/4000: Train Loss: 2838.448365962341 Test Loss: 2999.475416498488\n",
      "Epoch 1618/4000: Train Loss: 2838.4457085387194 Test Loss: 2999.4714586779255\n",
      "Epoch 1619/4000: Train Loss: 2838.4430539099053 Test Loss: 2999.4675046852412\n",
      "Epoch 1620/4000: Train Loss: 2838.440402072959 Test Loss: 2999.4635545165875\n",
      "Epoch 1621/4000: Train Loss: 2838.4377530249444 Test Loss: 2999.4596081681207\n",
      "Epoch 1622/4000: Train Loss: 2838.4351067629286 Test Loss: 2999.4556656359987\n",
      "Epoch 1623/4000: Train Loss: 2838.432463283981 Test Loss: 2999.451726916386\n",
      "Epoch 1624/4000: Train Loss: 2838.4298225851753 Test Loss: 2999.447792005451\n",
      "Epoch 1625/4000: Train Loss: 2838.427184663588 Test Loss: 2999.4438608993646\n",
      "Epoch 1626/4000: Train Loss: 2838.424549516297 Test Loss: 2999.439933594302\n",
      "Epoch 1627/4000: Train Loss: 2838.421917140385 Test Loss: 2999.4360100864433\n",
      "Epoch 1628/4000: Train Loss: 2838.419287532938 Test Loss: 2999.432090371971\n",
      "Epoch 1629/4000: Train Loss: 2838.416660691044 Test Loss: 2999.4281744470736\n",
      "Epoch 1630/4000: Train Loss: 2838.414036611794 Test Loss: 2999.4242623079413\n",
      "Epoch 1631/4000: Train Loss: 2838.4114152922834 Test Loss: 2999.4203539507703\n",
      "Epoch 1632/4000: Train Loss: 2838.408796729609 Test Loss: 2999.416449371757\n",
      "Epoch 1633/4000: Train Loss: 2838.406180920873 Test Loss: 2999.4125485671075\n",
      "Epoch 1634/4000: Train Loss: 2838.4035678631767 Test Loss: 2999.408651533026\n",
      "Epoch 1635/4000: Train Loss: 2838.4009575536284 Test Loss: 2999.4047582657245\n",
      "Epoch 1636/4000: Train Loss: 2838.398349989338 Test Loss: 2999.400868761416\n",
      "Epoch 1637/4000: Train Loss: 2838.3957451674173 Test Loss: 2999.3969830163214\n",
      "Epoch 1638/4000: Train Loss: 2838.393143084983 Test Loss: 2999.3931010266615\n",
      "Epoch 1639/4000: Train Loss: 2838.390543739153 Test Loss: 2999.3892227886627\n",
      "Epoch 1640/4000: Train Loss: 2838.38794712705 Test Loss: 2999.3853482985546\n",
      "Epoch 1641/4000: Train Loss: 2838.3853532457997 Test Loss: 2999.381477552571\n",
      "Epoch 1642/4000: Train Loss: 2838.3827620925285 Test Loss: 2999.377610546951\n",
      "Epoch 1643/4000: Train Loss: 2838.380173664369 Test Loss: 2999.3737472779335\n",
      "Epoch 1644/4000: Train Loss: 2838.377587958454 Test Loss: 2999.3698877417664\n",
      "Epoch 1645/4000: Train Loss: 2838.3750049719206 Test Loss: 2999.366031934699\n",
      "Epoch 1646/4000: Train Loss: 2838.3724247019086 Test Loss: 2999.362179852984\n",
      "Epoch 1647/4000: Train Loss: 2838.3698471455623 Test Loss: 2999.358331492876\n",
      "Epoch 1648/4000: Train Loss: 2838.367272300027 Test Loss: 2999.354486850639\n",
      "Epoch 1649/4000: Train Loss: 2838.364700162451 Test Loss: 2999.3506459225373\n",
      "Epoch 1650/4000: Train Loss: 2838.3621307299873 Test Loss: 2999.3468087048373\n",
      "Epoch 1651/4000: Train Loss: 2838.3595639997907 Test Loss: 2999.342975193814\n",
      "Epoch 1652/4000: Train Loss: 2838.356999969019 Test Loss: 2999.33914538574\n",
      "Epoch 1653/4000: Train Loss: 2838.354438634834 Test Loss: 2999.335319276899\n",
      "Epoch 1654/4000: Train Loss: 2838.351879994399 Test Loss: 2999.3314968635736\n",
      "Epoch 1655/4000: Train Loss: 2838.349324044881 Test Loss: 2999.3276781420495\n",
      "Epoch 1656/4000: Train Loss: 2838.34677078345 Test Loss: 2999.3238631086197\n",
      "Epoch 1657/4000: Train Loss: 2838.3442202072792 Test Loss: 2999.32005175958\n",
      "Epoch 1658/4000: Train Loss: 2838.3416723135447 Test Loss: 2999.316244091228\n",
      "Epoch 1659/4000: Train Loss: 2838.3391270994243 Test Loss: 2999.312440099868\n",
      "Epoch 1660/4000: Train Loss: 2838.336584562101 Test Loss: 2999.3086397818042\n",
      "Epoch 1661/4000: Train Loss: 2838.334044698759 Test Loss: 2999.304843133347\n",
      "Epoch 1662/4000: Train Loss: 2838.331507506587 Test Loss: 2999.301050150814\n",
      "Epoch 1663/4000: Train Loss: 2838.3289729827743 Test Loss: 2999.29726083052\n",
      "Epoch 1664/4000: Train Loss: 2838.3264411245154 Test Loss: 2999.293475168788\n",
      "Epoch 1665/4000: Train Loss: 2838.323911929007 Test Loss: 2999.289693161942\n",
      "Epoch 1666/4000: Train Loss: 2838.3213853934485 Test Loss: 2999.285914806313\n",
      "Epoch 1667/4000: Train Loss: 2838.3188615150425 Test Loss: 2999.2821400982325\n",
      "Epoch 1668/4000: Train Loss: 2838.316340290995 Test Loss: 2999.2783690340384\n",
      "Epoch 1669/4000: Train Loss: 2838.3138217185133 Test Loss: 2999.2746016100696\n",
      "Epoch 1670/4000: Train Loss: 2838.3113057948094 Test Loss: 2999.2708378226735\n",
      "Epoch 1671/4000: Train Loss: 2838.3087925170976 Test Loss: 2999.267077668195\n",
      "Epoch 1672/4000: Train Loss: 2838.306281882595 Test Loss: 2999.263321142986\n",
      "Epoch 1673/4000: Train Loss: 2838.303773888522 Test Loss: 2999.2595682434035\n",
      "Epoch 1674/4000: Train Loss: 2838.301268532101 Test Loss: 2999.2558189658057\n",
      "Epoch 1675/4000: Train Loss: 2838.298765810559 Test Loss: 2999.2520733065558\n",
      "Epoch 1676/4000: Train Loss: 2838.2962657211233 Test Loss: 2999.2483312620193\n",
      "Epoch 1677/4000: Train Loss: 2838.293768261028 Test Loss: 2999.2445928285692\n",
      "Epoch 1678/4000: Train Loss: 2838.291273427506 Test Loss: 2999.240858002577\n",
      "Epoch 1679/4000: Train Loss: 2838.2887812177955 Test Loss: 2999.237126780423\n",
      "Epoch 1680/4000: Train Loss: 2838.2862916291374 Test Loss: 2999.2333991584865\n",
      "Epoch 1681/4000: Train Loss: 2838.2838046587744 Test Loss: 2999.2296751331537\n",
      "Epoch 1682/4000: Train Loss: 2838.2813203039536 Test Loss: 2999.2259547008125\n",
      "Epoch 1683/4000: Train Loss: 2838.2788385619237 Test Loss: 2999.2222378578576\n",
      "Epoch 1684/4000: Train Loss: 2838.276359429937 Test Loss: 2999.2185246006834\n",
      "Epoch 1685/4000: Train Loss: 2838.2738829052487 Test Loss: 2999.214814925692\n",
      "Epoch 1686/4000: Train Loss: 2838.2714089851165 Test Loss: 2999.2111088292845\n",
      "Epoch 1687/4000: Train Loss: 2838.2689376668013 Test Loss: 2999.2074063078694\n",
      "Epoch 1688/4000: Train Loss: 2838.2664689475664 Test Loss: 2999.2037073578585\n",
      "Epoch 1689/4000: Train Loss: 2838.264002824679 Test Loss: 2999.200011975666\n",
      "Epoch 1690/4000: Train Loss: 2838.261539295408 Test Loss: 2999.19632015771\n",
      "Epoch 1691/4000: Train Loss: 2838.2590783570263 Test Loss: 2999.1926319004147\n",
      "Epoch 1692/4000: Train Loss: 2838.256620006808 Test Loss: 2999.188947200203\n",
      "Epoch 1693/4000: Train Loss: 2838.254164242033 Test Loss: 2999.185266053507\n",
      "Epoch 1694/4000: Train Loss: 2838.25171105998 Test Loss: 2999.1815884567563\n",
      "Epoch 1695/4000: Train Loss: 2838.249260457934 Test Loss: 2999.1779144063908\n",
      "Epoch 1696/4000: Train Loss: 2838.2468124331813 Test Loss: 2999.174243898849\n",
      "Epoch 1697/4000: Train Loss: 2838.2443669830113 Test Loss: 2999.170576930577\n",
      "Epoch 1698/4000: Train Loss: 2838.2419241047164 Test Loss: 2999.1669134980207\n",
      "Epoch 1699/4000: Train Loss: 2838.239483795592 Test Loss: 2999.1632535976305\n",
      "Epoch 1700/4000: Train Loss: 2838.237046052936 Test Loss: 2999.159597225865\n",
      "Epoch 1701/4000: Train Loss: 2838.23461087405 Test Loss: 2999.1559443791807\n",
      "Epoch 1702/4000: Train Loss: 2838.232178256235 Test Loss: 2999.1522950540393\n",
      "Epoch 1703/4000: Train Loss: 2838.229748196801 Test Loss: 2999.148649246908\n",
      "Epoch 1704/4000: Train Loss: 2838.227320693055 Test Loss: 2999.1450069542543\n",
      "Epoch 1705/4000: Train Loss: 2838.22489574231 Test Loss: 2999.1413681725526\n",
      "Epoch 1706/4000: Train Loss: 2838.2224733418807 Test Loss: 2999.1377328982794\n",
      "Epoch 1707/4000: Train Loss: 2838.220053489086 Test Loss: 2999.1341011279155\n",
      "Epoch 1708/4000: Train Loss: 2838.217636181245 Test Loss: 2999.1304728579444\n",
      "Epoch 1709/4000: Train Loss: 2838.215221415682 Test Loss: 2999.1268480848544\n",
      "Epoch 1710/4000: Train Loss: 2838.2128091897234 Test Loss: 2999.123226805136\n",
      "Epoch 1711/4000: Train Loss: 2838.210399500698 Test Loss: 2999.1196090152825\n",
      "Epoch 1712/4000: Train Loss: 2838.2079923459382 Test Loss: 2999.115994711796\n",
      "Epoch 1713/4000: Train Loss: 2838.205587722778 Test Loss: 2999.112383891174\n",
      "Epoch 1714/4000: Train Loss: 2838.2031856285553 Test Loss: 2999.108776549925\n",
      "Epoch 1715/4000: Train Loss: 2838.20078606061 Test Loss: 2999.1051726845567\n",
      "Epoch 1716/4000: Train Loss: 2838.1983890162865 Test Loss: 2999.1015722915827\n",
      "Epoch 1717/4000: Train Loss: 2838.195994492929 Test Loss: 2999.097975367518\n",
      "Epoch 1718/4000: Train Loss: 2838.1936024878873 Test Loss: 2999.094381908883\n",
      "Epoch 1719/4000: Train Loss: 2838.1912129985126 Test Loss: 2999.0907919122023\n",
      "Epoch 1720/4000: Train Loss: 2838.188826022159 Test Loss: 2999.0872053740027\n",
      "Epoch 1721/4000: Train Loss: 2838.186441556184 Test Loss: 2999.0836222908115\n",
      "Epoch 1722/4000: Train Loss: 2838.184059597947 Test Loss: 2999.080042659165\n",
      "Epoch 1723/4000: Train Loss: 2838.1816801448113 Test Loss: 2999.0764664756025\n",
      "Epoch 1724/4000: Train Loss: 2838.1793031941415 Test Loss: 2999.072893736662\n",
      "Epoch 1725/4000: Train Loss: 2838.1769287433062 Test Loss: 2999.069324438891\n",
      "Epoch 1726/4000: Train Loss: 2838.1745567896755 Test Loss: 2999.0657585788344\n",
      "Epoch 1727/4000: Train Loss: 2838.1721873306246 Test Loss: 2999.0621961530474\n",
      "Epoch 1728/4000: Train Loss: 2838.169820363529 Test Loss: 2999.058637158083\n",
      "Epoch 1729/4000: Train Loss: 2838.1674558857676 Test Loss: 2999.0550815905012\n",
      "Epoch 1730/4000: Train Loss: 2838.165093894723 Test Loss: 2999.051529446864\n",
      "Epoch 1731/4000: Train Loss: 2838.1627343877803 Test Loss: 2999.047980723738\n",
      "Epoch 1732/4000: Train Loss: 2838.160377362326 Test Loss: 2999.0444354176907\n",
      "Epoch 1733/4000: Train Loss: 2838.1580228157504 Test Loss: 2999.0408935252967\n",
      "Epoch 1734/4000: Train Loss: 2838.155670745447 Test Loss: 2999.037355043131\n",
      "Epoch 1735/4000: Train Loss: 2838.1533211488118 Test Loss: 2999.0338199677753\n",
      "Epoch 1736/4000: Train Loss: 2838.1509740232423 Test Loss: 2999.030288295813\n",
      "Epoch 1737/4000: Train Loss: 2838.1486293661405 Test Loss: 2999.0267600238294\n",
      "Epoch 1738/4000: Train Loss: 2838.14628717491 Test Loss: 2999.0232351484156\n",
      "Epoch 1739/4000: Train Loss: 2838.143947446957 Test Loss: 2999.0197136661654\n",
      "Epoch 1740/4000: Train Loss: 2838.1416101796917 Test Loss: 2999.016195573677\n",
      "Epoch 1741/4000: Train Loss: 2838.139275370526 Test Loss: 2999.0126808675514\n",
      "Epoch 1742/4000: Train Loss: 2838.136943016874 Test Loss: 2999.009169544392\n",
      "Epoch 1743/4000: Train Loss: 2838.1346131161536 Test Loss: 2999.005661600807\n",
      "Epoch 1744/4000: Train Loss: 2838.1322856657853 Test Loss: 2999.0021570334075\n",
      "Epoch 1745/4000: Train Loss: 2838.129960663192 Test Loss: 2998.998655838809\n",
      "Epoch 1746/4000: Train Loss: 2838.1276381058 Test Loss: 2998.995158013629\n",
      "Epoch 1747/4000: Train Loss: 2838.125317991037 Test Loss: 2998.9916635544896\n",
      "Epoch 1748/4000: Train Loss: 2838.123000316333 Test Loss: 2998.9881724580164\n",
      "Epoch 1749/4000: Train Loss: 2838.120685079124 Test Loss: 2998.9846847208382\n",
      "Epoch 1750/4000: Train Loss: 2838.118372276845 Test Loss: 2998.9812003395864\n",
      "Epoch 1751/4000: Train Loss: 2838.116061906936 Test Loss: 2998.977719310897\n",
      "Epoch 1752/4000: Train Loss: 2838.113753966839 Test Loss: 2998.9742416314075\n",
      "Epoch 1753/4000: Train Loss: 2838.1114484539967 Test Loss: 2998.9707672977624\n",
      "Epoch 1754/4000: Train Loss: 2838.1091453658587 Test Loss: 2998.967296306608\n",
      "Epoch 1755/4000: Train Loss: 2838.106844699874 Test Loss: 2998.963828654591\n",
      "Epoch 1756/4000: Train Loss: 2838.104546453494 Test Loss: 2998.9603643383675\n",
      "Epoch 1757/4000: Train Loss: 2838.1022506241766 Test Loss: 2998.95690335459\n",
      "Epoch 1758/4000: Train Loss: 2838.099957209378 Test Loss: 2998.9534456999218\n",
      "Epoch 1759/4000: Train Loss: 2838.0976662065586 Test Loss: 2998.9499913710238\n",
      "Epoch 1760/4000: Train Loss: 2838.0953776131832 Test Loss: 2998.9465403645618\n",
      "Epoch 1761/4000: Train Loss: 2838.093091426717 Test Loss: 2998.9430926772075\n",
      "Epoch 1762/4000: Train Loss: 2838.090807644628 Test Loss: 2998.9396483056335\n",
      "Epoch 1763/4000: Train Loss: 2838.0885262643887 Test Loss: 2998.936207246516\n",
      "Epoch 1764/4000: Train Loss: 2838.0862472834724 Test Loss: 2998.932769496536\n",
      "Epoch 1765/4000: Train Loss: 2838.0839706993556 Test Loss: 2998.929335052377\n",
      "Epoch 1766/4000: Train Loss: 2838.0816965095178 Test Loss: 2998.925903910725\n",
      "Epoch 1767/4000: Train Loss: 2838.079424711442 Test Loss: 2998.92247606827\n",
      "Epoch 1768/4000: Train Loss: 2838.07715530261 Test Loss: 2998.919051521708\n",
      "Epoch 1769/4000: Train Loss: 2838.074888280512 Test Loss: 2998.9156302677334\n",
      "Epoch 1770/4000: Train Loss: 2838.0726236426362 Test Loss: 2998.912212303048\n",
      "Epoch 1771/4000: Train Loss: 2838.070361386476 Test Loss: 2998.9087976243554\n",
      "Epoch 1772/4000: Train Loss: 2838.068101509526 Test Loss: 2998.9053862283636\n",
      "Epoch 1773/4000: Train Loss: 2838.065844009285 Test Loss: 2998.9019781117813\n",
      "Epoch 1774/4000: Train Loss: 2838.063588883251 Test Loss: 2998.8985732713245\n",
      "Epoch 1775/4000: Train Loss: 2838.0613361289297 Test Loss: 2998.89517170371\n",
      "Epoch 1776/4000: Train Loss: 2838.0590857438256 Test Loss: 2998.8917734056567\n",
      "Epoch 1777/4000: Train Loss: 2838.0568377254467 Test Loss: 2998.8883783738916\n",
      "Epoch 1778/4000: Train Loss: 2838.0545920713043 Test Loss: 2998.8849866051396\n",
      "Epoch 1779/4000: Train Loss: 2838.052348778913 Test Loss: 2998.881598096133\n",
      "Epoch 1780/4000: Train Loss: 2838.050107845787 Test Loss: 2998.878212843606\n",
      "Epoch 1781/4000: Train Loss: 2838.047869269446 Test Loss: 2998.874830844294\n",
      "Epoch 1782/4000: Train Loss: 2838.0456330474112 Test Loss: 2998.871452094939\n",
      "Epoch 1783/4000: Train Loss: 2838.043399177207 Test Loss: 2998.868076592286\n",
      "Epoch 1784/4000: Train Loss: 2838.04116765636 Test Loss: 2998.8647043330825\n",
      "Epoch 1785/4000: Train Loss: 2838.038938482399 Test Loss: 2998.8613353140786\n",
      "Epoch 1786/4000: Train Loss: 2838.0367116528555 Test Loss: 2998.8579695320277\n",
      "Epoch 1787/4000: Train Loss: 2838.034487165264 Test Loss: 2998.854606983689\n",
      "Epoch 1788/4000: Train Loss: 2838.0322650171624 Test Loss: 2998.8512476658216\n",
      "Epoch 1789/4000: Train Loss: 2838.0300452060897 Test Loss: 2998.84789157519\n",
      "Epoch 1790/4000: Train Loss: 2838.027827729587 Test Loss: 2998.844538708562\n",
      "Epoch 1791/4000: Train Loss: 2838.0256125852006 Test Loss: 2998.8411890627085\n",
      "Epoch 1792/4000: Train Loss: 2838.0233997704777 Test Loss: 2998.837842634401\n",
      "Epoch 1793/4000: Train Loss: 2838.021189282967 Test Loss: 2998.8344994204226\n",
      "Epoch 1794/4000: Train Loss: 2838.018981120222 Test Loss: 2998.8311594175484\n",
      "Epoch 1795/4000: Train Loss: 2838.0167752797975 Test Loss: 2998.8278226225643\n",
      "Epoch 1796/4000: Train Loss: 2838.014571759251 Test Loss: 2998.824489032258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1797/4000: Train Loss: 2838.0123705561427 Test Loss: 2998.8211586434204\n",
      "Epoch 1798/4000: Train Loss: 2838.010171668035 Test Loss: 2998.8178314528427\n",
      "Epoch 1799/4000: Train Loss: 2838.007975092494 Test Loss: 2998.8145074573245\n",
      "Epoch 1800/4000: Train Loss: 2838.005780827087 Test Loss: 2998.8111866536656\n",
      "Epoch 1801/4000: Train Loss: 2838.0035888693847 Test Loss: 2998.8078690386687\n",
      "Epoch 1802/4000: Train Loss: 2838.00139921696 Test Loss: 2998.804554609143\n",
      "Epoch 1803/4000: Train Loss: 2837.999211867389 Test Loss: 2998.8012433618965\n",
      "Epoch 1804/4000: Train Loss: 2837.9970268182487 Test Loss: 2998.7979352937423\n",
      "Epoch 1805/4000: Train Loss: 2837.99484406712 Test Loss: 2998.7946304014995\n",
      "Epoch 1806/4000: Train Loss: 2837.992663611587 Test Loss: 2998.7913286819853\n",
      "Epoch 1807/4000: Train Loss: 2837.9904854492347 Test Loss: 2998.788030132025\n",
      "Epoch 1808/4000: Train Loss: 2837.9883095776513 Test Loss: 2998.784734748445\n",
      "Epoch 1809/4000: Train Loss: 2837.986135994428 Test Loss: 2998.781442528073\n",
      "Epoch 1810/4000: Train Loss: 2837.9839646971577 Test Loss: 2998.7781534677447\n",
      "Epoch 1811/4000: Train Loss: 2837.9817956834363 Test Loss: 2998.7748675642947\n",
      "Epoch 1812/4000: Train Loss: 2837.9796289508627 Test Loss: 2998.7715848145626\n",
      "Epoch 1813/4000: Train Loss: 2837.9774644970375 Test Loss: 2998.768305215391\n",
      "Epoch 1814/4000: Train Loss: 2837.975302319564 Test Loss: 2998.7650287636275\n",
      "Epoch 1815/4000: Train Loss: 2837.973142416048 Test Loss: 2998.7617554561193\n",
      "Epoch 1816/4000: Train Loss: 2837.9709847840986 Test Loss: 2998.7584852897194\n",
      "Epoch 1817/4000: Train Loss: 2837.968829421327 Test Loss: 2998.755218261283\n",
      "Epoch 1818/4000: Train Loss: 2837.966676325346 Test Loss: 2998.751954367669\n",
      "Epoch 1819/4000: Train Loss: 2837.9645254937714 Test Loss: 2998.7486936057417\n",
      "Epoch 1820/4000: Train Loss: 2837.9623769242226 Test Loss: 2998.7454359723633\n",
      "Epoch 1821/4000: Train Loss: 2837.9602306143197 Test Loss: 2998.7421814644026\n",
      "Epoch 1822/4000: Train Loss: 2837.9580865616867 Test Loss: 2998.7389300787336\n",
      "Epoch 1823/4000: Train Loss: 2837.9559447639504 Test Loss: 2998.7356818122284\n",
      "Epoch 1824/4000: Train Loss: 2837.9538052187377 Test Loss: 2998.732436661768\n",
      "Epoch 1825/4000: Train Loss: 2837.951667923681 Test Loss: 2998.729194624232\n",
      "Epoch 1826/4000: Train Loss: 2837.949532876413 Test Loss: 2998.725955696505\n",
      "Epoch 1827/4000: Train Loss: 2837.94740007457 Test Loss: 2998.722719875473\n",
      "Epoch 1828/4000: Train Loss: 2837.9452695157906 Test Loss: 2998.7194871580305\n",
      "Epoch 1829/4000: Train Loss: 2837.943141197716 Test Loss: 2998.7162575410694\n",
      "Epoch 1830/4000: Train Loss: 2837.9410151179895 Test Loss: 2998.7130310214857\n",
      "Epoch 1831/4000: Train Loss: 2837.938891274256 Test Loss: 2998.709807596182\n",
      "Epoch 1832/4000: Train Loss: 2837.9367696641652 Test Loss: 2998.706587262062\n",
      "Epoch 1833/4000: Train Loss: 2837.9346502853678 Test Loss: 2998.7033700160305\n",
      "Epoch 1834/4000: Train Loss: 2837.932533135517 Test Loss: 2998.7001558549982\n",
      "Epoch 1835/4000: Train Loss: 2837.9304182122687 Test Loss: 2998.69694477588\n",
      "Epoch 1836/4000: Train Loss: 2837.9283055132805 Test Loss: 2998.6937367755895\n",
      "Epoch 1837/4000: Train Loss: 2837.926195036214 Test Loss: 2998.6905318510467\n",
      "Epoch 1838/4000: Train Loss: 2837.924086778732 Test Loss: 2998.6873299991753\n",
      "Epoch 1839/4000: Train Loss: 2837.9219807385 Test Loss: 2998.6841312168995\n",
      "Epoch 1840/4000: Train Loss: 2837.9198769131867 Test Loss: 2998.6809355011505\n",
      "Epoch 1841/4000: Train Loss: 2837.9177753004624 Test Loss: 2998.677742848857\n",
      "Epoch 1842/4000: Train Loss: 2837.9156758980002 Test Loss: 2998.674553256957\n",
      "Epoch 1843/4000: Train Loss: 2837.913578703475 Test Loss: 2998.671366722387\n",
      "Epoch 1844/4000: Train Loss: 2837.9114837145653 Test Loss: 2998.6681832420886\n",
      "Epoch 1845/4000: Train Loss: 2837.9093909289513 Test Loss: 2998.665002813008\n",
      "Epoch 1846/4000: Train Loss: 2837.907300344316 Test Loss: 2998.661825432092\n",
      "Epoch 1847/4000: Train Loss: 2837.905211958345 Test Loss: 2998.65865109629\n",
      "Epoch 1848/4000: Train Loss: 2837.9031257687243 Test Loss: 2998.6554798025563\n",
      "Epoch 1849/4000: Train Loss: 2837.9010417731456 Test Loss: 2998.65231154785\n",
      "Epoch 1850/4000: Train Loss: 2837.8989599693014 Test Loss: 2998.649146329129\n",
      "Epoch 1851/4000: Train Loss: 2837.8968803548855 Test Loss: 2998.6459841433575\n",
      "Epoch 1852/4000: Train Loss: 2837.8948029275966 Test Loss: 2998.642824987501\n",
      "Epoch 1853/4000: Train Loss: 2837.8927276851337 Test Loss: 2998.639668858531\n",
      "Epoch 1854/4000: Train Loss: 2837.8906546251997 Test Loss: 2998.636515753418\n",
      "Epoch 1855/4000: Train Loss: 2837.8885837454977 Test Loss: 2998.633365669138\n",
      "Epoch 1856/4000: Train Loss: 2837.8865150437364 Test Loss: 2998.630218602672\n",
      "Epoch 1857/4000: Train Loss: 2837.8844485176246 Test Loss: 2998.627074550999\n",
      "Epoch 1858/4000: Train Loss: 2837.882384164874 Test Loss: 2998.6239335111045\n",
      "Epoch 1859/4000: Train Loss: 2837.8803219832 Test Loss: 2998.620795479979\n",
      "Epoch 1860/4000: Train Loss: 2837.8782619703175 Test Loss: 2998.61766045461\n",
      "Epoch 1861/4000: Train Loss: 2837.876204123947 Test Loss: 2998.614528431994\n",
      "Epoch 1862/4000: Train Loss: 2837.874148441809 Test Loss: 2998.611399409128\n",
      "Epoch 1863/4000: Train Loss: 2837.872094921629 Test Loss: 2998.6082733830103\n",
      "Epoch 1864/4000: Train Loss: 2837.8700435611313 Test Loss: 2998.605150350649\n",
      "Epoch 1865/4000: Train Loss: 2837.8679943580455 Test Loss: 2998.6020303090463\n",
      "Epoch 1866/4000: Train Loss: 2837.8659473101025 Test Loss: 2998.598913255212\n",
      "Epoch 1867/4000: Train Loss: 2837.8639024150366 Test Loss: 2998.5957991861615\n",
      "Epoch 1868/4000: Train Loss: 2837.8618596705815 Test Loss: 2998.5926880989073\n",
      "Epoch 1869/4000: Train Loss: 2837.859819074478 Test Loss: 2998.5895799904706\n",
      "Epoch 1870/4000: Train Loss: 2837.8577806244643 Test Loss: 2998.5864748578724\n",
      "Epoch 1871/4000: Train Loss: 2837.8557443182845 Test Loss: 2998.5833726981377\n",
      "Epoch 1872/4000: Train Loss: 2837.8537101536845 Test Loss: 2998.580273508292\n",
      "Epoch 1873/4000: Train Loss: 2837.8516781284115 Test Loss: 2998.57717728537\n",
      "Epoch 1874/4000: Train Loss: 2837.849648240215 Test Loss: 2998.574084026404\n",
      "Epoch 1875/4000: Train Loss: 2837.847620486849 Test Loss: 2998.5709937284296\n",
      "Epoch 1876/4000: Train Loss: 2837.845594866066 Test Loss: 2998.567906388489\n",
      "Epoch 1877/4000: Train Loss: 2837.843571375625 Test Loss: 2998.564822003625\n",
      "Epoch 1878/4000: Train Loss: 2837.841550013285 Test Loss: 2998.561740570882\n",
      "Epoch 1879/4000: Train Loss: 2837.8395307768074 Test Loss: 2998.5586620873114\n",
      "Epoch 1880/4000: Train Loss: 2837.8375136639575 Test Loss: 2998.5555865499623\n",
      "Epoch 1881/4000: Train Loss: 2837.8354986725008 Test Loss: 2998.552513955894\n",
      "Epoch 1882/4000: Train Loss: 2837.8334858002067 Test Loss: 2998.54944430216\n",
      "Epoch 1883/4000: Train Loss: 2837.8314750448462 Test Loss: 2998.546377585825\n",
      "Epoch 1884/4000: Train Loss: 2837.8294664041937 Test Loss: 2998.543313803952\n",
      "Epoch 1885/4000: Train Loss: 2837.827459876024 Test Loss: 2998.5402529536077\n",
      "Epoch 1886/4000: Train Loss: 2837.8254554581167 Test Loss: 2998.5371950318627\n",
      "Epoch 1887/4000: Train Loss: 2837.8234531482512 Test Loss: 2998.53414003579\n",
      "Epoch 1888/4000: Train Loss: 2837.821452944211 Test Loss: 2998.5310879624667\n",
      "Epoch 1889/4000: Train Loss: 2837.819454843782 Test Loss: 2998.528038808972\n",
      "Epoch 1890/4000: Train Loss: 2837.8174588447514 Test Loss: 2998.524992572386\n",
      "Epoch 1891/4000: Train Loss: 2837.8154649449084 Test Loss: 2998.5219492497954\n",
      "Epoch 1892/4000: Train Loss: 2837.8134731420464 Test Loss: 2998.518908838287\n",
      "Epoch 1893/4000: Train Loss: 2837.81148343396 Test Loss: 2998.515871334954\n",
      "Epoch 1894/4000: Train Loss: 2837.8094958184447 Test Loss: 2998.5128367368898\n",
      "Epoch 1895/4000: Train Loss: 2837.807510293301 Test Loss: 2998.5098050411907\n",
      "Epoch 1896/4000: Train Loss: 2837.8055268563307 Test Loss: 2998.506776244958\n",
      "Epoch 1897/4000: Train Loss: 2837.803545505337 Test Loss: 2998.5037503452927\n",
      "Epoch 1898/4000: Train Loss: 2837.8015662381263 Test Loss: 2998.5007273393026\n",
      "Epoch 1899/4000: Train Loss: 2837.7995890525067 Test Loss: 2998.4977072240954\n",
      "Epoch 1900/4000: Train Loss: 2837.7976139462894 Test Loss: 2998.4946899967836\n",
      "Epoch 1901/4000: Train Loss: 2837.7956409172875 Test Loss: 2998.491675654482\n",
      "Epoch 1902/4000: Train Loss: 2837.7936699633165 Test Loss: 2998.488664194309\n",
      "Epoch 1903/4000: Train Loss: 2837.791701082194 Test Loss: 2998.4856556133836\n",
      "Epoch 1904/4000: Train Loss: 2837.7897342717392 Test Loss: 2998.4826499088304\n",
      "Epoch 1905/4000: Train Loss: 2837.7877695297757 Test Loss: 2998.479647077778\n",
      "Epoch 1906/4000: Train Loss: 2837.7858068541273 Test Loss: 2998.476647117352\n",
      "Epoch 1907/4000: Train Loss: 2837.783846242621 Test Loss: 2998.4736500246877\n",
      "Epoch 1908/4000: Train Loss: 2837.7818876930864 Test Loss: 2998.4706557969207\n",
      "Epoch 1909/4000: Train Loss: 2837.7799312033544 Test Loss: 2998.467664431188\n",
      "Epoch 1910/4000: Train Loss: 2837.7779767712586 Test Loss: 2998.464675924632\n",
      "Epoch 1911/4000: Train Loss: 2837.7760243946354 Test Loss: 2998.4616902743956\n",
      "Epoch 1912/4000: Train Loss: 2837.7740740713225 Test Loss: 2998.458707477628\n",
      "Epoch 1913/4000: Train Loss: 2837.772125799161 Test Loss: 2998.455727531478\n",
      "Epoch 1914/4000: Train Loss: 2837.770179575993 Test Loss: 2998.452750433096\n",
      "Epoch 1915/4000: Train Loss: 2837.7682353996647 Test Loss: 2998.4497761796424\n",
      "Epoch 1916/4000: Train Loss: 2837.766293268023 Test Loss: 2998.446804768275\n",
      "Epoch 1917/4000: Train Loss: 2837.7643531789176 Test Loss: 2998.4438361961534\n",
      "Epoch 1918/4000: Train Loss: 2837.7624151301993 Test Loss: 2998.4408704604434\n",
      "Epoch 1919/4000: Train Loss: 2837.760479119723 Test Loss: 2998.437907558312\n",
      "Epoch 1920/4000: Train Loss: 2837.758545145346 Test Loss: 2998.434947486932\n",
      "Epoch 1921/4000: Train Loss: 2837.7566132049255 Test Loss: 2998.431990243472\n",
      "Epoch 1922/4000: Train Loss: 2837.7546832963226 Test Loss: 2998.4290358251133\n",
      "Epoch 1923/4000: Train Loss: 2837.752755417401 Test Loss: 2998.4260842290314\n",
      "Epoch 1924/4000: Train Loss: 2837.7508295660264 Test Loss: 2998.423135452411\n",
      "Epoch 1925/4000: Train Loss: 2837.7489057400658 Test Loss: 2998.420189492435\n",
      "Epoch 1926/4000: Train Loss: 2837.746983937389 Test Loss: 2998.417246346291\n",
      "Epoch 1927/4000: Train Loss: 2837.7450641558676 Test Loss: 2998.4143060111714\n",
      "Epoch 1928/4000: Train Loss: 2837.7431463933776 Test Loss: 2998.411368484269\n",
      "Epoch 1929/4000: Train Loss: 2837.7412306477945 Test Loss: 2998.4084337627773\n",
      "Epoch 1930/4000: Train Loss: 2837.7393169169973 Test Loss: 2998.4055018439\n",
      "Epoch 1931/4000: Train Loss: 2837.737405198867 Test Loss: 2998.4025727248372\n",
      "Epoch 1932/4000: Train Loss: 2837.735495491286 Test Loss: 2998.3996464027923\n",
      "Epoch 1933/4000: Train Loss: 2837.733587792142 Test Loss: 2998.396722874977\n",
      "Epoch 1934/4000: Train Loss: 2837.731682099321 Test Loss: 2998.393802138598\n",
      "Epoch 1935/4000: Train Loss: 2837.7297784107127 Test Loss: 2998.3908841908706\n",
      "Epoch 1936/4000: Train Loss: 2837.727876724211 Test Loss: 2998.3879690290105\n",
      "Epoch 1937/4000: Train Loss: 2837.725977037709 Test Loss: 2998.385056650239\n",
      "Epoch 1938/4000: Train Loss: 2837.7240793491032 Test Loss: 2998.382147051775\n",
      "Epoch 1939/4000: Train Loss: 2837.7221836562926 Test Loss: 2998.3792402308463\n",
      "Epoch 1940/4000: Train Loss: 2837.7202899571794 Test Loss: 2998.376336184679\n",
      "Epoch 1941/4000: Train Loss: 2837.7183982496654 Test Loss: 2998.3734349105034\n",
      "Epoch 1942/4000: Train Loss: 2837.716508531656 Test Loss: 2998.3705364055545\n",
      "Epoch 1943/4000: Train Loss: 2837.7146208010604 Test Loss: 2998.3676406670666\n",
      "Epoch 1944/4000: Train Loss: 2837.712735055786 Test Loss: 2998.364747692282\n",
      "Epoch 1945/4000: Train Loss: 2837.710851293747 Test Loss: 2998.361857478438\n",
      "Epoch 1946/4000: Train Loss: 2837.708969512857 Test Loss: 2998.3589700227813\n",
      "Epoch 1947/4000: Train Loss: 2837.707089711032 Test Loss: 2998.3560853225617\n",
      "Epoch 1948/4000: Train Loss: 2837.7052118861907 Test Loss: 2998.353203375027\n",
      "Epoch 1949/4000: Train Loss: 2837.7033360362543 Test Loss: 2998.3503241774324\n",
      "Epoch 1950/4000: Train Loss: 2837.7014621591456 Test Loss: 2998.34744772703\n",
      "Epoch 1951/4000: Train Loss: 2837.6995902527897 Test Loss: 2998.3445740210827\n",
      "Epoch 1952/4000: Train Loss: 2837.697720315114 Test Loss: 2998.34170305685\n",
      "Epoch 1953/4000: Train Loss: 2837.695852344048 Test Loss: 2998.338834831597\n",
      "Epoch 1954/4000: Train Loss: 2837.693986337524 Test Loss: 2998.33596934259\n",
      "Epoch 1955/4000: Train Loss: 2837.6921222934757 Test Loss: 2998.3331065870993\n",
      "Epoch 1956/4000: Train Loss: 2837.6902602098376 Test Loss: 2998.3302465623997\n",
      "Epoch 1957/4000: Train Loss: 2837.6884000845503 Test Loss: 2998.327389265764\n",
      "Epoch 1958/4000: Train Loss: 2837.6865419155524 Test Loss: 2998.3245346944714\n",
      "Epoch 1959/4000: Train Loss: 2837.6846857007877 Test Loss: 2998.321682845803\n",
      "Epoch 1960/4000: Train Loss: 2837.6828314382 Test Loss: 2998.3188337170427\n",
      "Epoch 1961/4000: Train Loss: 2837.680979125737 Test Loss: 2998.3159873054788\n",
      "Epoch 1962/4000: Train Loss: 2837.679128761348 Test Loss: 2998.3131436083995\n",
      "Epoch 1963/4000: Train Loss: 2837.677280342982 Test Loss: 2998.310302623096\n",
      "Epoch 1964/4000: Train Loss: 2837.675433868595 Test Loss: 2998.3074643468635\n",
      "Epoch 1965/4000: Train Loss: 2837.6735893361415 Test Loss: 2998.3046287770017\n",
      "Epoch 1966/4000: Train Loss: 2837.6717467435788 Test Loss: 2998.301795910809\n",
      "Epoch 1967/4000: Train Loss: 2837.6699060888673 Test Loss: 2998.2989657455896\n",
      "Epoch 1968/4000: Train Loss: 2837.668067369969 Test Loss: 2998.29613827865\n",
      "Epoch 1969/4000: Train Loss: 2837.666230584847 Test Loss: 2998.2933135072985\n",
      "Epoch 1970/4000: Train Loss: 2837.664395731469 Test Loss: 2998.2904914288465\n",
      "Epoch 1971/4000: Train Loss: 2837.6625628078023 Test Loss: 2998.2876720406084\n",
      "Epoch 1972/4000: Train Loss: 2837.6607318118186 Test Loss: 2998.284855339902\n",
      "Epoch 1973/4000: Train Loss: 2837.658902741489 Test Loss: 2998.2820413240447\n",
      "Epoch 1974/4000: Train Loss: 2837.6570755947887 Test Loss: 2998.2792299903635\n",
      "Epoch 1975/4000: Train Loss: 2837.655250369696 Test Loss: 2998.276421336179\n",
      "Epoch 1976/4000: Train Loss: 2837.6534270641887 Test Loss: 2998.2736153588226\n",
      "Epoch 1977/4000: Train Loss: 2837.651605676248 Test Loss: 2998.270812055621\n",
      "Epoch 1978/4000: Train Loss: 2837.6497862038573 Test Loss: 2998.268011423913\n",
      "Epoch 1979/4000: Train Loss: 2837.647968645002 Test Loss: 2998.26521346103\n",
      "Epoch 1980/4000: Train Loss: 2837.6461529976705 Test Loss: 2998.2624181643114\n",
      "Epoch 1981/4000: Train Loss: 2837.644339259851 Test Loss: 2998.259625531104\n",
      "Epoch 1982/4000: Train Loss: 2837.6425274295366 Test Loss: 2998.2568355587473\n",
      "Epoch 1983/4000: Train Loss: 2837.6407175047207 Test Loss: 2998.2540482445893\n",
      "Epoch 1984/4000: Train Loss: 2837.638909483399 Test Loss: 2998.2512635859803\n",
      "Epoch 1985/4000: Train Loss: 2837.637103363569 Test Loss: 2998.2484815802727\n",
      "Epoch 1986/4000: Train Loss: 2837.6352991432323 Test Loss: 2998.245702224822\n",
      "Epoch 1987/4000: Train Loss: 2837.633496820391 Test Loss: 2998.242925516985\n",
      "Epoch 1988/4000: Train Loss: 2837.6316963930485 Test Loss: 2998.240151454122\n",
      "Epoch 1989/4000: Train Loss: 2837.629897859212 Test Loss: 2998.237380033599\n",
      "Epoch 1990/4000: Train Loss: 2837.62810121689 Test Loss: 2998.2346112527803\n",
      "Epoch 1991/4000: Train Loss: 2837.626306464094 Test Loss: 2998.231845109033\n",
      "Epoch 1992/4000: Train Loss: 2837.624513598835 Test Loss: 2998.229081599732\n",
      "Epoch 1993/4000: Train Loss: 2837.622722619129 Test Loss: 2998.2263207222477\n",
      "Epoch 1994/4000: Train Loss: 2837.620933522993 Test Loss: 2998.2235624739596\n",
      "Epoch 1995/4000: Train Loss: 2837.6191463084456 Test Loss: 2998.2208068522473\n",
      "Epoch 1996/4000: Train Loss: 2837.6173609735088 Test Loss: 2998.2180538544894\n",
      "Epoch 1997/4000: Train Loss: 2837.615577516205 Test Loss: 2998.215303478074\n",
      "Epoch 1998/4000: Train Loss: 2837.6137959345597 Test Loss: 2998.212555720388\n",
      "Epoch 1999/4000: Train Loss: 2837.6120162266006 Test Loss: 2998.20981057882\n",
      "Epoch 2000/4000: Train Loss: 2837.6102383903567 Test Loss: 2998.207068050765\n",
      "Epoch 2001/4000: Train Loss: 2837.6084624238597 Test Loss: 2998.204328133616\n",
      "Epoch 2002/4000: Train Loss: 2837.606688325143 Test Loss: 2998.201590824773\n",
      "Epoch 2003/4000: Train Loss: 2837.604916092242 Test Loss: 2998.1988561216376\n",
      "Epoch 2004/4000: Train Loss: 2837.603145723196 Test Loss: 2998.1961240216106\n",
      "Epoch 2005/4000: Train Loss: 2837.601377216043 Test Loss: 2998.1933945221\n",
      "Epoch 2006/4000: Train Loss: 2837.5996105688255 Test Loss: 2998.1906676205135\n",
      "Epoch 2007/4000: Train Loss: 2837.5978457795877 Test Loss: 2998.187943314263\n",
      "Epoch 2008/4000: Train Loss: 2837.5960828463753 Test Loss: 2998.1852216007633\n",
      "Epoch 2009/4000: Train Loss: 2837.594321767236 Test Loss: 2998.1825024774303\n",
      "Epoch 2010/4000: Train Loss: 2837.59256254022 Test Loss: 2998.179785941682\n",
      "Epoch 2011/4000: Train Loss: 2837.59080516338 Test Loss: 2998.177071990943\n",
      "Epoch 2012/4000: Train Loss: 2837.5890496347697 Test Loss: 2998.174360622636\n",
      "Epoch 2013/4000: Train Loss: 2837.587295952445 Test Loss: 2998.171651834188\n",
      "Epoch 2014/4000: Train Loss: 2837.5855441144654 Test Loss: 2998.1689456230315\n",
      "Epoch 2015/4000: Train Loss: 2837.58379411889 Test Loss: 2998.1662419865957\n",
      "Epoch 2016/4000: Train Loss: 2837.5820459637816 Test Loss: 2998.163540922318\n",
      "Epoch 2017/4000: Train Loss: 2837.5802996472044 Test Loss: 2998.160842427635\n",
      "Epoch 2018/4000: Train Loss: 2837.5785551672247 Test Loss: 2998.158146499988\n",
      "Epoch 2019/4000: Train Loss: 2837.5768125219106 Test Loss: 2998.155453136819\n",
      "Epoch 2020/4000: Train Loss: 2837.575071709334 Test Loss: 2998.1527623355723\n",
      "Epoch 2021/4000: Train Loss: 2837.5733327275666 Test Loss: 2998.1500740936995\n",
      "Epoch 2022/4000: Train Loss: 2837.5715955746828 Test Loss: 2998.147388408649\n",
      "Epoch 2023/4000: Train Loss: 2837.569860248759 Test Loss: 2998.144705277875\n",
      "Epoch 2024/4000: Train Loss: 2837.5681267478744 Test Loss: 2998.1420246988328\n",
      "Epoch 2025/4000: Train Loss: 2837.566395070109 Test Loss: 2998.139346668983\n",
      "Epoch 2026/4000: Train Loss: 2837.564665213546 Test Loss: 2998.136671185784\n",
      "Epoch 2027/4000: Train Loss: 2837.5629371762698 Test Loss: 2998.133998246702\n",
      "Epoch 2028/4000: Train Loss: 2837.5612109563667 Test Loss: 2998.1313278492007\n",
      "Epoch 2029/4000: Train Loss: 2837.559486551926 Test Loss: 2998.1286599907517\n",
      "Epoch 2030/4000: Train Loss: 2837.557763961038 Test Loss: 2998.125994668825\n",
      "Epoch 2031/4000: Train Loss: 2837.5560431817958 Test Loss: 2998.1233318808963\n",
      "Epoch 2032/4000: Train Loss: 2837.5543242122926 Test Loss: 2998.12067162444\n",
      "Epoch 2033/4000: Train Loss: 2837.552607050627 Test Loss: 2998.1180138969376\n",
      "Epoch 2034/4000: Train Loss: 2837.550891694897 Test Loss: 2998.1153586958703\n",
      "Epoch 2035/4000: Train Loss: 2837.5491781432033 Test Loss: 2998.1127060187227\n",
      "Epoch 2036/4000: Train Loss: 2837.547466393649 Test Loss: 2998.1100558629796\n",
      "Epoch 2037/4000: Train Loss: 2837.5457564443373 Test Loss: 2998.1074082261343\n",
      "Epoch 2038/4000: Train Loss: 2837.544048293377 Test Loss: 2998.1047631056754\n",
      "Epoch 2039/4000: Train Loss: 2837.542341938875 Test Loss: 2998.1021204991007\n",
      "Epoch 2040/4000: Train Loss: 2837.5406373789424 Test Loss: 2998.0994804039055\n",
      "Epoch 2041/4000: Train Loss: 2837.538934611693 Test Loss: 2998.09684281759\n",
      "Epoch 2042/4000: Train Loss: 2837.5372336352398 Test Loss: 2998.0942077376576\n",
      "Epoch 2043/4000: Train Loss: 2837.5355344477007 Test Loss: 2998.0915751616126\n",
      "Epoch 2044/4000: Train Loss: 2837.5338370471936 Test Loss: 2998.088945086962\n",
      "Epoch 2045/4000: Train Loss: 2837.5321414318396 Test Loss: 2998.0863175112163\n",
      "Epoch 2046/4000: Train Loss: 2837.5304475997605 Test Loss: 2998.0836924318883\n",
      "Epoch 2047/4000: Train Loss: 2837.5287555490813 Test Loss: 2998.0810698464925\n",
      "Epoch 2048/4000: Train Loss: 2837.527065277929 Test Loss: 2998.078449752547\n",
      "Epoch 2049/4000: Train Loss: 2837.525376784431 Test Loss: 2998.075832147572\n",
      "Epoch 2050/4000: Train Loss: 2837.5236900667187 Test Loss: 2998.0732170290885\n",
      "Epoch 2051/4000: Train Loss: 2837.522005122924 Test Loss: 2998.070604394625\n",
      "Epoch 2052/4000: Train Loss: 2837.5203219511814 Test Loss: 2998.067994241708\n",
      "Epoch 2053/4000: Train Loss: 2837.518640549627 Test Loss: 2998.065386567866\n",
      "Epoch 2054/4000: Train Loss: 2837.516960916399 Test Loss: 2998.0627813706355\n",
      "Epoch 2055/4000: Train Loss: 2837.515283049639 Test Loss: 2998.0601786475477\n",
      "Epoch 2056/4000: Train Loss: 2837.5136069474875 Test Loss: 2998.0575783961435\n",
      "Epoch 2057/4000: Train Loss: 2837.5119326080894 Test Loss: 2998.054980613962\n",
      "Epoch 2058/4000: Train Loss: 2837.510260029591 Test Loss: 2998.0523852985457\n",
      "Epoch 2059/4000: Train Loss: 2837.50858921014 Test Loss: 2998.0497924474416\n",
      "Epoch 2060/4000: Train Loss: 2837.506920147886 Test Loss: 2998.0472020581956\n",
      "Epoch 2061/4000: Train Loss: 2837.5052528409824 Test Loss: 2998.0446141283596\n",
      "Epoch 2062/4000: Train Loss: 2837.5035872875815 Test Loss: 2998.0420286554863\n",
      "Epoch 2063/4000: Train Loss: 2837.50192348584 Test Loss: 2998.0394456371305\n",
      "Epoch 2064/4000: Train Loss: 2837.5002614339155 Test Loss: 2998.0368650708515\n",
      "Epoch 2065/4000: Train Loss: 2837.498601129968 Test Loss: 2998.0342869542073\n",
      "Epoch 2066/4000: Train Loss: 2837.4969425721592 Test Loss: 2998.031711284763\n",
      "Epoch 2067/4000: Train Loss: 2837.495285758652 Test Loss: 2998.0291380600834\n",
      "Epoch 2068/4000: Train Loss: 2837.493630687612 Test Loss: 2998.026567277736\n",
      "Epoch 2069/4000: Train Loss: 2837.491977357207 Test Loss: 2998.023998935292\n",
      "Epoch 2070/4000: Train Loss: 2837.490325765607 Test Loss: 2998.0214330303234\n",
      "Epoch 2071/4000: Train Loss: 2837.4886759109827 Test Loss: 2998.0188695604065\n",
      "Epoch 2072/4000: Train Loss: 2837.4870277915065 Test Loss: 2998.0163085231175\n",
      "Epoch 2073/4000: Train Loss: 2837.4853814053545 Test Loss: 2998.013749916037\n",
      "Epoch 2074/4000: Train Loss: 2837.4837367507034 Test Loss: 2998.0111937367483\n",
      "Epoch 2075/4000: Train Loss: 2837.482093825733 Test Loss: 2998.008639982838\n",
      "Epoch 2076/4000: Train Loss: 2837.4804526286234 Test Loss: 2998.006088651891\n",
      "Epoch 2077/4000: Train Loss: 2837.4788131575574 Test Loss: 2998.003539741499\n",
      "Epoch 2078/4000: Train Loss: 2837.47717541072 Test Loss: 2998.0009932492567\n",
      "Epoch 2079/4000: Train Loss: 2837.475539386298 Test Loss: 2997.998449172755\n",
      "Epoch 2080/4000: Train Loss: 2837.4739050824796 Test Loss: 2997.995907509593\n",
      "Epoch 2081/4000: Train Loss: 2837.472272497455 Test Loss: 2997.9933682573715\n",
      "Epoch 2082/4000: Train Loss: 2837.470641629417 Test Loss: 2997.990831413693\n",
      "Epoch 2083/4000: Train Loss: 2837.46901247656 Test Loss: 2997.988296976162\n",
      "Epoch 2084/4000: Train Loss: 2837.467385037079 Test Loss: 2997.9857649423857\n",
      "Epoch 2085/4000: Train Loss: 2837.4657593091742 Test Loss: 2997.9832353099728\n",
      "Epoch 2086/4000: Train Loss: 2837.464135291043 Test Loss: 2997.9807080765363\n",
      "Epoch 2087/4000: Train Loss: 2837.462512980889 Test Loss: 2997.978183239692\n",
      "Epoch 2088/4000: Train Loss: 2837.4608923769156 Test Loss: 2997.9756607970558\n",
      "Epoch 2089/4000: Train Loss: 2837.4592734773278 Test Loss: 2997.973140746247\n",
      "Epoch 2090/4000: Train Loss: 2837.4576562803345 Test Loss: 2997.970623084889\n",
      "Epoch 2091/4000: Train Loss: 2837.456040784143 Test Loss: 2997.968107810604\n",
      "Epoch 2092/4000: Train Loss: 2837.454426986966 Test Loss: 2997.965594921021\n",
      "Epoch 2093/4000: Train Loss: 2837.4528148870163 Test Loss: 2997.9630844137673\n",
      "Epoch 2094/4000: Train Loss: 2837.451204482509 Test Loss: 2997.9605762864753\n",
      "Epoch 2095/4000: Train Loss: 2837.449595771661 Test Loss: 2997.95807053678\n",
      "Epoch 2096/4000: Train Loss: 2837.4479887526913 Test Loss: 2997.955567162316\n",
      "Epoch 2097/4000: Train Loss: 2837.44638342382 Test Loss: 2997.953066160725\n",
      "Epoch 2098/4000: Train Loss: 2837.44477978327 Test Loss: 2997.950567529644\n",
      "Epoch 2099/4000: Train Loss: 2837.443177829266 Test Loss: 2997.9480712667205\n",
      "Epoch 2100/4000: Train Loss: 2837.4415775600332 Test Loss: 2997.9455773695995\n",
      "Epoch 2101/4000: Train Loss: 2837.439978973801 Test Loss: 2997.9430858359287\n",
      "Epoch 2102/4000: Train Loss: 2837.438382068798 Test Loss: 2997.94059666336\n",
      "Epoch 2103/4000: Train Loss: 2837.436786843258 Test Loss: 2997.9381098495473\n",
      "Epoch 2104/4000: Train Loss: 2837.435193295413 Test Loss: 2997.9356253921446\n",
      "Epoch 2105/4000: Train Loss: 2837.4336014234987 Test Loss: 2997.9331432888093\n",
      "Epoch 2106/4000: Train Loss: 2837.4320112257533 Test Loss: 2997.930663537206\n",
      "Epoch 2107/4000: Train Loss: 2837.430422700416 Test Loss: 2997.9281861349923\n",
      "Epoch 2108/4000: Train Loss: 2837.428835845727 Test Loss: 2997.9257110798376\n",
      "Epoch 2109/4000: Train Loss: 2837.4272506599305 Test Loss: 2997.923238369407\n",
      "Epoch 2110/4000: Train Loss: 2837.4256671412713 Test Loss: 2997.920768001373\n",
      "Epoch 2111/4000: Train Loss: 2837.424085287995 Test Loss: 2997.9182999734057\n",
      "Epoch 2112/4000: Train Loss: 2837.422505098351 Test Loss: 2997.91583428318\n",
      "Epoch 2113/4000: Train Loss: 2837.4209265705886 Test Loss: 2997.913370928375\n",
      "Epoch 2114/4000: Train Loss: 2837.419349702961 Test Loss: 2997.9109099066686\n",
      "Epoch 2115/4000: Train Loss: 2837.4177744937224 Test Loss: 2997.9084512157433\n",
      "Epoch 2116/4000: Train Loss: 2837.4162009411284 Test Loss: 2997.905994853282\n",
      "Epoch 2117/4000: Train Loss: 2837.414629043436 Test Loss: 2997.9035408169743\n",
      "Epoch 2118/4000: Train Loss: 2837.4130587989057 Test Loss: 2997.901089104506\n",
      "Epoch 2119/4000: Train Loss: 2837.411490205798 Test Loss: 2997.898639713569\n",
      "Epoch 2120/4000: Train Loss: 2837.409923262377 Test Loss: 2997.8961926418597\n",
      "Epoch 2121/4000: Train Loss: 2837.4083579669077 Test Loss: 2997.893747887072\n",
      "Epoch 2122/4000: Train Loss: 2837.4067943176556 Test Loss: 2997.8913054469044\n",
      "Epoch 2123/4000: Train Loss: 2837.4052323128913 Test Loss: 2997.888865319057\n",
      "Epoch 2124/4000: Train Loss: 2837.403671950884 Test Loss: 2997.8864275012334\n",
      "Epoch 2125/4000: Train Loss: 2837.4021132299063 Test Loss: 2997.88399199114\n",
      "Epoch 2126/4000: Train Loss: 2837.400556148232 Test Loss: 2997.8815587864856\n",
      "Epoch 2127/4000: Train Loss: 2837.399000704138 Test Loss: 2997.8791278849762\n",
      "Epoch 2128/4000: Train Loss: 2837.397446895902 Test Loss: 2997.876699284328\n",
      "Epoch 2129/4000: Train Loss: 2837.3958947218016 Test Loss: 2997.874272982254\n",
      "Epoch 2130/4000: Train Loss: 2837.3943441801207 Test Loss: 2997.871848976472\n",
      "Epoch 2131/4000: Train Loss: 2837.3927952691415 Test Loss: 2997.8694272647017\n",
      "Epoch 2132/4000: Train Loss: 2837.3912479871487 Test Loss: 2997.8670078446635\n",
      "Epoch 2133/4000: Train Loss: 2837.389702332429 Test Loss: 2997.864590714084\n",
      "Epoch 2134/4000: Train Loss: 2837.3881583032717 Test Loss: 2997.8621758706868\n",
      "Epoch 2135/4000: Train Loss: 2837.386615897967 Test Loss: 2997.8597633122013\n",
      "Epoch 2136/4000: Train Loss: 2837.385075114807 Test Loss: 2997.857353036361\n",
      "Epoch 2137/4000: Train Loss: 2837.383535952085 Test Loss: 2997.854945040897\n",
      "Epoch 2138/4000: Train Loss: 2837.3819984080983 Test Loss: 2997.852539323545\n",
      "Epoch 2139/4000: Train Loss: 2837.380462481143 Test Loss: 2997.850135882045\n",
      "Epoch 2140/4000: Train Loss: 2837.378928169519 Test Loss: 2997.8477347141347\n",
      "Epoch 2141/4000: Train Loss: 2837.3773954715275 Test Loss: 2997.8453358175575\n",
      "Epoch 2142/4000: Train Loss: 2837.3758643854717 Test Loss: 2997.842939190059\n",
      "Epoch 2143/4000: Train Loss: 2837.374334909656 Test Loss: 2997.840544829385\n",
      "Epoch 2144/4000: Train Loss: 2837.3728070423867 Test Loss: 2997.838152733288\n",
      "Epoch 2145/4000: Train Loss: 2837.3712807819734 Test Loss: 2997.8357628995172\n",
      "Epoch 2146/4000: Train Loss: 2837.3697561267245 Test Loss: 2997.833375325827\n",
      "Epoch 2147/4000: Train Loss: 2837.3682330749525 Test Loss: 2997.830990009976\n",
      "Epoch 2148/4000: Train Loss: 2837.3667116249712 Test Loss: 2997.8286069497194\n",
      "Epoch 2149/4000: Train Loss: 2837.365191775096 Test Loss: 2997.8262261428213\n",
      "Epoch 2150/4000: Train Loss: 2837.3636735236437 Test Loss: 2997.823847587042\n",
      "Epoch 2151/4000: Train Loss: 2837.362156868933 Test Loss: 2997.8214712801505\n",
      "Epoch 2152/4000: Train Loss: 2837.360641809286 Test Loss: 2997.819097219912\n",
      "Epoch 2153/4000: Train Loss: 2837.3591283430237 Test Loss: 2997.816725404097\n",
      "Epoch 2154/4000: Train Loss: 2837.3576164684714 Test Loss: 2997.8143558304773\n",
      "Epoch 2155/4000: Train Loss: 2837.356106183954 Test Loss: 2997.811988496831\n",
      "Epoch 2156/4000: Train Loss: 2837.3545974878007 Test Loss: 2997.8096234009304\n",
      "Epoch 2157/4000: Train Loss: 2837.353090378339 Test Loss: 2997.8072605405578\n",
      "Epoch 2158/4000: Train Loss: 2837.3515848539023 Test Loss: 2997.804899913494\n",
      "Epoch 2159/4000: Train Loss: 2837.3500809128227 Test Loss: 2997.802541517521\n",
      "Epoch 2160/4000: Train Loss: 2837.3485785534344 Test Loss: 2997.8001853504275\n",
      "Epoch 2161/4000: Train Loss: 2837.3470777740745 Test Loss: 2997.79783141\n",
      "Epoch 2162/4000: Train Loss: 2837.345578573082 Test Loss: 2997.7954796940294\n",
      "Epoch 2163/4000: Train Loss: 2837.3440809487956 Test Loss: 2997.7931302003076\n",
      "Epoch 2164/4000: Train Loss: 2837.342584899558 Test Loss: 2997.790782926631\n",
      "Epoch 2165/4000: Train Loss: 2837.3410904237126 Test Loss: 2997.7884378707963\n",
      "Epoch 2166/4000: Train Loss: 2837.3395975196045 Test Loss: 2997.7860950306035\n",
      "Epoch 2167/4000: Train Loss: 2837.3381061855803 Test Loss: 2997.783754403853\n",
      "Epoch 2168/4000: Train Loss: 2837.3366164199897 Test Loss: 2997.7814159883496\n",
      "Epoch 2169/4000: Train Loss: 2837.335128221182 Test Loss: 2997.7790797819\n",
      "Epoch 2170/4000: Train Loss: 2837.3336415875106 Test Loss: 2997.7767457823124\n",
      "Epoch 2171/4000: Train Loss: 2837.332156517329 Test Loss: 2997.7744139873957\n",
      "Epoch 2172/4000: Train Loss: 2837.3306730089926 Test Loss: 2997.772084394967\n",
      "Epoch 2173/4000: Train Loss: 2837.3291910608586 Test Loss: 2997.769757002837\n",
      "Epoch 2174/4000: Train Loss: 2837.3277106712876 Test Loss: 2997.767431808825\n",
      "Epoch 2175/4000: Train Loss: 2837.3262318386383 Test Loss: 2997.7651088107505\n",
      "Epoch 2176/4000: Train Loss: 2837.3247545612753 Test Loss: 2997.762788006436\n",
      "Epoch 2177/4000: Train Loss: 2837.323278837561 Test Loss: 2997.760469393704\n",
      "Epoch 2178/4000: Train Loss: 2837.3218046658635 Test Loss: 2997.7581529703825\n",
      "Epoch 2179/4000: Train Loss: 2837.3203320445486 Test Loss: 2997.755838734299\n",
      "Epoch 2180/4000: Train Loss: 2837.318860971987 Test Loss: 2997.7535266832833\n",
      "Epoch 2181/4000: Train Loss: 2837.3173914465497 Test Loss: 2997.751216815171\n",
      "Epoch 2182/4000: Train Loss: 2837.315923466609 Test Loss: 2997.7489091277935\n",
      "Epoch 2183/4000: Train Loss: 2837.3144570305403 Test Loss: 2997.746603618992\n",
      "Epoch 2184/4000: Train Loss: 2837.3129921367195 Test Loss: 2997.744300286604\n",
      "Epoch 2185/4000: Train Loss: 2837.3115287835244 Test Loss: 2997.741999128471\n",
      "Epoch 2186/4000: Train Loss: 2837.310066969335 Test Loss: 2997.739700142438\n",
      "Epoch 2187/4000: Train Loss: 2837.308606692533 Test Loss: 2997.7374033263504\n",
      "Epoch 2188/4000: Train Loss: 2837.307147951501 Test Loss: 2997.735108678056\n",
      "Epoch 2189/4000: Train Loss: 2837.3056907446244 Test Loss: 2997.7328161954065\n",
      "Epoch 2190/4000: Train Loss: 2837.3042350702895 Test Loss: 2997.730525876254\n",
      "Epoch 2191/4000: Train Loss: 2837.302780926884 Test Loss: 2997.7282377184533\n",
      "Epoch 2192/4000: Train Loss: 2837.301328312799 Test Loss: 2997.7259517198627\n",
      "Epoch 2193/4000: Train Loss: 2837.299877226424 Test Loss: 2997.723667878342\n",
      "Epoch 2194/4000: Train Loss: 2837.298427666155 Test Loss: 2997.721386191749\n",
      "Epoch 2195/4000: Train Loss: 2837.2969796303855 Test Loss: 2997.719106657949\n",
      "Epoch 2196/4000: Train Loss: 2837.2955331175117 Test Loss: 2997.716829274809\n",
      "Epoch 2197/4000: Train Loss: 2837.294088125933 Test Loss: 2997.7145540401975\n",
      "Epoch 2198/4000: Train Loss: 2837.292644654049 Test Loss: 2997.7122809519815\n",
      "Epoch 2199/4000: Train Loss: 2837.291202700262 Test Loss: 2997.710010008038\n",
      "Epoch 2200/4000: Train Loss: 2837.2897622629753 Test Loss: 2997.707741206238\n",
      "Epoch 2201/4000: Train Loss: 2837.2883233405923 Test Loss: 2997.7054745444593\n",
      "Epoch 2202/4000: Train Loss: 2837.286885931522 Test Loss: 2997.7032100205784\n",
      "Epoch 2203/4000: Train Loss: 2837.285450034172 Test Loss: 2997.7009476324815\n",
      "Epoch 2204/4000: Train Loss: 2837.284015646952 Test Loss: 2997.698687378046\n",
      "Epoch 2205/4000: Train Loss: 2837.282582768275 Test Loss: 2997.696429255161\n",
      "Epoch 2206/4000: Train Loss: 2837.2811513965535 Test Loss: 2997.694173261714\n",
      "Epoch 2207/4000: Train Loss: 2837.279721530202 Test Loss: 2997.6919193955914\n",
      "Epoch 2208/4000: Train Loss: 2837.2782931676393 Test Loss: 2997.689667654689\n",
      "Epoch 2209/4000: Train Loss: 2837.276866307282 Test Loss: 2997.687418036899\n",
      "Epoch 2210/4000: Train Loss: 2837.275440947551 Test Loss: 2997.685170540117\n",
      "Epoch 2211/4000: Train Loss: 2837.274017086868 Test Loss: 2997.6829251622407\n",
      "Epoch 2212/4000: Train Loss: 2837.272594723657 Test Loss: 2997.6806819011717\n",
      "Epoch 2213/4000: Train Loss: 2837.2711738563416 Test Loss: 2997.6784407548116\n",
      "Epoch 2214/4000: Train Loss: 2837.2697544833504 Test Loss: 2997.676201721066\n",
      "Epoch 2215/4000: Train Loss: 2837.2683366031106 Test Loss: 2997.6739647978425\n",
      "Epoch 2216/4000: Train Loss: 2837.2669202140532 Test Loss: 2997.671729983049\n",
      "Epoch 2217/4000: Train Loss: 2837.265505314608 Test Loss: 2997.669497274594\n",
      "Epoch 2218/4000: Train Loss: 2837.264091903212 Test Loss: 2997.6672666703944\n",
      "Epoch 2219/4000: Train Loss: 2837.262679978296 Test Loss: 2997.665038168365\n",
      "Epoch 2220/4000: Train Loss: 2837.2612695383 Test Loss: 2997.6628117664195\n",
      "Epoch 2221/4000: Train Loss: 2837.25986058166 Test Loss: 2997.6605874624815\n",
      "Epoch 2222/4000: Train Loss: 2837.258453106818 Test Loss: 2997.658365254474\n",
      "Epoch 2223/4000: Train Loss: 2837.257047112214 Test Loss: 2997.6561451403154\n",
      "Epoch 2224/4000: Train Loss: 2837.2556425962916 Test Loss: 2997.6539271179345\n",
      "Epoch 2225/4000: Train Loss: 2837.2542395574956 Test Loss: 2997.6517111852604\n",
      "Epoch 2226/4000: Train Loss: 2837.2528379942732 Test Loss: 2997.649497340222\n",
      "Epoch 2227/4000: Train Loss: 2837.2514379050717 Test Loss: 2997.6472855807515\n",
      "Epoch 2228/4000: Train Loss: 2837.2500392883417 Test Loss: 2997.6450759047834\n",
      "Epoch 2229/4000: Train Loss: 2837.248642142534 Test Loss: 2997.6428683102545\n",
      "Epoch 2230/4000: Train Loss: 2837.2472464661023 Test Loss: 2997.6406627951005\n",
      "Epoch 2231/4000: Train Loss: 2837.2458522575 Test Loss: 2997.6384593572666\n",
      "Epoch 2232/4000: Train Loss: 2837.244459515184 Test Loss: 2997.636257994694\n",
      "Epoch 2233/4000: Train Loss: 2837.243068237613 Test Loss: 2997.6340587053264\n",
      "Epoch 2234/4000: Train Loss: 2837.241678423246 Test Loss: 2997.6318614871116\n",
      "Epoch 2235/4000: Train Loss: 2837.2402900705433 Test Loss: 2997.6296663379976\n",
      "Epoch 2236/4000: Train Loss: 2837.238903177969 Test Loss: 2997.6274732559373\n",
      "Epoch 2237/4000: Train Loss: 2837.2375177439867 Test Loss: 2997.625282238882\n",
      "Epoch 2238/4000: Train Loss: 2837.236133767063 Test Loss: 2997.623093284789\n",
      "Epoch 2239/4000: Train Loss: 2837.2347512456645 Test Loss: 2997.620906391613\n",
      "Epoch 2240/4000: Train Loss: 2837.233370178261 Test Loss: 2997.6187215573163\n",
      "Epoch 2241/4000: Train Loss: 2837.231990563324 Test Loss: 2997.6165387798587\n",
      "Epoch 2242/4000: Train Loss: 2837.230612399325 Test Loss: 2997.6143580572043\n",
      "Epoch 2243/4000: Train Loss: 2837.2292356847383 Test Loss: 2997.612179387319\n",
      "Epoch 2244/4000: Train Loss: 2837.22786041804 Test Loss: 2997.6100027681705\n",
      "Epoch 2245/4000: Train Loss: 2837.2264865977063 Test Loss: 2997.6078281977293\n",
      "Epoch 2246/4000: Train Loss: 2837.225114222217 Test Loss: 2997.605655673966\n",
      "Epoch 2247/4000: Train Loss: 2837.223743290053 Test Loss: 2997.6034851948534\n",
      "Epoch 2248/4000: Train Loss: 2837.222373799695 Test Loss: 2997.601316758371\n",
      "Epoch 2249/4000: Train Loss: 2837.2210057496272 Test Loss: 2997.599150362493\n",
      "Epoch 2250/4000: Train Loss: 2837.2196391383354 Test Loss: 2997.5969860052046\n",
      "Epoch 2251/4000: Train Loss: 2837.2182739643063 Test Loss: 2997.594823684483\n",
      "Epoch 2252/4000: Train Loss: 2837.2169102260273 Test Loss: 2997.592663398314\n",
      "Epoch 2253/4000: Train Loss: 2837.215547921989 Test Loss: 2997.590505144688\n",
      "Epoch 2254/4000: Train Loss: 2837.2141870506844 Test Loss: 2997.5883489215867\n",
      "Epoch 2255/4000: Train Loss: 2837.212827610604 Test Loss: 2997.586194727004\n",
      "Epoch 2256/4000: Train Loss: 2837.211469600245 Test Loss: 2997.584042558932\n",
      "Epoch 2257/4000: Train Loss: 2837.2101130181027 Test Loss: 2997.5818924153646\n",
      "Epoch 2258/4000: Train Loss: 2837.208757862675 Test Loss: 2997.5797442943017\n",
      "Epoch 2259/4000: Train Loss: 2837.2074041324618 Test Loss: 2997.5775981937363\n",
      "Epoch 2260/4000: Train Loss: 2837.206051825964 Test Loss: 2997.5754541116735\n",
      "Epoch 2261/4000: Train Loss: 2837.204700941684 Test Loss: 2997.5733120461127\n",
      "Epoch 2262/4000: Train Loss: 2837.203351478127 Test Loss: 2997.571171995062\n",
      "Epoch 2263/4000: Train Loss: 2837.2020034337975 Test Loss: 2997.5690339565253\n",
      "Epoch 2264/4000: Train Loss: 2837.2006568072043 Test Loss: 2997.5668979285133\n",
      "Epoch 2265/4000: Train Loss: 2837.199311596855 Test Loss: 2997.5647639090357\n",
      "Epoch 2266/4000: Train Loss: 2837.1979678012613 Test Loss: 2997.5626318961054\n",
      "Epoch 2267/4000: Train Loss: 2837.196625418934 Test Loss: 2997.5605018877377\n",
      "Epoch 2268/4000: Train Loss: 2837.195284448388 Test Loss: 2997.55837388195\n",
      "Epoch 2269/4000: Train Loss: 2837.193944888138 Test Loss: 2997.556247876759\n",
      "Epoch 2270/4000: Train Loss: 2837.1926067367012 Test Loss: 2997.554123870187\n",
      "Epoch 2271/4000: Train Loss: 2837.1912699925956 Test Loss: 2997.5520018602574\n",
      "Epoch 2272/4000: Train Loss: 2837.189934654341 Test Loss: 2997.549881844994\n",
      "Epoch 2273/4000: Train Loss: 2837.1886007204594 Test Loss: 2997.547763822426\n",
      "Epoch 2274/4000: Train Loss: 2837.1872681894724 Test Loss: 2997.5456477905796\n",
      "Epoch 2275/4000: Train Loss: 2837.185937059907 Test Loss: 2997.5435337474864\n",
      "Epoch 2276/4000: Train Loss: 2837.184607330287 Test Loss: 2997.5414216911813\n",
      "Epoch 2277/4000: Train Loss: 2837.183278999141 Test Loss: 2997.539311619696\n",
      "Epoch 2278/4000: Train Loss: 2837.1819520649988 Test Loss: 2997.537203531071\n",
      "Epoch 2279/4000: Train Loss: 2837.1806265263904 Test Loss: 2997.5350974233424\n",
      "Epoch 2280/4000: Train Loss: 2837.179302381848 Test Loss: 2997.5329932945524\n",
      "Epoch 2281/4000: Train Loss: 2837.1779796299065 Test Loss: 2997.5308911427446\n",
      "Epoch 2282/4000: Train Loss: 2837.1766582691002 Test Loss: 2997.5287909659637\n",
      "Epoch 2283/4000: Train Loss: 2837.1753382979664 Test Loss: 2997.5266927622542\n",
      "Epoch 2284/4000: Train Loss: 2837.1740197150443 Test Loss: 2997.524596529667\n",
      "Epoch 2285/4000: Train Loss: 2837.172702518872 Test Loss: 2997.5225022662544\n",
      "Epoch 2286/4000: Train Loss: 2837.171386707994 Test Loss: 2997.520409970069\n",
      "Epoch 2287/4000: Train Loss: 2837.17007228095 Test Loss: 2997.518319639162\n",
      "Epoch 2288/4000: Train Loss: 2837.168759236288 Test Loss: 2997.516231271594\n",
      "Epoch 2289/4000: Train Loss: 2837.1674475725517 Test Loss: 2997.5141448654235\n",
      "Epoch 2290/4000: Train Loss: 2837.166137288289 Test Loss: 2997.5120604187105\n",
      "Epoch 2291/4000: Train Loss: 2837.1648283820505 Test Loss: 2997.509977929516\n",
      "Epoch 2292/4000: Train Loss: 2837.163520852386 Test Loss: 2997.5078973959075\n",
      "Epoch 2293/4000: Train Loss: 2837.162214697847 Test Loss: 2997.505818815952\n",
      "Epoch 2294/4000: Train Loss: 2837.1609099169896 Test Loss: 2997.5037421877146\n",
      "Epoch 2295/4000: Train Loss: 2837.159606508367 Test Loss: 2997.50166750927\n",
      "Epoch 2296/4000: Train Loss: 2837.158304470536 Test Loss: 2997.4995947786892\n",
      "Epoch 2297/4000: Train Loss: 2837.1570038020573 Test Loss: 2997.497523994047\n",
      "Epoch 2298/4000: Train Loss: 2837.155704501488 Test Loss: 2997.49545515342\n",
      "Epoch 2299/4000: Train Loss: 2837.1544065673907 Test Loss: 2997.4933882548858\n",
      "Epoch 2300/4000: Train Loss: 2837.153109998328 Test Loss: 2997.4913232965255\n",
      "Epoch 2301/4000: Train Loss: 2837.1518147928646 Test Loss: 2997.4892602764226\n",
      "Epoch 2302/4000: Train Loss: 2837.1505209495663 Test Loss: 2997.48719919266\n",
      "Epoch 2303/4000: Train Loss: 2837.149228467 Test Loss: 2997.4851400433236\n",
      "Epoch 2304/4000: Train Loss: 2837.1479373437355 Test Loss: 2997.4830828265026\n",
      "Epoch 2305/4000: Train Loss: 2837.1466475783423 Test Loss: 2997.4810275402874\n",
      "Epoch 2306/4000: Train Loss: 2837.1453591693935 Test Loss: 2997.4789741827703\n",
      "Epoch 2307/4000: Train Loss: 2837.144072115462 Test Loss: 2997.4769227520446\n",
      "Epoch 2308/4000: Train Loss: 2837.1427864151224 Test Loss: 2997.4748732462067\n",
      "Epoch 2309/4000: Train Loss: 2837.1415020669506 Test Loss: 2997.4728256633557\n",
      "Epoch 2310/4000: Train Loss: 2837.1402190695258 Test Loss: 2997.4707800015904\n",
      "Epoch 2311/4000: Train Loss: 2837.1389374214273 Test Loss: 2997.468736259012\n",
      "Epoch 2312/4000: Train Loss: 2837.137657121235 Test Loss: 2997.4666944337246\n",
      "Epoch 2313/4000: Train Loss: 2837.136378167532 Test Loss: 2997.464654523835\n",
      "Epoch 2314/4000: Train Loss: 2837.135100558902 Test Loss: 2997.4626165274494\n",
      "Epoch 2315/4000: Train Loss: 2837.133824293931 Test Loss: 2997.460580442679\n",
      "Epoch 2316/4000: Train Loss: 2837.1325493712047 Test Loss: 2997.458546267633\n",
      "Epoch 2317/4000: Train Loss: 2837.131275789313 Test Loss: 2997.456514000428\n",
      "Epoch 2318/4000: Train Loss: 2837.1300035468444 Test Loss: 2997.454483639177\n",
      "Epoch 2319/4000: Train Loss: 2837.128732642391 Test Loss: 2997.4524551819977\n",
      "Epoch 2320/4000: Train Loss: 2837.1274630745447 Test Loss: 2997.4504286270094\n",
      "Epoch 2321/4000: Train Loss: 2837.126194841901 Test Loss: 2997.448403972332\n",
      "Epoch 2322/4000: Train Loss: 2837.124927943055 Test Loss: 2997.446381216091\n",
      "Epoch 2323/4000: Train Loss: 2837.123662376603 Test Loss: 2997.444360356411\n",
      "Epoch 2324/4000: Train Loss: 2837.1223981411454 Test Loss: 2997.442341391416\n",
      "Epoch 2325/4000: Train Loss: 2837.121135235282 Test Loss: 2997.440324319236\n",
      "Epoch 2326/4000: Train Loss: 2837.1198736576143 Test Loss: 2997.4383091380037\n",
      "Epoch 2327/4000: Train Loss: 2837.118613406745 Test Loss: 2997.4362958458482\n",
      "Epoch 2328/4000: Train Loss: 2837.11735448128 Test Loss: 2997.4342844409066\n",
      "Epoch 2329/4000: Train Loss: 2837.1160968798235 Test Loss: 2997.4322749213165\n",
      "Epoch 2330/4000: Train Loss: 2837.1148406009843 Test Loss: 2997.4302672852104\n",
      "Epoch 2331/4000: Train Loss: 2837.113585643371 Test Loss: 2997.4282615307334\n",
      "Epoch 2332/4000: Train Loss: 2837.112332005594 Test Loss: 2997.426257656027\n",
      "Epoch 2333/4000: Train Loss: 2837.111079686266 Test Loss: 2997.424255659232\n",
      "Epoch 2334/4000: Train Loss: 2837.1098286839992 Test Loss: 2997.4222555384995\n",
      "Epoch 2335/4000: Train Loss: 2837.108578997409 Test Loss: 2997.420257291971\n",
      "Epoch 2336/4000: Train Loss: 2837.107330625112 Test Loss: 2997.4182609178006\n",
      "Epoch 2337/4000: Train Loss: 2837.106083565726 Test Loss: 2997.416266414137\n",
      "Epoch 2338/4000: Train Loss: 2837.104837817869 Test Loss: 2997.4142737791362\n",
      "Epoch 2339/4000: Train Loss: 2837.103593380163 Test Loss: 2997.412283010952\n",
      "Epoch 2340/4000: Train Loss: 2837.10235025123 Test Loss: 2997.4102941077394\n",
      "Epoch 2341/4000: Train Loss: 2837.101108429693 Test Loss: 2997.4083070676597\n",
      "Epoch 2342/4000: Train Loss: 2837.099867914177 Test Loss: 2997.406321888874\n",
      "Epoch 2343/4000: Train Loss: 2837.0986287033093 Test Loss: 2997.4043385695454\n",
      "Epoch 2344/4000: Train Loss: 2837.097390795717 Test Loss: 2997.402357107836\n",
      "Epoch 2345/4000: Train Loss: 2837.0961541900297 Test Loss: 2997.400377501914\n",
      "Epoch 2346/4000: Train Loss: 2837.0949188848786 Test Loss: 2997.398399749946\n",
      "Epoch 2347/4000: Train Loss: 2837.0936848788947 Test Loss: 2997.3964238501053\n",
      "Epoch 2348/4000: Train Loss: 2837.092452170713 Test Loss: 2997.3944498005612\n",
      "Epoch 2349/4000: Train Loss: 2837.0912207589677 Test Loss: 2997.392477599489\n",
      "Epoch 2350/4000: Train Loss: 2837.089990642296 Test Loss: 2997.3905072450625\n",
      "Epoch 2351/4000: Train Loss: 2837.088761819335 Test Loss: 2997.3885387354608\n",
      "Epoch 2352/4000: Train Loss: 2837.087534288726 Test Loss: 2997.3865720688646\n",
      "Epoch 2353/4000: Train Loss: 2837.0863080491076 Test Loss: 2997.384607243453\n",
      "Epoch 2354/4000: Train Loss: 2837.085083099123 Test Loss: 2997.38264425741\n",
      "Epoch 2355/4000: Train Loss: 2837.0838594374163 Test Loss: 2997.38068310892\n",
      "Epoch 2356/4000: Train Loss: 2837.082637062632 Test Loss: 2997.3787237961697\n",
      "Epoch 2357/4000: Train Loss: 2837.081415973417 Test Loss: 2997.376766317349\n",
      "Epoch 2358/4000: Train Loss: 2837.0801961684188 Test Loss: 2997.3748106706475\n",
      "Epoch 2359/4000: Train Loss: 2837.078977646287 Test Loss: 2997.372856854259\n",
      "Epoch 2360/4000: Train Loss: 2837.077760405673 Test Loss: 2997.370904866377\n",
      "Epoch 2361/4000: Train Loss: 2837.076544445228 Test Loss: 2997.3689547051954\n",
      "Epoch 2362/4000: Train Loss: 2837.0753297636074 Test Loss: 2997.367006368914\n",
      "Epoch 2363/4000: Train Loss: 2837.074116359464 Test Loss: 2997.365059855733\n",
      "Epoch 2364/4000: Train Loss: 2837.072904231456 Test Loss: 2997.363115163853\n",
      "Epoch 2365/4000: Train Loss: 2837.07169337824 Test Loss: 2997.3611722914766\n",
      "Epoch 2366/4000: Train Loss: 2837.0704837984763 Test Loss: 2997.3592312368114\n",
      "Epoch 2367/4000: Train Loss: 2837.0692754908255 Test Loss: 2997.357291998062\n",
      "Epoch 2368/4000: Train Loss: 2837.0680684539493 Test Loss: 2997.355354573438\n",
      "Epoch 2369/4000: Train Loss: 2837.0668626865113 Test Loss: 2997.3534189611505\n",
      "Epoch 2370/4000: Train Loss: 2837.0656581871763 Test Loss: 2997.3514851594123\n",
      "Epoch 2371/4000: Train Loss: 2837.0644549546114 Test Loss: 2997.349553166436\n",
      "Epoch 2372/4000: Train Loss: 2837.063252987484 Test Loss: 2997.34762298044\n",
      "Epoch 2373/4000: Train Loss: 2837.062052284462 Test Loss: 2997.3456945996395\n",
      "Epoch 2374/4000: Train Loss: 2837.0608528442176 Test Loss: 2997.3437680222573\n",
      "Epoch 2375/4000: Train Loss: 2837.0596546654224 Test Loss: 2997.3418432465132\n",
      "Epoch 2376/4000: Train Loss: 2837.0584577467494 Test Loss: 2997.33992027063\n",
      "Epoch 2377/4000: Train Loss: 2837.0572620868734 Test Loss: 2997.337999092834\n",
      "Epoch 2378/4000: Train Loss: 2837.0560676844707 Test Loss: 2997.3360797113523\n",
      "Epoch 2379/4000: Train Loss: 2837.054874538218 Test Loss: 2997.334162124412\n",
      "Epoch 2380/4000: Train Loss: 2837.0536826467956 Test Loss: 2997.3322463302457\n",
      "Epoch 2381/4000: Train Loss: 2837.052492008883 Test Loss: 2997.3303323270843\n",
      "Epoch 2382/4000: Train Loss: 2837.051302623161 Test Loss: 2997.3284201131637\n",
      "Epoch 2383/4000: Train Loss: 2837.050114488315 Test Loss: 2997.3265096867176\n",
      "Epoch 2384/4000: Train Loss: 2837.0489276030276 Test Loss: 2997.324601045985\n",
      "Epoch 2385/4000: Train Loss: 2837.0477419659846 Test Loss: 2997.3226941892053\n",
      "Epoch 2386/4000: Train Loss: 2837.0465575758744 Test Loss: 2997.3207891146203\n",
      "Epoch 2387/4000: Train Loss: 2837.045374431385 Test Loss: 2997.318885820472\n",
      "Epoch 2388/4000: Train Loss: 2837.044192531206 Test Loss: 2997.3169843050064\n",
      "Epoch 2389/4000: Train Loss: 2837.043011874029 Test Loss: 2997.3150845664686\n",
      "Epoch 2390/4000: Train Loss: 2837.0418324585467 Test Loss: 2997.313186603109\n",
      "Epoch 2391/4000: Train Loss: 2837.0406542834544 Test Loss: 2997.3112904131767\n",
      "Epoch 2392/4000: Train Loss: 2837.0394773474454 Test Loss: 2997.3093959949256\n",
      "Epoch 2393/4000: Train Loss: 2837.038301649218 Test Loss: 2997.3075033466066\n",
      "Epoch 2394/4000: Train Loss: 2837.03712718747 Test Loss: 2997.305612466477\n",
      "Epoch 2395/4000: Train Loss: 2837.035953960902 Test Loss: 2997.3037233527934\n",
      "Epoch 2396/4000: Train Loss: 2837.034781968213 Test Loss: 2997.301836003816\n",
      "Epoch 2397/4000: Train Loss: 2837.033611208107 Test Loss: 2997.299950417805\n",
      "Epoch 2398/4000: Train Loss: 2837.032441679287 Test Loss: 2997.298066593022\n",
      "Epoch 2399/4000: Train Loss: 2837.031273380458 Test Loss: 2997.2961845277327\n",
      "Epoch 2400/4000: Train Loss: 2837.0301063103266 Test Loss: 2997.294304220205\n",
      "Epoch 2401/4000: Train Loss: 2837.028940467601 Test Loss: 2997.2924256687033\n",
      "Epoch 2402/4000: Train Loss: 2837.027775850989 Test Loss: 2997.2905488715\n",
      "Epoch 2403/4000: Train Loss: 2837.026612459203 Test Loss: 2997.288673826864\n",
      "Epoch 2404/4000: Train Loss: 2837.0254502909534 Test Loss: 2997.286800533071\n",
      "Epoch 2405/4000: Train Loss: 2837.024289344954 Test Loss: 2997.284928988395\n",
      "Epoch 2406/4000: Train Loss: 2837.023129619919 Test Loss: 2997.2830591911124\n",
      "Epoch 2407/4000: Train Loss: 2837.021971114564 Test Loss: 2997.281191139502\n",
      "Epoch 2408/4000: Train Loss: 2837.020813827608 Test Loss: 2997.279324831845\n",
      "Epoch 2409/4000: Train Loss: 2837.019657757768 Test Loss: 2997.2774602664217\n",
      "Epoch 2410/4000: Train Loss: 2837.018502903765 Test Loss: 2997.2755974415168\n",
      "Epoch 2411/4000: Train Loss: 2837.0173492643185 Test Loss: 2997.2737363554165\n",
      "Epoch 2412/4000: Train Loss: 2837.0161968381526 Test Loss: 2997.271877006406\n",
      "Epoch 2413/4000: Train Loss: 2837.015045623991 Test Loss: 2997.270019392775\n",
      "Epoch 2414/4000: Train Loss: 2837.0138956205597 Test Loss: 2997.2681635128147\n",
      "Epoch 2415/4000: Train Loss: 2837.012746826584 Test Loss: 2997.2663093648193\n",
      "Epoch 2416/4000: Train Loss: 2837.0115992407937 Test Loss: 2997.2644569470795\n",
      "Epoch 2417/4000: Train Loss: 2837.0104528619163 Test Loss: 2997.262606257892\n",
      "Epoch 2418/4000: Train Loss: 2837.0093076886837 Test Loss: 2997.2607572955562\n",
      "Epoch 2419/4000: Train Loss: 2837.008163719827 Test Loss: 2997.258910058371\n",
      "Epoch 2420/4000: Train Loss: 2837.0070209540804 Test Loss: 2997.2570645446344\n",
      "Epoch 2421/4000: Train Loss: 2837.0058793901785 Test Loss: 2997.2552207526546\n",
      "Epoch 2422/4000: Train Loss: 2837.004739026857 Test Loss: 2997.2533786807317\n",
      "Epoch 2423/4000: Train Loss: 2837.0035998628528 Test Loss: 2997.2515383271734\n",
      "Epoch 2424/4000: Train Loss: 2837.002461896906 Test Loss: 2997.249699690288\n",
      "Epoch 2425/4000: Train Loss: 2837.001325127756 Test Loss: 2997.247862768386\n",
      "Epoch 2426/4000: Train Loss: 2837.000189554143 Test Loss: 2997.246027559777\n",
      "Epoch 2427/4000: Train Loss: 2836.999055174811 Test Loss: 2997.2441940627746\n",
      "Epoch 2428/4000: Train Loss: 2836.997921988503 Test Loss: 2997.2423622756946\n",
      "Epoch 2429/4000: Train Loss: 2836.996789993965 Test Loss: 2997.240532196852\n",
      "Epoch 2430/4000: Train Loss: 2836.9956591899445 Test Loss: 2997.238703824567\n",
      "Epoch 2431/4000: Train Loss: 2836.9945295751877 Test Loss: 2997.2368771571573\n",
      "Epoch 2432/4000: Train Loss: 2836.9934011484443 Test Loss: 2997.2350521929466\n",
      "Epoch 2433/4000: Train Loss: 2836.992273908465 Test Loss: 2997.233228930258\n",
      "Epoch 2434/4000: Train Loss: 2836.9911478540016 Test Loss: 2997.231407367416\n",
      "Epoch 2435/4000: Train Loss: 2836.990022983808 Test Loss: 2997.2295875027467\n",
      "Epoch 2436/4000: Train Loss: 2836.988899296638 Test Loss: 2997.227769334581\n",
      "Epoch 2437/4000: Train Loss: 2836.987776791247 Test Loss: 2997.225952861246\n",
      "Epoch 2438/4000: Train Loss: 2836.9866554663936 Test Loss: 2997.224138081075\n",
      "Epoch 2439/4000: Train Loss: 2836.985535320835 Test Loss: 2997.222324992403\n",
      "Epoch 2440/4000: Train Loss: 2836.984416353331 Test Loss: 2997.2205135935624\n",
      "Epoch 2441/4000: Train Loss: 2836.9832985626435 Test Loss: 2997.2187038828924\n",
      "Epoch 2442/4000: Train Loss: 2836.982181947534 Test Loss: 2997.21689585873\n",
      "Epoch 2443/4000: Train Loss: 2836.9810665067666 Test Loss: 2997.215089519418\n",
      "Epoch 2444/4000: Train Loss: 2836.979952239106 Test Loss: 2997.213284863296\n",
      "Epoch 2445/4000: Train Loss: 2836.978839143318 Test Loss: 2997.21148188871\n",
      "Epoch 2446/4000: Train Loss: 2836.9777272181714 Test Loss: 2997.2096805940014\n",
      "Epoch 2447/4000: Train Loss: 2836.9766164624343 Test Loss: 2997.207880977521\n",
      "Epoch 2448/4000: Train Loss: 2836.9755068748764 Test Loss: 2997.2060830376163\n",
      "Epoch 2449/4000: Train Loss: 2836.9743984542692 Test Loss: 2997.2042867726377\n",
      "Epoch 2450/4000: Train Loss: 2836.9732911993865 Test Loss: 2997.202492180937\n",
      "Epoch 2451/4000: Train Loss: 2836.972185109001 Test Loss: 2997.200699260869\n",
      "Epoch 2452/4000: Train Loss: 2836.971080181889 Test Loss: 2997.1989080107883\n",
      "Epoch 2453/4000: Train Loss: 2836.969976416827 Test Loss: 2997.1971184290524\n",
      "Epoch 2454/4000: Train Loss: 2836.968873812592 Test Loss: 2997.195330514019\n",
      "Epoch 2455/4000: Train Loss: 2836.967772367964 Test Loss: 2997.1935442640506\n",
      "Epoch 2456/4000: Train Loss: 2836.9666720817227 Test Loss: 2997.191759677508\n",
      "Epoch 2457/4000: Train Loss: 2836.9655729526507 Test Loss: 2997.1899767527534\n",
      "Epoch 2458/4000: Train Loss: 2836.9644749795307 Test Loss: 2997.1881954881555\n",
      "Epoch 2459/4000: Train Loss: 2836.9633781611465 Test Loss: 2997.18641588208\n",
      "Epoch 2460/4000: Train Loss: 2836.9622824962844 Test Loss: 2997.184637932894\n",
      "Epoch 2461/4000: Train Loss: 2836.9611879837307 Test Loss: 2997.18286163897\n",
      "Epoch 2462/4000: Train Loss: 2836.9600946222736 Test Loss: 2997.1810869986803\n",
      "Epoch 2463/4000: Train Loss: 2836.9590024107024 Test Loss: 2997.1793140103973\n",
      "Epoch 2464/4000: Train Loss: 2836.9579113478085 Test Loss: 2997.177542672496\n",
      "Epoch 2465/4000: Train Loss: 2836.956821432383 Test Loss: 2997.1757729833553\n",
      "Epoch 2466/4000: Train Loss: 2836.95573266322 Test Loss: 2997.1740049413534\n",
      "Epoch 2467/4000: Train Loss: 2836.9546450391126 Test Loss: 2997.1722385448693\n",
      "Epoch 2468/4000: Train Loss: 2836.953558558858 Test Loss: 2997.1704737922846\n",
      "Epoch 2469/4000: Train Loss: 2836.9524732212517 Test Loss: 2997.168710681984\n",
      "Epoch 2470/4000: Train Loss: 2836.9513890250932 Test Loss: 2997.1669492123533\n",
      "Epoch 2471/4000: Train Loss: 2836.950305969182 Test Loss: 2997.165189381779\n",
      "Epoch 2472/4000: Train Loss: 2836.949224052318 Test Loss: 2997.1634311886496\n",
      "Epoch 2473/4000: Train Loss: 2836.948143273305 Test Loss: 2997.1616746313534\n",
      "Epoch 2474/4000: Train Loss: 2836.9470636309443 Test Loss: 2997.1599197082855\n",
      "Epoch 2475/4000: Train Loss: 2836.9459851240413 Test Loss: 2997.158166417837\n",
      "Epoch 2476/4000: Train Loss: 2836.944907751402 Test Loss: 2997.156414758404\n",
      "Epoch 2477/4000: Train Loss: 2836.943831511833 Test Loss: 2997.1546647283817\n",
      "Epoch 2478/4000: Train Loss: 2836.9427564041434 Test Loss: 2997.1529163261703\n",
      "Epoch 2479/4000: Train Loss: 2836.941682427143 Test Loss: 2997.151169550168\n",
      "Epoch 2480/4000: Train Loss: 2836.940609579641 Test Loss: 2997.149424398778\n",
      "Epoch 2481/4000: Train Loss: 2836.9395378604513 Test Loss: 2997.147680870402\n",
      "Epoch 2482/4000: Train Loss: 2836.938467268387 Test Loss: 2997.145938963445\n",
      "Epoch 2483/4000: Train Loss: 2836.937397802262 Test Loss: 2997.1441986763157\n",
      "Epoch 2484/4000: Train Loss: 2836.9363294608916 Test Loss: 2997.142460007418\n",
      "Epoch 2485/4000: Train Loss: 2836.9352622430947 Test Loss: 2997.140722955165\n",
      "Epoch 2486/4000: Train Loss: 2836.934196147688 Test Loss: 2997.138987517967\n",
      "Epoch 2487/4000: Train Loss: 2836.9331311734923 Test Loss: 2997.1372536942363\n",
      "Epoch 2488/4000: Train Loss: 2836.932067319328 Test Loss: 2997.135521482388\n",
      "Epoch 2489/4000: Train Loss: 2836.9310045840166 Test Loss: 2997.133790880837\n",
      "Epoch 2490/4000: Train Loss: 2836.9299429663815 Test Loss: 2997.1320618880027\n",
      "Epoch 2491/4000: Train Loss: 2836.928882465248 Test Loss: 2997.1303345023043\n",
      "Epoch 2492/4000: Train Loss: 2836.9278230794416 Test Loss: 2997.1286087221606\n",
      "Epoch 2493/4000: Train Loss: 2836.926764807789 Test Loss: 2997.1268845459963\n",
      "Epoch 2494/4000: Train Loss: 2836.925707649119 Test Loss: 2997.1251619722325\n",
      "Epoch 2495/4000: Train Loss: 2836.92465160226 Test Loss: 2997.1234409993003\n",
      "Epoch 2496/4000: Train Loss: 2836.923596666044 Test Loss: 2997.1217216256227\n",
      "Epoch 2497/4000: Train Loss: 2836.922542839302 Test Loss: 2997.12000384963\n",
      "Epoch 2498/4000: Train Loss: 2836.9214901208675 Test Loss: 2997.118287669751\n",
      "Epoch 2499/4000: Train Loss: 2836.920438509575 Test Loss: 2997.1165730844205\n",
      "Epoch 2500/4000: Train Loss: 2836.91938800426 Test Loss: 2997.114860092071\n",
      "Epoch 2501/4000: Train Loss: 2836.9183386037594 Test Loss: 2997.1131486911386\n",
      "Epoch 2502/4000: Train Loss: 2836.9172903069116 Test Loss: 2997.1114388800574\n",
      "Epoch 2503/4000: Train Loss: 2836.916243112555 Test Loss: 2997.109730657268\n",
      "Epoch 2504/4000: Train Loss: 2836.915197019531 Test Loss: 2997.1080240212104\n",
      "Epoch 2505/4000: Train Loss: 2836.9141520266808 Test Loss: 2997.106318970324\n",
      "Epoch 2506/4000: Train Loss: 2836.913108132848 Test Loss: 2997.1046155030563\n",
      "Epoch 2507/4000: Train Loss: 2836.912065336876 Test Loss: 2997.102913617849\n",
      "Epoch 2508/4000: Train Loss: 2836.91102363761 Test Loss: 2997.1012133131476\n",
      "Epoch 2509/4000: Train Loss: 2836.9099830338973 Test Loss: 2997.0995145874044\n",
      "Epoch 2510/4000: Train Loss: 2836.908943524586 Test Loss: 2997.0978174390634\n",
      "Epoch 2511/4000: Train Loss: 2836.9079051085246 Test Loss: 2997.0961218665784\n",
      "Epoch 2512/4000: Train Loss: 2836.9068677845626 Test Loss: 2997.094427868404\n",
      "Epoch 2513/4000: Train Loss: 2836.9058315515526 Test Loss: 2997.0927354429905\n",
      "Epoch 2514/4000: Train Loss: 2836.904796408347 Test Loss: 2997.0910445887953\n",
      "Epoch 2515/4000: Train Loss: 2836.9037623537993 Test Loss: 2997.0893553042765\n",
      "Epoch 2516/4000: Train Loss: 2836.902729386765 Test Loss: 2997.0876675878912\n",
      "Epoch 2517/4000: Train Loss: 2836.901697506099 Test Loss: 2997.0859814381038\n",
      "Epoch 2518/4000: Train Loss: 2836.900666710661 Test Loss: 2997.0842968533716\n",
      "Epoch 2519/4000: Train Loss: 2836.8996369993083 Test Loss: 2997.0826138321595\n",
      "Epoch 2520/4000: Train Loss: 2836.8986083709005 Test Loss: 2997.0809323729354\n",
      "Epoch 2521/4000: Train Loss: 2836.8975808243 Test Loss: 2997.079252474162\n",
      "Epoch 2522/4000: Train Loss: 2836.896554358368 Test Loss: 2997.0775741343105\n",
      "Epoch 2523/4000: Train Loss: 2836.895528971968 Test Loss: 2997.0758973518496\n",
      "Epoch 2524/4000: Train Loss: 2836.894504663965 Test Loss: 2997.074222125251\n",
      "Epoch 2525/4000: Train Loss: 2836.8934814332247 Test Loss: 2997.072548452987\n",
      "Epoch 2526/4000: Train Loss: 2836.8924592786143 Test Loss: 2997.0708763335324\n",
      "Epoch 2527/4000: Train Loss: 2836.8914381990016 Test Loss: 2997.0692057653628\n",
      "Epoch 2528/4000: Train Loss: 2836.8904181932567 Test Loss: 2997.067536746956\n",
      "Epoch 2529/4000: Train Loss: 2836.889399260249 Test Loss: 2997.0658692767925\n",
      "Epoch 2530/4000: Train Loss: 2836.888381398852 Test Loss: 2997.064203353351\n",
      "Epoch 2531/4000: Train Loss: 2836.8873646079373 Test Loss: 2997.0625389751144\n",
      "Epoch 2532/4000: Train Loss: 2836.88634888638 Test Loss: 2997.060876140567\n",
      "Epoch 2533/4000: Train Loss: 2836.885334233055 Test Loss: 2997.0592148481924\n",
      "Epoch 2534/4000: Train Loss: 2836.8843206468387 Test Loss: 2997.0575550964786\n",
      "Epoch 2535/4000: Train Loss: 2836.8833081266093 Test Loss: 2997.055896883916\n",
      "Epoch 2536/4000: Train Loss: 2836.8822966712455 Test Loss: 2997.054240208989\n",
      "Epoch 2537/4000: Train Loss: 2836.8812862796276 Test Loss: 2997.0525850701947\n",
      "Epoch 2538/4000: Train Loss: 2836.880276950636 Test Loss: 2997.050931466023\n",
      "Epoch 2539/4000: Train Loss: 2836.879268683154 Test Loss: 2997.049279394968\n",
      "Epoch 2540/4000: Train Loss: 2836.878261476064 Test Loss: 2997.0476288555287\n",
      "Epoch 2541/4000: Train Loss: 2836.877255328253 Test Loss: 2997.045979846198\n",
      "Epoch 2542/4000: Train Loss: 2836.8762502386053 Test Loss: 2997.0443323654786\n",
      "Epoch 2543/4000: Train Loss: 2836.875246206008 Test Loss: 2997.04268641187\n",
      "Epoch 2544/4000: Train Loss: 2836.8742432293507 Test Loss: 2997.041041983873\n",
      "Epoch 2545/4000: Train Loss: 2836.8732413075204 Test Loss: 2997.0393990799935\n",
      "Epoch 2546/4000: Train Loss: 2836.8722404394107 Test Loss: 2997.0377576987353\n",
      "Epoch 2547/4000: Train Loss: 2836.871240623912 Test Loss: 2997.0361178386042\n",
      "Epoch 2548/4000: Train Loss: 2836.870241859917 Test Loss: 2997.0344794981106\n",
      "Epoch 2549/4000: Train Loss: 2836.86924414632 Test Loss: 2997.0328426757615\n",
      "Epoch 2550/4000: Train Loss: 2836.868247482016 Test Loss: 2997.0312073700716\n",
      "Epoch 2551/4000: Train Loss: 2836.8672518659027 Test Loss: 2997.0295735795476\n",
      "Epoch 2552/4000: Train Loss: 2836.8662572968765 Test Loss: 2997.02794130271\n",
      "Epoch 2553/4000: Train Loss: 2836.865263773837 Test Loss: 2997.026310538072\n",
      "Epoch 2554/4000: Train Loss: 2836.8642712956835 Test Loss: 2997.024681284149\n",
      "Epoch 2555/4000: Train Loss: 2836.863279861317 Test Loss: 2997.023053539464\n",
      "Epoch 2556/4000: Train Loss: 2836.862289469641 Test Loss: 2997.0214273025317\n",
      "Epoch 2557/4000: Train Loss: 2836.8613001195567 Test Loss: 2997.0198025718764\n",
      "Epoch 2558/4000: Train Loss: 2836.860311809971 Test Loss: 2997.018179346023\n",
      "Epoch 2559/4000: Train Loss: 2836.859324539788 Test Loss: 2997.016557623494\n",
      "Epoch 2560/4000: Train Loss: 2836.858338307916 Test Loss: 2997.0149374028165\n",
      "Epoch 2561/4000: Train Loss: 2836.857353113261 Test Loss: 2997.0133186825174\n",
      "Epoch 2562/4000: Train Loss: 2836.8563689547336 Test Loss: 2997.0117014611264\n",
      "Epoch 2563/4000: Train Loss: 2836.8553858312443 Test Loss: 2997.010085737175\n",
      "Epoch 2564/4000: Train Loss: 2836.8544037417028 Test Loss: 2997.0084715091925\n",
      "Epoch 2565/4000: Train Loss: 2836.8534226850243 Test Loss: 2997.0068587757164\n",
      "Epoch 2566/4000: Train Loss: 2836.8524426601207 Test Loss: 2997.0052475352786\n",
      "Epoch 2567/4000: Train Loss: 2836.851463665907 Test Loss: 2997.003637786417\n",
      "Epoch 2568/4000: Train Loss: 2836.8504857013004 Test Loss: 2997.00202952767\n",
      "Epoch 2569/4000: Train Loss: 2836.8495087652163 Test Loss: 2997.0004227575764\n",
      "Epoch 2570/4000: Train Loss: 2836.848532856575 Test Loss: 2996.998817474678\n",
      "Epoch 2571/4000: Train Loss: 2836.847557974294 Test Loss: 2996.997213677516\n",
      "Epoch 2572/4000: Train Loss: 2836.846584117295 Test Loss: 2996.9956113646376\n",
      "Epoch 2573/4000: Train Loss: 2836.8456112844997 Test Loss: 2996.994010534585\n",
      "Epoch 2574/4000: Train Loss: 2836.8446394748307 Test Loss: 2996.992411185904\n",
      "Epoch 2575/4000: Train Loss: 2836.843668687212 Test Loss: 2996.990813317147\n",
      "Epoch 2576/4000: Train Loss: 2836.8426989205686 Test Loss: 2996.9892169268624\n",
      "Epoch 2577/4000: Train Loss: 2836.8417301738273 Test Loss: 2996.9876220136007\n",
      "Epoch 2578/4000: Train Loss: 2836.840762445915 Test Loss: 2996.986028575914\n",
      "Epoch 2579/4000: Train Loss: 2836.8397957357597 Test Loss: 2996.9844366123593\n",
      "Epoch 2580/4000: Train Loss: 2836.838830042291 Test Loss: 2996.9828461214897\n",
      "Epoch 2581/4000: Train Loss: 2836.837865364442 Test Loss: 2996.9812571018642\n",
      "Epoch 2582/4000: Train Loss: 2836.836901701142 Test Loss: 2996.979669552041\n",
      "Epoch 2583/4000: Train Loss: 2836.8359390513247 Test Loss: 2996.9780834705803\n",
      "Epoch 2584/4000: Train Loss: 2836.834977413924 Test Loss: 2996.976498856041\n",
      "Epoch 2585/4000: Train Loss: 2836.834016787876 Test Loss: 2996.974915706992\n",
      "Epoch 2586/4000: Train Loss: 2836.8330571721162 Test Loss: 2996.973334021992\n",
      "Epoch 2587/4000: Train Loss: 2836.8320985655823 Test Loss: 2996.9717537996103\n",
      "Epoch 2588/4000: Train Loss: 2836.831140967214 Test Loss: 2996.9701750384133\n",
      "Epoch 2589/4000: Train Loss: 2836.830184375949 Test Loss: 2996.968597736971\n",
      "Epoch 2590/4000: Train Loss: 2836.8292287907293 Test Loss: 2996.967021893851\n",
      "Epoch 2591/4000: Train Loss: 2836.828274210497 Test Loss: 2996.9654475076277\n",
      "Epoch 2592/4000: Train Loss: 2836.8273206341946 Test Loss: 2996.963874576874\n",
      "Epoch 2593/4000: Train Loss: 2836.826368060767 Test Loss: 2996.962303100161\n",
      "Epoch 2594/4000: Train Loss: 2836.8254164891587 Test Loss: 2996.960733076071\n",
      "Epoch 2595/4000: Train Loss: 2836.824465918316 Test Loss: 2996.9591645031774\n",
      "Epoch 2596/4000: Train Loss: 2836.8235163471873 Test Loss: 2996.9575973800584\n",
      "Epoch 2597/4000: Train Loss: 2836.822567774721 Test Loss: 2996.956031705298\n",
      "Epoch 2598/4000: Train Loss: 2836.8216201998653 Test Loss: 2996.9544674774756\n",
      "Epoch 2599/4000: Train Loss: 2836.8206736215734 Test Loss: 2996.952904695174\n",
      "Epoch 2600/4000: Train Loss: 2836.819728038796 Test Loss: 2996.95134335698\n",
      "Epoch 2601/4000: Train Loss: 2836.8187834504847 Test Loss: 2996.9497834614776\n",
      "Epoch 2602/4000: Train Loss: 2836.817839855597 Test Loss: 2996.948225007256\n",
      "Epoch 2603/4000: Train Loss: 2836.816897253085 Test Loss: 2996.9466679929046\n",
      "Epoch 2604/4000: Train Loss: 2836.815955641907 Test Loss: 2996.9451124170128\n",
      "Epoch 2605/4000: Train Loss: 2836.8150150210186 Test Loss: 2996.943558278172\n",
      "Epoch 2606/4000: Train Loss: 2836.8140753893795 Test Loss: 2996.942005574975\n",
      "Epoch 2607/4000: Train Loss: 2836.8131367459496 Test Loss: 2996.9404543060195\n",
      "Epoch 2608/4000: Train Loss: 2836.8121990896893 Test Loss: 2996.938904469899\n",
      "Epoch 2609/4000: Train Loss: 2836.81126241956 Test Loss: 2996.937356065212\n",
      "Epoch 2610/4000: Train Loss: 2836.8103267345246 Test Loss: 2996.9358090905585\n",
      "Epoch 2611/4000: Train Loss: 2836.8093920335477 Test Loss: 2996.9342635445364\n",
      "Epoch 2612/4000: Train Loss: 2836.808458315594 Test Loss: 2996.932719425748\n",
      "Epoch 2613/4000: Train Loss: 2836.80752557963 Test Loss: 2996.9311767328\n",
      "Epoch 2614/4000: Train Loss: 2836.806593824622 Test Loss: 2996.929635464293\n",
      "Epoch 2615/4000: Train Loss: 2836.805663049539 Test Loss: 2996.9280956188354\n",
      "Epoch 2616/4000: Train Loss: 2836.80473325335 Test Loss: 2996.926557195032\n",
      "Epoch 2617/4000: Train Loss: 2836.803804435026 Test Loss: 2996.925020191495\n",
      "Epoch 2618/4000: Train Loss: 2836.802876593539 Test Loss: 2996.9234846068334\n",
      "Epoch 2619/4000: Train Loss: 2836.8019497278606 Test Loss: 2996.9219504396588\n",
      "Epoch 2620/4000: Train Loss: 2836.801023836965 Test Loss: 2996.9204176885837\n",
      "Epoch 2621/4000: Train Loss: 2836.800098919827 Test Loss: 2996.9188863522245\n",
      "Epoch 2622/4000: Train Loss: 2836.799174975423 Test Loss: 2996.917356429194\n",
      "Epoch 2623/4000: Train Loss: 2836.798252002729 Test Loss: 2996.915827918113\n",
      "Epoch 2624/4000: Train Loss: 2836.797330000724 Test Loss: 2996.914300817597\n",
      "Epoch 2625/4000: Train Loss: 2836.796408968387 Test Loss: 2996.9127751262686\n",
      "Epoch 2626/4000: Train Loss: 2836.795488904697 Test Loss: 2996.9112508427493\n",
      "Epoch 2627/4000: Train Loss: 2836.7945698086364 Test Loss: 2996.90972796566\n",
      "Epoch 2628/4000: Train Loss: 2836.7936516791874 Test Loss: 2996.908206493626\n",
      "Epoch 2629/4000: Train Loss: 2836.7927345153334 Test Loss: 2996.9066864252736\n",
      "Epoch 2630/4000: Train Loss: 2836.791818316059 Test Loss: 2996.905167759229\n",
      "Epoch 2631/4000: Train Loss: 2836.7909030803494 Test Loss: 2996.9036504941214\n",
      "Epoch 2632/4000: Train Loss: 2836.7899888071915 Test Loss: 2996.902134628581\n",
      "Epoch 2633/4000: Train Loss: 2836.7890754955724 Test Loss: 2996.900620161237\n",
      "Epoch 2634/4000: Train Loss: 2836.788163144482 Test Loss: 2996.8991070907236\n",
      "Epoch 2635/4000: Train Loss: 2836.787251752909 Test Loss: 2996.8975954156763\n",
      "Epoch 2636/4000: Train Loss: 2836.786341319845 Test Loss: 2996.8960851347274\n",
      "Epoch 2637/4000: Train Loss: 2836.7854318442814 Test Loss: 2996.8945762465146\n",
      "Epoch 2638/4000: Train Loss: 2836.784523325212 Test Loss: 2996.8930687496777\n",
      "Epoch 2639/4000: Train Loss: 2836.78361576163 Test Loss: 2996.891562642855\n",
      "Epoch 2640/4000: Train Loss: 2836.782709152531 Test Loss: 2996.8900579246874\n",
      "Epoch 2641/4000: Train Loss: 2836.781803496911 Test Loss: 2996.8885545938174\n",
      "Epoch 2642/4000: Train Loss: 2836.780898793767 Test Loss: 2996.8870526488895\n",
      "Epoch 2643/4000: Train Loss: 2836.779995042097 Test Loss: 2996.8855520885463\n",
      "Epoch 2644/4000: Train Loss: 2836.779092240902 Test Loss: 2996.8840529114373\n",
      "Epoch 2645/4000: Train Loss: 2836.7781903891805 Test Loss: 2996.882555116207\n",
      "Epoch 2646/4000: Train Loss: 2836.777289485935 Test Loss: 2996.8810587015064\n",
      "Epoch 2647/4000: Train Loss: 2836.776389530167 Test Loss: 2996.8795636659875\n",
      "Epoch 2648/4000: Train Loss: 2836.7754905208817 Test Loss: 2996.8780700082993\n",
      "Epoch 2649/4000: Train Loss: 2836.7745924570822 Test Loss: 2996.8765777270974\n",
      "Epoch 2650/4000: Train Loss: 2836.773695337775 Test Loss: 2996.8750868210345\n",
      "Epoch 2651/4000: Train Loss: 2836.772799161966 Test Loss: 2996.8735972887675\n",
      "Epoch 2652/4000: Train Loss: 2836.7719039286635 Test Loss: 2996.872109128954\n",
      "Epoch 2653/4000: Train Loss: 2836.771009636876 Test Loss: 2996.8706223402532\n",
      "Epoch 2654/4000: Train Loss: 2836.7701162856138 Test Loss: 2996.869136921322\n",
      "Epoch 2655/4000: Train Loss: 2836.769223873887 Test Loss: 2996.8676528708256\n",
      "Epoch 2656/4000: Train Loss: 2836.768332400708 Test Loss: 2996.8661701874253\n",
      "Epoch 2657/4000: Train Loss: 2836.767441865089 Test Loss: 2996.8646888697845\n",
      "Epoch 2658/4000: Train Loss: 2836.7665522660454 Test Loss: 2996.863208916569\n",
      "Epoch 2659/4000: Train Loss: 2836.7656636025913 Test Loss: 2996.861730326447\n",
      "Epoch 2660/4000: Train Loss: 2836.7647758737426 Test Loss: 2996.8602530980847\n",
      "Epoch 2661/4000: Train Loss: 2836.7638890785165 Test Loss: 2996.8587772301526\n",
      "Epoch 2662/4000: Train Loss: 2836.763003215932 Test Loss: 2996.8573027213215\n",
      "Epoch 2663/4000: Train Loss: 2836.762118285006 Test Loss: 2996.8558295702637\n",
      "Epoch 2664/4000: Train Loss: 2836.7612342847606 Test Loss: 2996.8543577756523\n",
      "Epoch 2665/4000: Train Loss: 2836.760351214217 Test Loss: 2996.8528873361624\n",
      "Epoch 2666/4000: Train Loss: 2836.7594690723963 Test Loss: 2996.8514182504705\n",
      "Epoch 2667/4000: Train Loss: 2836.758587858323 Test Loss: 2996.8499505172526\n",
      "Epoch 2668/4000: Train Loss: 2836.757707571021 Test Loss: 2996.848484135191\n",
      "Epoch 2669/4000: Train Loss: 2836.7568282095153 Test Loss: 2996.847019102963\n",
      "Epoch 2670/4000: Train Loss: 2836.7559497728316 Test Loss: 2996.8455554192496\n",
      "Epoch 2671/4000: Train Loss: 2836.755072259999 Test Loss: 2996.844093082736\n",
      "Epoch 2672/4000: Train Loss: 2836.7541956700443 Test Loss: 2996.8426320921067\n",
      "Epoch 2673/4000: Train Loss: 2836.753320001998 Test Loss: 2996.841172446044\n",
      "Epoch 2674/4000: Train Loss: 2836.75244525489 Test Loss: 2996.839714143238\n",
      "Epoch 2675/4000: Train Loss: 2836.7515714277506 Test Loss: 2996.838257182375\n",
      "Epoch 2676/4000: Train Loss: 2836.7506985196155 Test Loss: 2996.836801562147\n",
      "Epoch 2677/4000: Train Loss: 2836.749826529515 Test Loss: 2996.8353472812414\n",
      "Epoch 2678/4000: Train Loss: 2836.748955456485 Test Loss: 2996.833894338352\n",
      "Epoch 2679/4000: Train Loss: 2836.748085299561 Test Loss: 2996.8324427321727\n",
      "Epoch 2680/4000: Train Loss: 2836.7472160577786 Test Loss: 2996.830992461399\n",
      "Epoch 2681/4000: Train Loss: 2836.7463477301767 Test Loss: 2996.8295435247255\n",
      "Epoch 2682/4000: Train Loss: 2836.7454803157934 Test Loss: 2996.828095920851\n",
      "Epoch 2683/4000: Train Loss: 2836.7446138136675 Test Loss: 2996.826649648473\n",
      "Epoch 2684/4000: Train Loss: 2836.7437482228406 Test Loss: 2996.825204706293\n",
      "Epoch 2685/4000: Train Loss: 2836.742883542354 Test Loss: 2996.82376109301\n",
      "Epoch 2686/4000: Train Loss: 2836.74201977125 Test Loss: 2996.8223188073302\n",
      "Epoch 2687/4000: Train Loss: 2836.7411569085725 Test Loss: 2996.8208778479548\n",
      "Epoch 2688/4000: Train Loss: 2836.7402949533657 Test Loss: 2996.8194382135916\n",
      "Epoch 2689/4000: Train Loss: 2836.739433904676 Test Loss: 2996.8179999029444\n",
      "Epoch 2690/4000: Train Loss: 2836.738573761549 Test Loss: 2996.8165629147215\n",
      "Epoch 2691/4000: Train Loss: 2836.7377145230325 Test Loss: 2996.8151272476357\n",
      "Epoch 2692/4000: Train Loss: 2836.736856188176 Test Loss: 2996.813692900394\n",
      "Epoch 2693/4000: Train Loss: 2836.735998756029 Test Loss: 2996.8122598717114\n",
      "Epoch 2694/4000: Train Loss: 2836.735142225641 Test Loss: 2996.8108281602968\n",
      "Epoch 2695/4000: Train Loss: 2836.7342865960645 Test Loss: 2996.809397764867\n",
      "Epoch 2696/4000: Train Loss: 2836.733431866352 Test Loss: 2996.807968684139\n",
      "Epoch 2697/4000: Train Loss: 2836.7325780355573 Test Loss: 2996.806540916828\n",
      "Epoch 2698/4000: Train Loss: 2836.7317251027343 Test Loss: 2996.8051144616543\n",
      "Epoch 2699/4000: Train Loss: 2836.7308730669397 Test Loss: 2996.8036893173357\n",
      "Epoch 2700/4000: Train Loss: 2836.7300219272292 Test Loss: 2996.8022654825927\n",
      "Epoch 2701/4000: Train Loss: 2836.7291716826603 Test Loss: 2996.8008429561496\n",
      "Epoch 2702/4000: Train Loss: 2836.7283223322925 Test Loss: 2996.79942173673\n",
      "Epoch 2703/4000: Train Loss: 2836.727473875184 Test Loss: 2996.798001823056\n",
      "Epoch 2704/4000: Train Loss: 2836.726626310397 Test Loss: 2996.7965832138575\n",
      "Epoch 2705/4000: Train Loss: 2836.725779636992 Test Loss: 2996.795165907859\n",
      "Epoch 2706/4000: Train Loss: 2836.724933854032 Test Loss: 2996.7937499037907\n",
      "Epoch 2707/4000: Train Loss: 2836.7240889605796 Test Loss: 2996.7923352003813\n",
      "Epoch 2708/4000: Train Loss: 2836.7232449557005 Test Loss: 2996.7909217963634\n",
      "Epoch 2709/4000: Train Loss: 2836.7224018384595 Test Loss: 2996.789509690468\n",
      "Epoch 2710/4000: Train Loss: 2836.7215596079236 Test Loss: 2996.788098881431\n",
      "Epoch 2711/4000: Train Loss: 2836.7207182631596 Test Loss: 2996.7866893679866\n",
      "Epoch 2712/4000: Train Loss: 2836.719877803236 Test Loss: 2996.78528114887\n",
      "Epoch 2713/4000: Train Loss: 2836.719038227223 Test Loss: 2996.7838742228214\n",
      "Epoch 2714/4000: Train Loss: 2836.7181995341903 Test Loss: 2996.782468588578\n",
      "Epoch 2715/4000: Train Loss: 2836.717361723209 Test Loss: 2996.7810642448803\n",
      "Epoch 2716/4000: Train Loss: 2836.7165247933517 Test Loss: 2996.77966119047\n",
      "Epoch 2717/4000: Train Loss: 2836.7156887436927 Test Loss: 2996.7782594240894\n",
      "Epoch 2718/4000: Train Loss: 2836.714853573305 Test Loss: 2996.7768589444813\n",
      "Epoch 2719/4000: Train Loss: 2836.7140192812644 Test Loss: 2996.7754597503945\n",
      "Epoch 2720/4000: Train Loss: 2836.713185866647 Test Loss: 2996.774061840573\n",
      "Epoch 2721/4000: Train Loss: 2836.7123533285308 Test Loss: 2996.7726652137653\n",
      "Epoch 2722/4000: Train Loss: 2836.7115216659927 Test Loss: 2996.771269868721\n",
      "Epoch 2723/4000: Train Loss: 2836.7106908781125 Test Loss: 2996.7698758041893\n",
      "Epoch 2724/4000: Train Loss: 2836.70986096397 Test Loss: 2996.768483018923\n",
      "Epoch 2725/4000: Train Loss: 2836.7090319226472 Test Loss: 2996.767091511674\n",
      "Epoch 2726/4000: Train Loss: 2836.708203753225 Test Loss: 2996.765701281198\n",
      "Epoch 2727/4000: Train Loss: 2836.707376454787 Test Loss: 2996.7643123262474\n",
      "Epoch 2728/4000: Train Loss: 2836.706550026418 Test Loss: 2996.762924645582\n",
      "Epoch 2729/4000: Train Loss: 2836.7057244672014 Test Loss: 2996.7615382379586\n",
      "Epoch 2730/4000: Train Loss: 2836.7048997762236 Test Loss: 2996.760153102136\n",
      "Epoch 2731/4000: Train Loss: 2836.704075952572 Test Loss: 2996.758769236875\n",
      "Epoch 2732/4000: Train Loss: 2836.703252995334 Test Loss: 2996.7573866409366\n",
      "Epoch 2733/4000: Train Loss: 2836.7024309035987 Test Loss: 2996.7560053130856\n",
      "Epoch 2734/4000: Train Loss: 2836.7016096764555 Test Loss: 2996.754625252085\n",
      "Epoch 2735/4000: Train Loss: 2836.7007893129958 Test Loss: 2996.7532464566993\n",
      "Epoch 2736/4000: Train Loss: 2836.6999698123095 Test Loss: 2996.751868925697\n",
      "Epoch 2737/4000: Train Loss: 2836.6991511734914 Test Loss: 2996.7504926578436\n",
      "Epoch 2738/4000: Train Loss: 2836.6983333956346 Test Loss: 2996.749117651912\n",
      "Epoch 2739/4000: Train Loss: 2836.6975164778323 Test Loss: 2996.747743906669\n",
      "Epoch 2740/4000: Train Loss: 2836.696700419181 Test Loss: 2996.7463714208875\n",
      "Epoch 2741/4000: Train Loss: 2836.6958852187768 Test Loss: 2996.7450001933403\n",
      "Epoch 2742/4000: Train Loss: 2836.6950708757176 Test Loss: 2996.743630222802\n",
      "Epoch 2743/4000: Train Loss: 2836.6942573891015 Test Loss: 2996.7422615080486\n",
      "Epoch 2744/4000: Train Loss: 2836.693444758027 Test Loss: 2996.740894047856\n",
      "Epoch 2745/4000: Train Loss: 2836.692632981596 Test Loss: 2996.7395278410017\n",
      "Epoch 2746/4000: Train Loss: 2836.691822058908 Test Loss: 2996.7381628862636\n",
      "Epoch 2747/4000: Train Loss: 2836.6910119890663 Test Loss: 2996.736799182425\n",
      "Epoch 2748/4000: Train Loss: 2836.6902027711726 Test Loss: 2996.7354367282655\n",
      "Epoch 2749/4000: Train Loss: 2836.689394404333 Test Loss: 2996.734075522568\n",
      "Epoch 2750/4000: Train Loss: 2836.68858688765 Test Loss: 2996.732715564118\n",
      "Epoch 2751/4000: Train Loss: 2836.6877802202316 Test Loss: 2996.731356851698\n",
      "Epoch 2752/4000: Train Loss: 2836.686974401183 Test Loss: 2996.729999384095\n",
      "Epoch 2753/4000: Train Loss: 2836.686169429613 Test Loss: 2996.7286431600996\n",
      "Epoch 2754/4000: Train Loss: 2836.685365304631 Test Loss: 2996.7272881784975\n",
      "Epoch 2755/4000: Train Loss: 2836.6845620253443 Test Loss: 2996.72593443808\n",
      "Epoch 2756/4000: Train Loss: 2836.6837595908655 Test Loss: 2996.72458193764\n",
      "Epoch 2757/4000: Train Loss: 2836.682958000305 Test Loss: 2996.7232306759665\n",
      "Epoch 2758/4000: Train Loss: 2836.6821572527765 Test Loss: 2996.7218806518586\n",
      "Epoch 2759/4000: Train Loss: 2836.681357347392 Test Loss: 2996.720531864105\n",
      "Epoch 2760/4000: Train Loss: 2836.6805582832667 Test Loss: 2996.719184311506\n",
      "Epoch 2761/4000: Train Loss: 2836.6797600595155 Test Loss: 2996.717837992858\n",
      "Epoch 2762/4000: Train Loss: 2836.6789626752548 Test Loss: 2996.7164929069604\n",
      "Epoch 2763/4000: Train Loss: 2836.678166129601 Test Loss: 2996.715149052612\n",
      "Epoch 2764/4000: Train Loss: 2836.677370421673 Test Loss: 2996.7138064286137\n",
      "Epoch 2765/4000: Train Loss: 2836.67657555059 Test Loss: 2996.712465033769\n",
      "Epoch 2766/4000: Train Loss: 2836.6757815154706 Test Loss: 2996.71112486688\n",
      "Epoch 2767/4000: Train Loss: 2836.6749883154366 Test Loss: 2996.709785926753\n",
      "Epoch 2768/4000: Train Loss: 2836.6741959496094 Test Loss: 2996.708448212193\n",
      "Epoch 2769/4000: Train Loss: 2836.6734044171117 Test Loss: 2996.7071117220057\n",
      "Epoch 2770/4000: Train Loss: 2836.672613717068 Test Loss: 2996.7057764550036\n",
      "Epoch 2771/4000: Train Loss: 2836.6718238486014 Test Loss: 2996.704442409992\n",
      "Epoch 2772/4000: Train Loss: 2836.671034810838 Test Loss: 2996.7031095857824\n",
      "Epoch 2773/4000: Train Loss: 2836.670246602904 Test Loss: 2996.701777981189\n",
      "Epoch 2774/4000: Train Loss: 2836.6694592239273 Test Loss: 2996.7004475950216\n",
      "Epoch 2775/4000: Train Loss: 2836.6686726730345 Test Loss: 2996.6991184260983\n",
      "Epoch 2776/4000: Train Loss: 2836.667886949356 Test Loss: 2996.6977904732325\n",
      "Epoch 2777/4000: Train Loss: 2836.667102052022 Test Loss: 2996.6964637352394\n",
      "Epoch 2778/4000: Train Loss: 2836.666317980163 Test Loss: 2996.6951382109405\n",
      "Epoch 2779/4000: Train Loss: 2836.6655347329106 Test Loss: 2996.6938138991522\n",
      "Epoch 2780/4000: Train Loss: 2836.6647523093975 Test Loss: 2996.692490798695\n",
      "Epoch 2781/4000: Train Loss: 2836.663970708758 Test Loss: 2996.6911689083913\n",
      "Epoch 2782/4000: Train Loss: 2836.6631899301265 Test Loss: 2996.6898482270644\n",
      "Epoch 2783/4000: Train Loss: 2836.662409972638 Test Loss: 2996.6885287535356\n",
      "Epoch 2784/4000: Train Loss: 2836.6616308354296 Test Loss: 2996.687210486633\n",
      "Epoch 2785/4000: Train Loss: 2836.6608525176384 Test Loss: 2996.6858934251813\n",
      "Epoch 2786/4000: Train Loss: 2836.660075018402 Test Loss: 2996.684577568007\n",
      "Epoch 2787/4000: Train Loss: 2836.6592983368605 Test Loss: 2996.68326291394\n",
      "Epoch 2788/4000: Train Loss: 2836.6585224721525 Test Loss: 2996.6819494618107\n",
      "Epoch 2789/4000: Train Loss: 2836.6577474234214 Test Loss: 2996.680637210447\n",
      "Epoch 2790/4000: Train Loss: 2836.6569731898066 Test Loss: 2996.6793261586854\n",
      "Epoch 2791/4000: Train Loss: 2836.6561997704516 Test Loss: 2996.678016305356\n",
      "Epoch 2792/4000: Train Loss: 2836.655427164501 Test Loss: 2996.6767076492956\n",
      "Epoch 2793/4000: Train Loss: 2836.6546553710978 Test Loss: 2996.6754001893373\n",
      "Epoch 2794/4000: Train Loss: 2836.6538843893886 Test Loss: 2996.674093924319\n",
      "Epoch 2795/4000: Train Loss: 2836.653114218519 Test Loss: 2996.67278885308\n",
      "Epoch 2796/4000: Train Loss: 2836.652344857637 Test Loss: 2996.671484974458\n",
      "Epoch 2797/4000: Train Loss: 2836.6515763058906 Test Loss: 2996.6701822872938\n",
      "Epoch 2798/4000: Train Loss: 2836.6508085624278 Test Loss: 2996.668880790429\n",
      "Epoch 2799/4000: Train Loss: 2836.6500416263993 Test Loss: 2996.6675804827064\n",
      "Epoch 2800/4000: Train Loss: 2836.6492754969563 Test Loss: 2996.6662813629705\n",
      "Epoch 2801/4000: Train Loss: 2836.64851017325 Test Loss: 2996.6649834300656\n",
      "Epoch 2802/4000: Train Loss: 2836.6477456544335 Test Loss: 2996.663686682836\n",
      "Epoch 2803/4000: Train Loss: 2836.64698193966 Test Loss: 2996.662391120133\n",
      "Epoch 2804/4000: Train Loss: 2836.646219028083 Test Loss: 2996.661096740801\n",
      "Epoch 2805/4000: Train Loss: 2836.645456918859 Test Loss: 2996.659803543693\n",
      "Epoch 2806/4000: Train Loss: 2836.6446956111436 Test Loss: 2996.6585115276594\n",
      "Epoch 2807/4000: Train Loss: 2836.643935104095 Test Loss: 2996.6572206915507\n",
      "Epoch 2808/4000: Train Loss: 2836.643175396869 Test Loss: 2996.655931034221\n",
      "Epoch 2809/4000: Train Loss: 2836.642416488626 Test Loss: 2996.654642554524\n",
      "Epoch 2810/4000: Train Loss: 2836.641658378526 Test Loss: 2996.653355251315\n",
      "Epoch 2811/4000: Train Loss: 2836.6409010657285 Test Loss: 2996.6520691234523\n",
      "Epoch 2812/4000: Train Loss: 2836.640144549395 Test Loss: 2996.6507841697935\n",
      "Epoch 2813/4000: Train Loss: 2836.639388828688 Test Loss: 2996.6495003891955\n",
      "Epoch 2814/4000: Train Loss: 2836.6386339027717 Test Loss: 2996.6482177805187\n",
      "Epoch 2815/4000: Train Loss: 2836.637879770809 Test Loss: 2996.646936342627\n",
      "Epoch 2816/4000: Train Loss: 2836.637126431966 Test Loss: 2996.64565607438\n",
      "Epoch 2817/4000: Train Loss: 2836.636373885407 Test Loss: 2996.644376974643\n",
      "Epoch 2818/4000: Train Loss: 2836.6356221303004 Test Loss: 2996.6430990422787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2819/4000: Train Loss: 2836.6348711658134 Test Loss: 2996.6418222761554\n",
      "Epoch 2820/4000: Train Loss: 2836.6341209911143 Test Loss: 2996.640546675138\n",
      "Epoch 2821/4000: Train Loss: 2836.633371605372 Test Loss: 2996.6392722380956\n",
      "Epoch 2822/4000: Train Loss: 2836.6326230077575 Test Loss: 2996.6379989638967\n",
      "Epoch 2823/4000: Train Loss: 2836.631875197442 Test Loss: 2996.6367268514136\n",
      "Epoch 2824/4000: Train Loss: 2836.6311281735957 Test Loss: 2996.6354558995154\n",
      "Epoch 2825/4000: Train Loss: 2836.630381935394 Test Loss: 2996.634186107076\n",
      "Epoch 2826/4000: Train Loss: 2836.629636482009 Test Loss: 2996.63291747297\n",
      "Epoch 2827/4000: Train Loss: 2836.628891812617 Test Loss: 2996.6316499960712\n",
      "Epoch 2828/4000: Train Loss: 2836.628147926392 Test Loss: 2996.6303836752545\n",
      "Epoch 2829/4000: Train Loss: 2836.6274048225096 Test Loss: 2996.6291185093996\n",
      "Epoch 2830/4000: Train Loss: 2836.626662500149 Test Loss: 2996.6278544973834\n",
      "Epoch 2831/4000: Train Loss: 2836.6259209584873 Test Loss: 2996.6265916380867\n",
      "Epoch 2832/4000: Train Loss: 2836.6251801967037 Test Loss: 2996.625329930388\n",
      "Epoch 2833/4000: Train Loss: 2836.624440213977 Test Loss: 2996.6240693731716\n",
      "Epoch 2834/4000: Train Loss: 2836.623701009489 Test Loss: 2996.6228099653176\n",
      "Epoch 2835/4000: Train Loss: 2836.6229625824217 Test Loss: 2996.621551705711\n",
      "Epoch 2836/4000: Train Loss: 2836.6222249319558 Test Loss: 2996.620294593237\n",
      "Epoch 2837/4000: Train Loss: 2836.621488057276 Test Loss: 2996.6190386267835\n",
      "Epoch 2838/4000: Train Loss: 2836.620751957565 Test Loss: 2996.6177838052345\n",
      "Epoch 2839/4000: Train Loss: 2836.6200166320095 Test Loss: 2996.6165301274814\n",
      "Epoch 2840/4000: Train Loss: 2836.6192820797946 Test Loss: 2996.615277592414\n",
      "Epoch 2841/4000: Train Loss: 2836.618548300106 Test Loss: 2996.61402619892\n",
      "Epoch 2842/4000: Train Loss: 2836.617815292133 Test Loss: 2996.6127759458946\n",
      "Epoch 2843/4000: Train Loss: 2836.6170830550627 Test Loss: 2996.6115268322274\n",
      "Epoch 2844/4000: Train Loss: 2836.616351588084 Test Loss: 2996.6102788568146\n",
      "Epoch 2845/4000: Train Loss: 2836.615620890388 Test Loss: 2996.609032018552\n",
      "Epoch 2846/4000: Train Loss: 2836.614890961166 Test Loss: 2996.607786316333\n",
      "Epoch 2847/4000: Train Loss: 2836.614161799608 Test Loss: 2996.6065417490586\n",
      "Epoch 2848/4000: Train Loss: 2836.613433404909 Test Loss: 2996.6052983156255\n",
      "Epoch 2849/4000: Train Loss: 2836.6127057762606 Test Loss: 2996.604056014932\n",
      "Epoch 2850/4000: Train Loss: 2836.611978912858 Test Loss: 2996.6028148458813\n",
      "Epoch 2851/4000: Train Loss: 2836.611252813896 Test Loss: 2996.6015748073755\n",
      "Epoch 2852/4000: Train Loss: 2836.6105274785705 Test Loss: 2996.600335898315\n",
      "Epoch 2853/4000: Train Loss: 2836.60980290608 Test Loss: 2996.5990981176055\n",
      "Epoch 2854/4000: Train Loss: 2836.6090790956196 Test Loss: 2996.59786146415\n",
      "Epoch 2855/4000: Train Loss: 2836.6083560463903 Test Loss: 2996.5966259368583\n",
      "Epoch 2856/4000: Train Loss: 2836.60763375759 Test Loss: 2996.5953915346354\n",
      "Epoch 2857/4000: Train Loss: 2836.6069122284193 Test Loss: 2996.5941582563914\n",
      "Epoch 2858/4000: Train Loss: 2836.6061914580796 Test Loss: 2996.5929261010333\n",
      "Epoch 2859/4000: Train Loss: 2836.6054714457723 Test Loss: 2996.591695067474\n",
      "Epoch 2860/4000: Train Loss: 2836.6047521907008 Test Loss: 2996.5904651546243\n",
      "Epoch 2861/4000: Train Loss: 2836.6040336920687 Test Loss: 2996.5892363613984\n",
      "Epoch 2862/4000: Train Loss: 2836.6033159490808 Test Loss: 2996.5880086867073\n",
      "Epoch 2863/4000: Train Loss: 2836.6025989609407 Test Loss: 2996.586782129469\n",
      "Epoch 2864/4000: Train Loss: 2836.6018827268554 Test Loss: 2996.585556688599\n",
      "Epoch 2865/4000: Train Loss: 2836.6011672460327 Test Loss: 2996.584332363014\n",
      "Epoch 2866/4000: Train Loss: 2836.6004525176795 Test Loss: 2996.583109151631\n",
      "Epoch 2867/4000: Train Loss: 2836.599738541005 Test Loss: 2996.5818870533712\n",
      "Epoch 2868/4000: Train Loss: 2836.599025315218 Test Loss: 2996.580666067156\n",
      "Epoch 2869/4000: Train Loss: 2836.5983128395296 Test Loss: 2996.5794461919054\n",
      "Epoch 2870/4000: Train Loss: 2836.59760111315 Test Loss: 2996.578227426542\n",
      "Epoch 2871/4000: Train Loss: 2836.5968901352917 Test Loss: 2996.5770097699906\n",
      "Epoch 2872/4000: Train Loss: 2836.5961799051684 Test Loss: 2996.5757932211745\n",
      "Epoch 2873/4000: Train Loss: 2836.5954704219916 Test Loss: 2996.5745777790203\n",
      "Epoch 2874/4000: Train Loss: 2836.594761684977 Test Loss: 2996.573363442456\n",
      "Epoch 2875/4000: Train Loss: 2836.5940536933404 Test Loss: 2996.572150210409\n",
      "Epoch 2876/4000: Train Loss: 2836.593346446297 Test Loss: 2996.570938081807\n",
      "Epoch 2877/4000: Train Loss: 2836.592639943063 Test Loss: 2996.569727055584\n",
      "Epoch 2878/4000: Train Loss: 2836.5919341828585 Test Loss: 2996.5685171306673\n",
      "Epoch 2879/4000: Train Loss: 2836.5912291648997 Test Loss: 2996.5673083059914\n",
      "Epoch 2880/4000: Train Loss: 2836.590524888407 Test Loss: 2996.566100580488\n",
      "Epoch 2881/4000: Train Loss: 2836.5898213526007 Test Loss: 2996.564893953094\n",
      "Epoch 2882/4000: Train Loss: 2836.589118556701 Test Loss: 2996.5636884227447\n",
      "Epoch 2883/4000: Train Loss: 2836.588416499931 Test Loss: 2996.5624839883744\n",
      "Epoch 2884/4000: Train Loss: 2836.587715181513 Test Loss: 2996.5612806489235\n",
      "Epoch 2885/4000: Train Loss: 2836.5870146006696 Test Loss: 2996.5600784033313\n",
      "Epoch 2886/4000: Train Loss: 2836.5863147566256 Test Loss: 2996.5588772505325\n",
      "Epoch 2887/4000: Train Loss: 2836.585615648607 Test Loss: 2996.5576771894766\n",
      "Epoch 2888/4000: Train Loss: 2836.5849172758385 Test Loss: 2996.556478219097\n",
      "Epoch 2889/4000: Train Loss: 2836.584219637547 Test Loss: 2996.555280338343\n",
      "Epoch 2890/4000: Train Loss: 2836.583522732961 Test Loss: 2996.5540835461543\n",
      "Epoch 2891/4000: Train Loss: 2836.5828265613077 Test Loss: 2996.5528878414802\n",
      "Epoch 2892/4000: Train Loss: 2836.582131121817 Test Loss: 2996.551693223264\n",
      "Epoch 2893/4000: Train Loss: 2836.581436413718 Test Loss: 2996.550499690456\n",
      "Epoch 2894/4000: Train Loss: 2836.580742436243 Test Loss: 2996.5493072420013\n",
      "Epoch 2895/4000: Train Loss: 2836.580049188622 Test Loss: 2996.548115876851\n",
      "Epoch 2896/4000: Train Loss: 2836.5793566700886 Test Loss: 2996.5469255939543\n",
      "Epoch 2897/4000: Train Loss: 2836.5786648798758 Test Loss: 2996.5457363922646\n",
      "Epoch 2898/4000: Train Loss: 2836.577973817217 Test Loss: 2996.5445482707346\n",
      "Epoch 2899/4000: Train Loss: 2836.577283481348 Test Loss: 2996.5433612283164\n",
      "Epoch 2900/4000: Train Loss: 2836.576593871503 Test Loss: 2996.5421752639663\n",
      "Epoch 2901/4000: Train Loss: 2836.57590498692 Test Loss: 2996.54099037664\n",
      "Epoch 2902/4000: Train Loss: 2836.5752168268345 Test Loss: 2996.5398065652907\n",
      "Epoch 2903/4000: Train Loss: 2836.574529390487 Test Loss: 2996.5386238288825\n",
      "Epoch 2904/4000: Train Loss: 2836.573842677114 Test Loss: 2996.5374421663696\n",
      "Epoch 2905/4000: Train Loss: 2836.573156685957 Test Loss: 2996.5362615767135\n",
      "Epoch 2906/4000: Train Loss: 2836.572471416255 Test Loss: 2996.535082058876\n",
      "Epoch 2907/4000: Train Loss: 2836.57178686725 Test Loss: 2996.5339036118194\n",
      "Epoch 2908/4000: Train Loss: 2836.571103038183 Test Loss: 2996.532726234504\n",
      "Epoch 2909/4000: Train Loss: 2836.5704199282986 Test Loss: 2996.5315499258963\n",
      "Epoch 2910/4000: Train Loss: 2836.5697375368395 Test Loss: 2996.530374684962\n",
      "Epoch 2911/4000: Train Loss: 2836.5690558630504 Test Loss: 2996.5292005106667\n",
      "Epoch 2912/4000: Train Loss: 2836.568374906176 Test Loss: 2996.528027401977\n",
      "Epoch 2913/4000: Train Loss: 2836.567694665462 Test Loss: 2996.5268553578617\n",
      "Epoch 2914/4000: Train Loss: 2836.567015140157 Test Loss: 2996.5256843772895\n",
      "Epoch 2915/4000: Train Loss: 2836.566336329506 Test Loss: 2996.5245144592323\n",
      "Epoch 2916/4000: Train Loss: 2836.5656582327597 Test Loss: 2996.5233456026617\n",
      "Epoch 2917/4000: Train Loss: 2836.5649808491667 Test Loss: 2996.522177806548\n",
      "Epoch 2918/4000: Train Loss: 2836.5643041779763 Test Loss: 2996.521011069866\n",
      "Epoch 2919/4000: Train Loss: 2836.563628218439 Test Loss: 2996.51984539159\n",
      "Epoch 2920/4000: Train Loss: 2836.5629529698076 Test Loss: 2996.518680770696\n",
      "Epoch 2921/4000: Train Loss: 2836.5622784313346 Test Loss: 2996.517517206161\n",
      "Epoch 2922/4000: Train Loss: 2836.5616046022715 Test Loss: 2996.5163546969625\n",
      "Epoch 2923/4000: Train Loss: 2836.5609314818735 Test Loss: 2996.5151932420777\n",
      "Epoch 2924/4000: Train Loss: 2836.5602590693943 Test Loss: 2996.5140328404877\n",
      "Epoch 2925/4000: Train Loss: 2836.5595873640905 Test Loss: 2996.5128734911723\n",
      "Epoch 2926/4000: Train Loss: 2836.558916365218 Test Loss: 2996.511715193116\n",
      "Epoch 2927/4000: Train Loss: 2836.5582460720334 Test Loss: 2996.510557945297\n",
      "Epoch 2928/4000: Train Loss: 2836.5575764837945 Test Loss: 2996.5094017467027\n",
      "Epoch 2929/4000: Train Loss: 2836.556907599761 Test Loss: 2996.5082465963164\n",
      "Epoch 2930/4000: Train Loss: 2836.556239419191 Test Loss: 2996.5070924931247\n",
      "Epoch 2931/4000: Train Loss: 2836.555571941346 Test Loss: 2996.5059394361137\n",
      "Epoch 2932/4000: Train Loss: 2836.554905165485 Test Loss: 2996.5047874242714\n",
      "Epoch 2933/4000: Train Loss: 2836.5542390908713 Test Loss: 2996.5036364565867\n",
      "Epoch 2934/4000: Train Loss: 2836.553573716767 Test Loss: 2996.5024865320497\n",
      "Epoch 2935/4000: Train Loss: 2836.552909042435 Test Loss: 2996.5013376496527\n",
      "Epoch 2936/4000: Train Loss: 2836.5522450671406 Test Loss: 2996.5001898083847\n",
      "Epoch 2937/4000: Train Loss: 2836.551581790147 Test Loss: 2996.4990430072417\n",
      "Epoch 2938/4000: Train Loss: 2836.5509192107206 Test Loss: 2996.497897245216\n",
      "Epoch 2939/4000: Train Loss: 2836.550257328128 Test Loss: 2996.496752521303\n",
      "Epoch 2940/4000: Train Loss: 2836.5495961416364 Test Loss: 2996.495608834497\n",
      "Epoch 2941/4000: Train Loss: 2836.548935650512 Test Loss: 2996.4944661837985\n",
      "Epoch 2942/4000: Train Loss: 2836.548275854025 Test Loss: 2996.4933245682028\n",
      "Epoch 2943/4000: Train Loss: 2836.547616751446 Test Loss: 2996.4921839867093\n",
      "Epoch 2944/4000: Train Loss: 2836.546958342043 Test Loss: 2996.491044438318\n",
      "Epoch 2945/4000: Train Loss: 2836.546300625088 Test Loss: 2996.489905922032\n",
      "Epoch 2946/4000: Train Loss: 2836.5456435998526 Test Loss: 2996.4887684368505\n",
      "Epoch 2947/4000: Train Loss: 2836.5449872656095 Test Loss: 2996.4876319817786\n",
      "Epoch 2948/4000: Train Loss: 2836.544331621632 Test Loss: 2996.4864965558195\n",
      "Epoch 2949/4000: Train Loss: 2836.5436766671933 Test Loss: 2996.4853621579778\n",
      "Epoch 2950/4000: Train Loss: 2836.5430224015695 Test Loss: 2996.4842287872593\n",
      "Epoch 2951/4000: Train Loss: 2836.5423688240353 Test Loss: 2996.483096442672\n",
      "Epoch 2952/4000: Train Loss: 2836.5417159338676 Test Loss: 2996.481965123225\n",
      "Epoch 2953/4000: Train Loss: 2836.5410637303426 Test Loss: 2996.480834827926\n",
      "Epoch 2954/4000: Train Loss: 2836.5404122127393 Test Loss: 2996.479705555784\n",
      "Epoch 2955/4000: Train Loss: 2836.539761380335 Test Loss: 2996.4785773058115\n",
      "Epoch 2956/4000: Train Loss: 2836.539111232411 Test Loss: 2996.47745007702\n",
      "Epoch 2957/4000: Train Loss: 2836.5384617682457 Test Loss: 2996.4763238684222\n",
      "Epoch 2958/4000: Train Loss: 2836.537812987121 Test Loss: 2996.475198679034\n",
      "Epoch 2959/4000: Train Loss: 2836.5371648883174 Test Loss: 2996.4740745078684\n",
      "Epoch 2960/4000: Train Loss: 2836.536517471119 Test Loss: 2996.4729513539405\n",
      "Epoch 2961/4000: Train Loss: 2836.5358707348073 Test Loss: 2996.4718292162706\n",
      "Epoch 2962/4000: Train Loss: 2836.535224678667 Test Loss: 2996.470708093874\n",
      "Epoch 2963/4000: Train Loss: 2836.5345793019833 Test Loss: 2996.4695879857713\n",
      "Epoch 2964/4000: Train Loss: 2836.53393460404 Test Loss: 2996.468468890982\n",
      "Epoch 2965/4000: Train Loss: 2836.533290584125 Test Loss: 2996.4673508085248\n",
      "Epoch 2966/4000: Train Loss: 2836.532647241524 Test Loss: 2996.4662337374257\n",
      "Epoch 2967/4000: Train Loss: 2836.532004575525 Test Loss: 2996.465117676704\n",
      "Epoch 2968/4000: Train Loss: 2836.5313625854174 Test Loss: 2996.464002625386\n",
      "Epoch 2969/4000: Train Loss: 2836.530721270489 Test Loss: 2996.4628885824955\n",
      "Epoch 2970/4000: Train Loss: 2836.53008063003 Test Loss: 2996.461775547058\n",
      "Epoch 2971/4000: Train Loss: 2836.5294406633316 Test Loss: 2996.4606635181003\n",
      "Epoch 2972/4000: Train Loss: 2836.528801369685 Test Loss: 2996.459552494651\n",
      "Epoch 2973/4000: Train Loss: 2836.5281627483823 Test Loss: 2996.458442475738\n",
      "Epoch 2974/4000: Train Loss: 2836.5275247987165 Test Loss: 2996.457333460392\n",
      "Epoch 2975/4000: Train Loss: 2836.526887519981 Test Loss: 2996.4562254476436\n",
      "Epoch 2976/4000: Train Loss: 2836.52625091147 Test Loss: 2996.455118436524\n",
      "Epoch 2977/4000: Train Loss: 2836.5256149724787 Test Loss: 2996.454012426065\n",
      "Epoch 2978/4000: Train Loss: 2836.524979702304 Test Loss: 2996.4529074153043\n",
      "Epoch 2979/4000: Train Loss: 2836.5243451002416 Test Loss: 2996.451803403271\n",
      "Epoch 2980/4000: Train Loss: 2836.523711165589 Test Loss: 2996.4507003890035\n",
      "Epoch 2981/4000: Train Loss: 2836.5230778976434 Test Loss: 2996.449598371538\n",
      "Epoch 2982/4000: Train Loss: 2836.5224452957045 Test Loss: 2996.4484973499134\n",
      "Epoch 2983/4000: Train Loss: 2836.5218133590724 Test Loss: 2996.4473973231666\n",
      "Epoch 2984/4000: Train Loss: 2836.521182087047 Test Loss: 2996.4462982903365\n",
      "Epoch 2985/4000: Train Loss: 2836.520551478929 Test Loss: 2996.4452002504645\n",
      "Epoch 2986/4000: Train Loss: 2836.51992153402 Test Loss: 2996.4441032025934\n",
      "Epoch 2987/4000: Train Loss: 2836.5192922516226 Test Loss: 2996.443007145762\n",
      "Epoch 2988/4000: Train Loss: 2836.518663631041 Test Loss: 2996.441912079017\n",
      "Epoch 2989/4000: Train Loss: 2836.518035671578 Test Loss: 2996.4408180014\n",
      "Epoch 2990/4000: Train Loss: 2836.5174083725387 Test Loss: 2996.4397249119593\n",
      "Epoch 2991/4000: Train Loss: 2836.5167817332285 Test Loss: 2996.4386328097376\n",
      "Epoch 2992/4000: Train Loss: 2836.5161557529536 Test Loss: 2996.4375416937846\n",
      "Epoch 2993/4000: Train Loss: 2836.5155304310215 Test Loss: 2996.436451563147\n",
      "Epoch 2994/4000: Train Loss: 2836.5149057667386 Test Loss: 2996.435362416875\n",
      "Epoch 2995/4000: Train Loss: 2836.5142817594146 Test Loss: 2996.434274254018\n",
      "Epoch 2996/4000: Train Loss: 2836.5136584083575 Test Loss: 2996.4331870736282\n",
      "Epoch 2997/4000: Train Loss: 2836.5130357128774 Test Loss: 2996.432100874755\n",
      "Epoch 2998/4000: Train Loss: 2836.512413672285 Test Loss: 2996.431015656453\n",
      "Epoch 2999/4000: Train Loss: 2836.5117922858913 Test Loss: 2996.4299314177765\n",
      "Epoch 3000/4000: Train Loss: 2836.511171553009 Test Loss: 2996.428848157779\n",
      "Epoch 3001/4000: Train Loss: 2836.51055147295 Test Loss: 2996.427765875516\n",
      "Epoch 3002/4000: Train Loss: 2836.5099320450277 Test Loss: 2996.426684570046\n",
      "Epoch 3003/4000: Train Loss: 2836.509313268556 Test Loss: 2996.425604240424\n",
      "Epoch 3004/4000: Train Loss: 2836.5086951428516 Test Loss: 2996.4245248857114\n",
      "Epoch 3005/4000: Train Loss: 2836.5080776672285 Test Loss: 2996.4234465049667\n",
      "Epoch 3006/4000: Train Loss: 2836.5074608410023 Test Loss: 2996.4223690972494\n",
      "Epoch 3007/4000: Train Loss: 2836.506844663492 Test Loss: 2996.421292661621\n",
      "Epoch 3008/4000: Train Loss: 2836.506229134013 Test Loss: 2996.420217197146\n",
      "Epoch 3009/4000: Train Loss: 2836.505614251887 Test Loss: 2996.419142702885\n",
      "Epoch 3010/4000: Train Loss: 2836.5050000164306 Test Loss: 2996.418069177904\n",
      "Epoch 3011/4000: Train Loss: 2836.5043864269646 Test Loss: 2996.4169966212667\n",
      "Epoch 3012/4000: Train Loss: 2836.503773482809 Test Loss: 2996.4159250320413\n",
      "Epoch 3013/4000: Train Loss: 2836.503161183286 Test Loss: 2996.4148544092923\n",
      "Epoch 3014/4000: Train Loss: 2836.502549527717 Test Loss: 2996.41378475209\n",
      "Epoch 3015/4000: Train Loss: 2836.5019385154246 Test Loss: 2996.4127160595026\n",
      "Epoch 3016/4000: Train Loss: 2836.501328145733 Test Loss: 2996.4116483305993\n",
      "Epoch 3017/4000: Train Loss: 2836.5007184179663 Test Loss: 2996.4105815644507\n",
      "Epoch 3018/4000: Train Loss: 2836.5001093314486 Test Loss: 2996.4095157601278\n",
      "Epoch 3019/4000: Train Loss: 2836.4995008855058 Test Loss: 2996.408450916707\n",
      "Epoch 3020/4000: Train Loss: 2836.4988930794652 Test Loss: 2996.4073870332577\n",
      "Epoch 3021/4000: Train Loss: 2836.4982859126526 Test Loss: 2996.406324108857\n",
      "Epoch 3022/4000: Train Loss: 2836.4976793843966 Test Loss: 2996.40526214258\n",
      "Epoch 3023/4000: Train Loss: 2836.4970734940243 Test Loss: 2996.4042011335014\n",
      "Epoch 3024/4000: Train Loss: 2836.496468240866 Test Loss: 2996.4031410806997\n",
      "Epoch 3025/4000: Train Loss: 2836.495863624252 Test Loss: 2996.402081983253\n",
      "Epoch 3026/4000: Train Loss: 2836.495259643511 Test Loss: 2996.401023840242\n",
      "Epoch 3027/4000: Train Loss: 2836.494656297976 Test Loss: 2996.399966650745\n",
      "Epoch 3028/4000: Train Loss: 2836.494053586978 Test Loss: 2996.398910413842\n",
      "Epoch 3029/4000: Train Loss: 2836.4934515098507 Test Loss: 2996.3978551286164\n",
      "Epoch 3030/4000: Train Loss: 2836.4928500659257 Test Loss: 2996.3968007941503\n",
      "Epoch 3031/4000: Train Loss: 2836.492249254539 Test Loss: 2996.3957474095296\n",
      "Epoch 3032/4000: Train Loss: 2836.491649075024 Test Loss: 2996.394694973837\n",
      "Epoch 3033/4000: Train Loss: 2836.4910495267163 Test Loss: 2996.3936434861585\n",
      "Epoch 3034/4000: Train Loss: 2836.490450608953 Test Loss: 2996.3925929455804\n",
      "Epoch 3035/4000: Train Loss: 2836.4898523210695 Test Loss: 2996.391543351191\n",
      "Epoch 3036/4000: Train Loss: 2836.4892546624055 Test Loss: 2996.390494702076\n",
      "Epoch 3037/4000: Train Loss: 2836.488657632297 Test Loss: 2996.389446997329\n",
      "Epoch 3038/4000: Train Loss: 2836.488061230084 Test Loss: 2996.3884002360373\n",
      "Epoch 3039/4000: Train Loss: 2836.4874654551063 Test Loss: 2996.3873544172907\n",
      "Epoch 3040/4000: Train Loss: 2836.4868703067036 Test Loss: 2996.3863095401844\n",
      "Epoch 3041/4000: Train Loss: 2836.4862757842175 Test Loss: 2996.385265603811\n",
      "Epoch 3042/4000: Train Loss: 2836.4856818869903 Test Loss: 2996.384222607262\n",
      "Epoch 3043/4000: Train Loss: 2836.485088614362 Test Loss: 2996.3831805496334\n",
      "Epoch 3044/4000: Train Loss: 2836.484495965679 Test Loss: 2996.3821394300217\n",
      "Epoch 3045/4000: Train Loss: 2836.4839039402823 Test Loss: 2996.3810992475233\n",
      "Epoch 3046/4000: Train Loss: 2836.483312537518 Test Loss: 2996.380060001233\n",
      "Epoch 3047/4000: Train Loss: 2836.482721756731 Test Loss: 2996.379021690252\n",
      "Epoch 3048/4000: Train Loss: 2836.4821315972667 Test Loss: 2996.377984313677\n",
      "Epoch 3049/4000: Train Loss: 2836.4815420584723 Test Loss: 2996.3769478706135\n",
      "Epoch 3050/4000: Train Loss: 2836.480953139694 Test Loss: 2996.3759123601567\n",
      "Epoch 3051/4000: Train Loss: 2836.4803648402813 Test Loss: 2996.374877781411\n",
      "Epoch 3052/4000: Train Loss: 2836.4797771595813 Test Loss: 2996.3738441334804\n",
      "Epoch 3053/4000: Train Loss: 2836.4791900969444 Test Loss: 2996.372811415466\n",
      "Epoch 3054/4000: Train Loss: 2836.47860365172 Test Loss: 2996.3717796264746\n",
      "Epoch 3055/4000: Train Loss: 2836.4780178232586 Test Loss: 2996.3707487656097\n",
      "Epoch 3056/4000: Train Loss: 2836.477432610912 Test Loss: 2996.369718831981\n",
      "Epoch 3057/4000: Train Loss: 2836.476848014032 Test Loss: 2996.3686898246924\n",
      "Epoch 3058/4000: Train Loss: 2836.476264031972 Test Loss: 2996.3676617428555\n",
      "Epoch 3059/4000: Train Loss: 2836.4756806640835 Test Loss: 2996.3666345855763\n",
      "Epoch 3060/4000: Train Loss: 2836.475097909723 Test Loss: 2996.365608351966\n",
      "Epoch 3061/4000: Train Loss: 2836.4745157682437 Test Loss: 2996.3645830411374\n",
      "Epoch 3062/4000: Train Loss: 2836.4739342390017 Test Loss: 2996.363558652199\n",
      "Epoch 3063/4000: Train Loss: 2836.4733533213525 Test Loss: 2996.362535184266\n",
      "Epoch 3064/4000: Train Loss: 2836.4727730146537 Test Loss: 2996.361512636452\n",
      "Epoch 3065/4000: Train Loss: 2836.472193318262 Test Loss: 2996.3604910078698\n",
      "Epoch 3066/4000: Train Loss: 2836.4716142315356 Test Loss: 2996.359470297637\n",
      "Epoch 3067/4000: Train Loss: 2836.471035753834 Test Loss: 2996.358450504868\n",
      "Epoch 3068/4000: Train Loss: 2836.4704578845162 Test Loss: 2996.357431628679\n",
      "Epoch 3069/4000: Train Loss: 2836.4698806229426 Test Loss: 2996.3564136681916\n",
      "Epoch 3070/4000: Train Loss: 2836.4693039684735 Test Loss: 2996.3553966225236\n",
      "Epoch 3071/4000: Train Loss: 2836.468727920471 Test Loss: 2996.3543804907913\n",
      "Epoch 3072/4000: Train Loss: 2836.4681524782973 Test Loss: 2996.3533652721203\n",
      "Epoch 3073/4000: Train Loss: 2836.467577641314 Test Loss: 2996.352350965629\n",
      "Epoch 3074/4000: Train Loss: 2836.4670034088863 Test Loss: 2996.3513375704415\n",
      "Epoch 3075/4000: Train Loss: 2836.466429780378 Test Loss: 2996.35032508568\n",
      "Epoch 3076/4000: Train Loss: 2836.4658567551533 Test Loss: 2996.34931351047\n",
      "Epoch 3077/4000: Train Loss: 2836.465284332578 Test Loss: 2996.348302843935\n",
      "Epoch 3078/4000: Train Loss: 2836.464712512019 Test Loss: 2996.3472930852026\n",
      "Epoch 3079/4000: Train Loss: 2836.464141292842 Test Loss: 2996.3462842333975\n",
      "Epoch 3080/4000: Train Loss: 2836.463570674415 Test Loss: 2996.3452762876504\n",
      "Epoch 3081/4000: Train Loss: 2836.463000656107 Test Loss: 2996.3442692470885\n",
      "Epoch 3082/4000: Train Loss: 2836.4624312372853 Test Loss: 2996.343263110839\n",
      "Epoch 3083/4000: Train Loss: 2836.461862417321 Test Loss: 2996.3422578780373\n",
      "Epoch 3084/4000: Train Loss: 2836.461294195583 Test Loss: 2996.341253547809\n",
      "Epoch 3085/4000: Train Loss: 2836.460726571443 Test Loss: 2996.340250119291\n",
      "Epoch 3086/4000: Train Loss: 2836.460159544272 Test Loss: 2996.339247591612\n",
      "Epoch 3087/4000: Train Loss: 2836.4595931134427 Test Loss: 2996.3382459639097\n",
      "Epoch 3088/4000: Train Loss: 2836.459027278327 Test Loss: 2996.337245235317\n",
      "Epoch 3089/4000: Train Loss: 2836.4584620382993 Test Loss: 2996.3362454049675\n",
      "Epoch 3090/4000: Train Loss: 2836.4578973927337 Test Loss: 2996.335246472002\n",
      "Epoch 3091/4000: Train Loss: 2836.457333341004 Test Loss: 2996.3342484355544\n",
      "Epoch 3092/4000: Train Loss: 2836.4567698824867 Test Loss: 2996.333251294765\n",
      "Epoch 3093/4000: Train Loss: 2836.4562070165584 Test Loss: 2996.332255048772\n",
      "Epoch 3094/4000: Train Loss: 2836.4556447425944 Test Loss: 2996.3312596967135\n",
      "Epoch 3095/4000: Train Loss: 2836.455083059973 Test Loss: 2996.330265237732\n",
      "Epoch 3096/4000: Train Loss: 2836.4545219680717 Test Loss: 2996.3292716709693\n",
      "Epoch 3097/4000: Train Loss: 2836.4539614662704 Test Loss: 2996.328278995567\n",
      "Epoch 3098/4000: Train Loss: 2836.4534015539466 Test Loss: 2996.32728721067\n",
      "Epoch 3099/4000: Train Loss: 2836.4528422304825 Test Loss: 2996.326296315421\n",
      "Epoch 3100/4000: Train Loss: 2836.4522834952572 Test Loss: 2996.325306308965\n",
      "Epoch 3101/4000: Train Loss: 2836.4517253476533 Test Loss: 2996.3243171904496\n",
      "Epoch 3102/4000: Train Loss: 2836.451167787052 Test Loss: 2996.323328959019\n",
      "Epoch 3103/4000: Train Loss: 2836.4506108128357 Test Loss: 2996.3223416138235\n",
      "Epoch 3104/4000: Train Loss: 2836.4500544243883 Test Loss: 2996.3213551540084\n",
      "Epoch 3105/4000: Train Loss: 2836.4494986210934 Test Loss: 2996.3203695787274\n",
      "Epoch 3106/4000: Train Loss: 2836.448943402336 Test Loss: 2996.3193848871265\n",
      "Epoch 3107/4000: Train Loss: 2836.4483887675005 Test Loss: 2996.3184010783575\n",
      "Epoch 3108/4000: Train Loss: 2836.4478347159743 Test Loss: 2996.317418151575\n",
      "Epoch 3109/4000: Train Loss: 2836.447281247142 Test Loss: 2996.3164361059294\n",
      "Epoch 3110/4000: Train Loss: 2836.4467283603926 Test Loss: 2996.3154549405745\n",
      "Epoch 3111/4000: Train Loss: 2836.4461760551126 Test Loss: 2996.314474654665\n",
      "Epoch 3112/4000: Train Loss: 2836.445624330691 Test Loss: 2996.3134952473556\n",
      "Epoch 3113/4000: Train Loss: 2836.4450731865168 Test Loss: 2996.312516717804\n",
      "Epoch 3114/4000: Train Loss: 2836.4445226219805 Test Loss: 2996.3115390651656\n",
      "Epoch 3115/4000: Train Loss: 2836.4439726364712 Test Loss: 2996.3105622885987\n",
      "Epoch 3116/4000: Train Loss: 2836.4434232293806 Test Loss: 2996.309586387263\n",
      "Epoch 3117/4000: Train Loss: 2836.442874400101 Test Loss: 2996.3086113603163\n",
      "Epoch 3118/4000: Train Loss: 2836.4423261480238 Test Loss: 2996.3076372069195\n",
      "Epoch 3119/4000: Train Loss: 2836.4417784725415 Test Loss: 2996.3066639262343\n",
      "Epoch 3120/4000: Train Loss: 2836.4412313730495 Test Loss: 2996.3056915174216\n",
      "Epoch 3121/4000: Train Loss: 2836.4406848489407 Test Loss: 2996.304719979645\n",
      "Epoch 3122/4000: Train Loss: 2836.44013889961 Test Loss: 2996.3037493120687\n",
      "Epoch 3123/4000: Train Loss: 2836.4395935244534 Test Loss: 2996.3027795138573\n",
      "Epoch 3124/4000: Train Loss: 2836.439048722866 Test Loss: 2996.3018105841743\n",
      "Epoch 3125/4000: Train Loss: 2836.4385044942464 Test Loss: 2996.300842522188\n",
      "Epoch 3126/4000: Train Loss: 2836.43796083799 Test Loss: 2996.299875327065\n",
      "Epoch 3127/4000: Train Loss: 2836.4374177534964 Test Loss: 2996.298908997972\n",
      "Epoch 3128/4000: Train Loss: 2836.4368752401638 Test Loss: 2996.29794353408\n",
      "Epoch 3129/4000: Train Loss: 2836.436333297391 Test Loss: 2996.296978934555\n",
      "Epoch 3130/4000: Train Loss: 2836.435791924578 Test Loss: 2996.2960151985694\n",
      "Epoch 3131/4000: Train Loss: 2836.4352511211273 Test Loss: 2996.295052325297\n",
      "Epoch 3132/4000: Train Loss: 2836.434710886437 Test Loss: 2996.294090313905\n",
      "Epoch 3133/4000: Train Loss: 2836.434171219911 Test Loss: 2996.293129163569\n",
      "Epoch 3134/4000: Train Loss: 2836.4336321209507 Test Loss: 2996.2921688734627\n",
      "Epoch 3135/4000: Train Loss: 2836.4330935889607 Test Loss: 2996.29120944276\n",
      "Epoch 3136/4000: Train Loss: 2836.4325556233434 Test Loss: 2996.290250870638\n",
      "Epoch 3137/4000: Train Loss: 2836.4320182235033 Test Loss: 2996.2892931562687\n",
      "Epoch 3138/4000: Train Loss: 2836.4314813888454 Test Loss: 2996.288336298833\n",
      "Epoch 3139/4000: Train Loss: 2836.4309451187755 Test Loss: 2996.287380297507\n",
      "Epoch 3140/4000: Train Loss: 2836.4304094127 Test Loss: 2996.2864251514693\n",
      "Epoch 3141/4000: Train Loss: 2836.4298742700257 Test Loss: 2996.285470859901\n",
      "Epoch 3142/4000: Train Loss: 2836.4293396901594 Test Loss: 2996.2845174219806\n",
      "Epoch 3143/4000: Train Loss: 2836.42880567251 Test Loss: 2996.2835648368896\n",
      "Epoch 3144/4000: Train Loss: 2836.4282722164858 Test Loss: 2996.2826131038096\n",
      "Epoch 3145/4000: Train Loss: 2836.427739321497 Test Loss: 2996.281662221925\n",
      "Epoch 3146/4000: Train Loss: 2836.4272069869517 Test Loss: 2996.2807121904184\n",
      "Epoch 3147/4000: Train Loss: 2836.426675212262 Test Loss: 2996.2797630084738\n",
      "Epoch 3148/4000: Train Loss: 2836.426143996839 Test Loss: 2996.2788146752755\n",
      "Epoch 3149/4000: Train Loss: 2836.4256133400936 Test Loss: 2996.2778671900114\n",
      "Epoch 3150/4000: Train Loss: 2836.4250832414396 Test Loss: 2996.276920551866\n",
      "Epoch 3151/4000: Train Loss: 2836.424553700289 Test Loss: 2996.2759747600307\n",
      "Epoch 3152/4000: Train Loss: 2836.4240247160556 Test Loss: 2996.2750298136907\n",
      "Epoch 3153/4000: Train Loss: 2836.4234962881537 Test Loss: 2996.274085712037\n",
      "Epoch 3154/4000: Train Loss: 2836.422968415999 Test Loss: 2996.2731424542585\n",
      "Epoch 3155/4000: Train Loss: 2836.422441099006 Test Loss: 2996.272200039547\n",
      "Epoch 3156/4000: Train Loss: 2836.4219143365917 Test Loss: 2996.2712584670926\n",
      "Epoch 3157/4000: Train Loss: 2836.4213881281717 Test Loss: 2996.2703177360895\n",
      "Epoch 3158/4000: Train Loss: 2836.420862473165 Test Loss: 2996.2693778457306\n",
      "Epoch 3159/4000: Train Loss: 2836.4203373709875 Test Loss: 2996.2684387952104\n",
      "Epoch 3160/4000: Train Loss: 2836.4198128210596 Test Loss: 2996.2675005837227\n",
      "Epoch 3161/4000: Train Loss: 2836.4192888228 Test Loss: 2996.2665632104645\n",
      "Epoch 3162/4000: Train Loss: 2836.4187653756276 Test Loss: 2996.265626674631\n",
      "Epoch 3163/4000: Train Loss: 2836.418242478964 Test Loss: 2996.264690975421\n",
      "Epoch 3164/4000: Train Loss: 2836.4177201322304 Test Loss: 2996.2637561120314\n",
      "Epoch 3165/4000: Train Loss: 2836.4171983348465 Test Loss: 2996.262822083662\n",
      "Epoch 3166/4000: Train Loss: 2836.416677086237 Test Loss: 2996.2618888895113\n",
      "Epoch 3167/4000: Train Loss: 2836.4161563858233 Test Loss: 2996.260956528781\n",
      "Epoch 3168/4000: Train Loss: 2836.415636233029 Test Loss: 2996.2600250006735\n",
      "Epoch 3169/4000: Train Loss: 2836.415116627279 Test Loss: 2996.2590943043892\n",
      "Epoch 3170/4000: Train Loss: 2836.414597567996 Test Loss: 2996.258164439131\n",
      "Epoch 3171/4000: Train Loss: 2836.414079054608 Test Loss: 2996.257235404103\n",
      "Epoch 3172/4000: Train Loss: 2836.413561086539 Test Loss: 2996.2563071985096\n",
      "Epoch 3173/4000: Train Loss: 2836.413043663216 Test Loss: 2996.2553798215567\n",
      "Epoch 3174/4000: Train Loss: 2836.4125267840664 Test Loss: 2996.2544532724505\n",
      "Epoch 3175/4000: Train Loss: 2836.4120104485173 Test Loss: 2996.253527550397\n",
      "Epoch 3176/4000: Train Loss: 2836.411494655997 Test Loss: 2996.2526026546047\n",
      "Epoch 3177/4000: Train Loss: 2836.4109794059354 Test Loss: 2996.2516785842827\n",
      "Epoch 3178/4000: Train Loss: 2836.4104646977607 Test Loss: 2996.2507553386376\n",
      "Epoch 3179/4000: Train Loss: 2836.409950530904 Test Loss: 2996.249832916884\n",
      "Epoch 3180/4000: Train Loss: 2836.4094369047953 Test Loss: 2996.2489113182282\n",
      "Epoch 3181/4000: Train Loss: 2836.408923818866 Test Loss: 2996.2479905418845\n",
      "Epoch 3182/4000: Train Loss: 2836.408411272548 Test Loss: 2996.247070587065\n",
      "Epoch 3183/4000: Train Loss: 2836.4078992652744 Test Loss: 2996.2461514529823\n",
      "Epoch 3184/4000: Train Loss: 2836.4073877964784 Test Loss: 2996.24523313885\n",
      "Epoch 3185/4000: Train Loss: 2836.4068768655925 Test Loss: 2996.2443156438853\n",
      "Epoch 3186/4000: Train Loss: 2836.4063664720516 Test Loss: 2996.2433989673013\n",
      "Epoch 3187/4000: Train Loss: 2836.4058566152903 Test Loss: 2996.242483108316\n",
      "Epoch 3188/4000: Train Loss: 2836.405347294745 Test Loss: 2996.241568066145\n",
      "Epoch 3189/4000: Train Loss: 2836.404838509851 Test Loss: 2996.2406538400073\n",
      "Epoch 3190/4000: Train Loss: 2836.404330260045 Test Loss: 2996.239740429122\n",
      "Epoch 3191/4000: Train Loss: 2836.403822544764 Test Loss: 2996.238827832708\n",
      "Epoch 3192/4000: Train Loss: 2836.403315363446 Test Loss: 2996.237916049985\n",
      "Epoch 3193/4000: Train Loss: 2836.40280871553 Test Loss: 2996.237005080176\n",
      "Epoch 3194/4000: Train Loss: 2836.4023026004547 Test Loss: 2996.2360949225017\n",
      "Epoch 3195/4000: Train Loss: 2836.401797017659 Test Loss: 2996.235185576183\n",
      "Epoch 3196/4000: Train Loss: 2836.4012919665847 Test Loss: 2996.2342770404457\n",
      "Epoch 3197/4000: Train Loss: 2836.4007874466706 Test Loss: 2996.2333693145147\n",
      "Epoch 3198/4000: Train Loss: 2836.40028345736 Test Loss: 2996.232462397612\n",
      "Epoch 3199/4000: Train Loss: 2836.3997799980934 Test Loss: 2996.2315562889653\n",
      "Epoch 3200/4000: Train Loss: 2836.3992770683144 Test Loss: 2996.2306509878013\n",
      "Epoch 3201/4000: Train Loss: 2836.398774667465 Test Loss: 2996.229746493346\n",
      "Epoch 3202/4000: Train Loss: 2836.3982727949906 Test Loss: 2996.2288428048278\n",
      "Epoch 3203/4000: Train Loss: 2836.3977714503335 Test Loss: 2996.2279399214767\n",
      "Epoch 3204/4000: Train Loss: 2836.39727063294 Test Loss: 2996.227037842522\n",
      "Epoch 3205/4000: Train Loss: 2836.3967703422554 Test Loss: 2996.226136567192\n",
      "Epoch 3206/4000: Train Loss: 2836.3962705777253 Test Loss: 2996.225236094721\n",
      "Epoch 3207/4000: Train Loss: 2836.3957713387963 Test Loss: 2996.2243364243386\n",
      "Epoch 3208/4000: Train Loss: 2836.3952726249167 Test Loss: 2996.2234375552785\n",
      "Epoch 3209/4000: Train Loss: 2836.394774435533 Test Loss: 2996.222539486774\n",
      "Epoch 3210/4000: Train Loss: 2836.394276770094 Test Loss: 2996.2216422180572\n",
      "Epoch 3211/4000: Train Loss: 2836.3937796280493 Test Loss: 2996.220745748368\n",
      "Epoch 3212/4000: Train Loss: 2836.393283008847 Test Loss: 2996.2198500769377\n",
      "Epoch 3213/4000: Train Loss: 2836.3927869119384 Test Loss: 2996.218955203004\n",
      "Epoch 3214/4000: Train Loss: 2836.392291336774 Test Loss: 2996.218061125805\n",
      "Epoch 3215/4000: Train Loss: 2836.3917962828054 Test Loss: 2996.2171678445784\n",
      "Epoch 3216/4000: Train Loss: 2836.391301749484 Test Loss: 2996.2162753585626\n",
      "Epoch 3217/4000: Train Loss: 2836.390807736262 Test Loss: 2996.215383666997\n",
      "Epoch 3218/4000: Train Loss: 2836.390314242593 Test Loss: 2996.214492769123\n",
      "Epoch 3219/4000: Train Loss: 2836.3898212679305 Test Loss: 2996.2136026641797\n",
      "Epoch 3220/4000: Train Loss: 2836.3893288117283 Test Loss: 2996.2127133514105\n",
      "Epoch 3221/4000: Train Loss: 2836.3888368734415 Test Loss: 2996.211824830059\n",
      "Epoch 3222/4000: Train Loss: 2836.3883454525244 Test Loss: 2996.210937099366\n",
      "Epoch 3223/4000: Train Loss: 2836.3878545484345 Test Loss: 2996.2100501585765\n",
      "Epoch 3224/4000: Train Loss: 2836.3873641606274 Test Loss: 2996.2091640069366\n",
      "Epoch 3225/4000: Train Loss: 2836.3868742885597 Test Loss: 2996.2082786436904\n",
      "Epoch 3226/4000: Train Loss: 2836.38638493169 Test Loss: 2996.2073940680843\n",
      "Epoch 3227/4000: Train Loss: 2836.385896089476 Test Loss: 2996.2065102793676\n",
      "Epoch 3228/4000: Train Loss: 2836.385407761376 Test Loss: 2996.2056272767845\n",
      "Epoch 3229/4000: Train Loss: 2836.3849199468495 Test Loss: 2996.204745059587\n",
      "Epoch 3230/4000: Train Loss: 2836.3844326453577 Test Loss: 2996.203863627021\n",
      "Epoch 3231/4000: Train Loss: 2836.383945856359 Test Loss: 2996.202982978341\n",
      "Epoch 3232/4000: Train Loss: 2836.3834595793155 Test Loss: 2996.202103112794\n",
      "Epoch 3233/4000: Train Loss: 2836.3829738136887 Test Loss: 2996.2012240296353\n",
      "Epoch 3234/4000: Train Loss: 2836.3824885589406 Test Loss: 2996.200345728113\n",
      "Epoch 3235/4000: Train Loss: 2836.3820038145336 Test Loss: 2996.199468207484\n",
      "Epoch 3236/4000: Train Loss: 2836.3815195799316 Test Loss: 2996.1985914670004\n",
      "Epoch 3237/4000: Train Loss: 2836.381035854599 Test Loss: 2996.1977155059158\n",
      "Epoch 3238/4000: Train Loss: 2836.3805526379983 Test Loss: 2996.196840323487\n",
      "Epoch 3239/4000: Train Loss: 2836.3800699295966 Test Loss: 2996.19596591897\n",
      "Epoch 3240/4000: Train Loss: 2836.3795877288576 Test Loss: 2996.195092291622\n",
      "Epoch 3241/4000: Train Loss: 2836.3791060352482 Test Loss: 2996.1942194407\n",
      "Epoch 3242/4000: Train Loss: 2836.378624848235 Test Loss: 2996.1933473654612\n",
      "Epoch 3243/4000: Train Loss: 2836.3781441672854 Test Loss: 2996.1924760651655\n",
      "Epoch 3244/4000: Train Loss: 2836.377663991867 Test Loss: 2996.191605539074\n",
      "Epoch 3245/4000: Train Loss: 2836.377184321448 Test Loss: 2996.190735786445\n",
      "Epoch 3246/4000: Train Loss: 2836.3767051554973 Test Loss: 2996.189866806543\n",
      "Epoch 3247/4000: Train Loss: 2836.3762264934853 Test Loss: 2996.188998598627\n",
      "Epoch 3248/4000: Train Loss: 2836.375748334881 Test Loss: 2996.188131161961\n",
      "Epoch 3249/4000: Train Loss: 2836.3752706791547 Test Loss: 2996.1872644958066\n",
      "Epoch 3250/4000: Train Loss: 2836.374793525778 Test Loss: 2996.186398599431\n",
      "Epoch 3251/4000: Train Loss: 2836.3743168742226 Test Loss: 2996.185533472097\n",
      "Epoch 3252/4000: Train Loss: 2836.3738407239607 Test Loss: 2996.184669113072\n",
      "Epoch 3253/4000: Train Loss: 2836.3733650744653 Test Loss: 2996.1838055216217\n",
      "Epoch 3254/4000: Train Loss: 2836.3728899252096 Test Loss: 2996.1829426970116\n",
      "Epoch 3255/4000: Train Loss: 2836.3724152756677 Test Loss: 2996.182080638512\n",
      "Epoch 3256/4000: Train Loss: 2836.371941125313 Test Loss: 2996.1812193453907\n",
      "Epoch 3257/4000: Train Loss: 2836.371467473621 Test Loss: 2996.1803588169173\n",
      "Epoch 3258/4000: Train Loss: 2836.370994320069 Test Loss: 2996.1794990523613\n",
      "Epoch 3259/4000: Train Loss: 2836.3705216641306 Test Loss: 2996.1786400509927\n",
      "Epoch 3260/4000: Train Loss: 2836.370049505284 Test Loss: 2996.177781812086\n",
      "Epoch 3261/4000: Train Loss: 2836.369577843006 Test Loss: 2996.1769243349095\n",
      "Epoch 3262/4000: Train Loss: 2836.3691066767733 Test Loss: 2996.1760676187387\n",
      "Epoch 3263/4000: Train Loss: 2836.3686360060665 Test Loss: 2996.175211662847\n",
      "Epoch 3264/4000: Train Loss: 2836.368165830362 Test Loss: 2996.174356466508\n",
      "Epoch 3265/4000: Train Loss: 2836.367696149141 Test Loss: 2996.1735020289984\n",
      "Epoch 3266/4000: Train Loss: 2836.3672269618833 Test Loss: 2996.172648349593\n",
      "Epoch 3267/4000: Train Loss: 2836.366758268068 Test Loss: 2996.1717954275678\n",
      "Epoch 3268/4000: Train Loss: 2836.3662900671775 Test Loss: 2996.1709432622015\n",
      "Epoch 3269/4000: Train Loss: 2836.3658223586935 Test Loss: 2996.170091852772\n",
      "Epoch 3270/4000: Train Loss: 2836.3653551420966 Test Loss: 2996.1692411985573\n",
      "Epoch 3271/4000: Train Loss: 2836.364888416871 Test Loss: 2996.1683912988374\n",
      "Epoch 3272/4000: Train Loss: 2836.364422182499 Test Loss: 2996.167542152891\n",
      "Epoch 3273/4000: Train Loss: 2836.3639564384653 Test Loss: 2996.166693760001\n",
      "Epoch 3274/4000: Train Loss: 2836.3634911842537 Test Loss: 2996.1658461194497\n",
      "Epoch 3275/4000: Train Loss: 2836.3630264193484 Test Loss: 2996.164999230517\n",
      "Epoch 3276/4000: Train Loss: 2836.362562143236 Test Loss: 2996.1641530924867\n",
      "Epoch 3277/4000: Train Loss: 2836.362098355402 Test Loss: 2996.163307704645\n",
      "Epoch 3278/4000: Train Loss: 2836.3616350553325 Test Loss: 2996.162463066273\n",
      "Epoch 3279/4000: Train Loss: 2836.361172242515 Test Loss: 2996.161619176656\n",
      "Epoch 3280/4000: Train Loss: 2836.360709916436 Test Loss: 2996.160776035084\n",
      "Epoch 3281/4000: Train Loss: 2836.360248076585 Test Loss: 2996.1599336408394\n",
      "Epoch 3282/4000: Train Loss: 2836.3597867224503 Test Loss: 2996.1590919932114\n",
      "Epoch 3283/4000: Train Loss: 2836.35932585352 Test Loss: 2996.158251091488\n",
      "Epoch 3284/4000: Train Loss: 2836.3588654692862 Test Loss: 2996.1574109349585\n",
      "Epoch 3285/4000: Train Loss: 2836.3584055692368 Test Loss: 2996.1565715229103\n",
      "Epoch 3286/4000: Train Loss: 2836.3579461528634 Test Loss: 2996.155732854636\n",
      "Epoch 3287/4000: Train Loss: 2836.357487219657 Test Loss: 2996.1548949294242\n",
      "Epoch 3288/4000: Train Loss: 2836.3570287691095 Test Loss: 2996.1540577465694\n",
      "Epoch 3289/4000: Train Loss: 2836.3565708007145 Test Loss: 2996.1532213053615\n",
      "Epoch 3290/4000: Train Loss: 2836.3561133139633 Test Loss: 2996.152385605094\n",
      "Epoch 3291/4000: Train Loss: 2836.35565630835 Test Loss: 2996.151550645061\n",
      "Epoch 3292/4000: Train Loss: 2836.355199783369 Test Loss: 2996.1507164245563\n",
      "Epoch 3293/4000: Train Loss: 2836.3547437385137 Test Loss: 2996.1498829428765\n",
      "Epoch 3294/4000: Train Loss: 2836.354288173281 Test Loss: 2996.149050199317\n",
      "Epoch 3295/4000: Train Loss: 2836.353833087164 Test Loss: 2996.148218193173\n",
      "Epoch 3296/4000: Train Loss: 2836.353378479661 Test Loss: 2996.147386923743\n",
      "Epoch 3297/4000: Train Loss: 2836.3529243502676 Test Loss: 2996.1465563903253\n",
      "Epoch 3298/4000: Train Loss: 2836.3524706984817 Test Loss: 2996.145726592217\n",
      "Epoch 3299/4000: Train Loss: 2836.3520175237995 Test Loss: 2996.144897528718\n",
      "Epoch 3300/4000: Train Loss: 2836.351564825721 Test Loss: 2996.1440691991293\n",
      "Epoch 3301/4000: Train Loss: 2836.3511126037447 Test Loss: 2996.1432416027496\n",
      "Epoch 3302/4000: Train Loss: 2836.350660857368 Test Loss: 2996.1424147388834\n",
      "Epoch 3303/4000: Train Loss: 2836.350209586093 Test Loss: 2996.1415886068307\n",
      "Epoch 3304/4000: Train Loss: 2836.349758789419 Test Loss: 2996.140763205895\n",
      "Epoch 3305/4000: Train Loss: 2836.3493084668467 Test Loss: 2996.1399385353784\n",
      "Epoch 3306/4000: Train Loss: 2836.348858617878 Test Loss: 2996.1391145945877\n",
      "Epoch 3307/4000: Train Loss: 2836.3484092420144 Test Loss: 2996.138291382826\n",
      "Epoch 3308/4000: Train Loss: 2836.347960338758 Test Loss: 2996.137468899399\n",
      "Epoch 3309/4000: Train Loss: 2836.3475119076134 Test Loss: 2996.1366471436145\n",
      "Epoch 3310/4000: Train Loss: 2836.3470639480825 Test Loss: 2996.1358261147775\n",
      "Epoch 3311/4000: Train Loss: 2836.3466164596693 Test Loss: 2996.135005812197\n",
      "Epoch 3312/4000: Train Loss: 2836.3461694418793 Test Loss: 2996.134186235181\n",
      "Epoch 3313/4000: Train Loss: 2836.3457228942166 Test Loss: 2996.1333673830377\n",
      "Epoch 3314/4000: Train Loss: 2836.3452768161874 Test Loss: 2996.1325492550777\n",
      "Epoch 3315/4000: Train Loss: 2836.344831207298 Test Loss: 2996.1317318506112\n",
      "Epoch 3316/4000: Train Loss: 2836.3443860670545 Test Loss: 2996.1309151689507\n",
      "Epoch 3317/4000: Train Loss: 2836.343941394964 Test Loss: 2996.1300992094057\n",
      "Epoch 3318/4000: Train Loss: 2836.343497190534 Test Loss: 2996.12928397129\n",
      "Epoch 3319/4000: Train Loss: 2836.3430534532736 Test Loss: 2996.1284694539168\n",
      "Epoch 3320/4000: Train Loss: 2836.34261018269 Test Loss: 2996.1276556566004\n",
      "Epoch 3321/4000: Train Loss: 2836.3421673782937 Test Loss: 2996.126842578654\n",
      "Epoch 3322/4000: Train Loss: 2836.3417250395933 Test Loss: 2996.126030219393\n",
      "Epoch 3323/4000: Train Loss: 2836.3412831661003 Test Loss: 2996.125218578134\n",
      "Epoch 3324/4000: Train Loss: 2836.3408417573246 Test Loss: 2996.124407654194\n",
      "Epoch 3325/4000: Train Loss: 2836.3404008127773 Test Loss: 2996.1235974468905\n",
      "Epoch 3326/4000: Train Loss: 2836.3399603319713 Test Loss: 2996.1227879555386\n",
      "Epoch 3327/4000: Train Loss: 2836.3395203144173 Test Loss: 2996.121979179461\n",
      "Epoch 3328/4000: Train Loss: 2836.3390807596293 Test Loss: 2996.121171117974\n",
      "Epoch 3329/4000: Train Loss: 2836.33864166712 Test Loss: 2996.1203637703998\n",
      "Epoch 3330/4000: Train Loss: 2836.338203036404 Test Loss: 2996.1195571360563\n",
      "Epoch 3331/4000: Train Loss: 2836.337764866994 Test Loss: 2996.1187512142674\n",
      "Epoch 3332/4000: Train Loss: 2836.337327158407 Test Loss: 2996.1179460043545\n",
      "Epoch 3333/4000: Train Loss: 2836.3368899101565 Test Loss: 2996.117141505641\n",
      "Epoch 3334/4000: Train Loss: 2836.33645312176 Test Loss: 2996.116337717448\n",
      "Epoch 3335/4000: Train Loss: 2836.336016792732 Test Loss: 2996.115534639099\n",
      "Epoch 3336/4000: Train Loss: 2836.335580922591 Test Loss: 2996.114732269923\n",
      "Epoch 3337/4000: Train Loss: 2836.3351455108536 Test Loss: 2996.1139306092423\n",
      "Epoch 3338/4000: Train Loss: 2836.334710557038 Test Loss: 2996.113129656385\n",
      "Epoch 3339/4000: Train Loss: 2836.334276060662 Test Loss: 2996.112329410674\n",
      "Epoch 3340/4000: Train Loss: 2836.3338420212453 Test Loss: 2996.1115298714417\n",
      "Epoch 3341/4000: Train Loss: 2836.333408438307 Test Loss: 2996.110731038012\n",
      "Epoch 3342/4000: Train Loss: 2836.332975311367 Test Loss: 2996.109932909716\n",
      "Epoch 3343/4000: Train Loss: 2836.332542639946 Test Loss: 2996.1091354858822\n",
      "Epoch 3344/4000: Train Loss: 2836.3321104235642 Test Loss: 2996.108338765842\n",
      "Epoch 3345/4000: Train Loss: 2836.3316786617443 Test Loss: 2996.1075427489236\n",
      "Epoch 3346/4000: Train Loss: 2836.3312473540063 Test Loss: 2996.106747434461\n",
      "Epoch 3347/4000: Train Loss: 2836.330816499875 Test Loss: 2996.105952821785\n",
      "Epoch 3348/4000: Train Loss: 2836.3303860988713 Test Loss: 2996.105158910228\n",
      "Epoch 3349/4000: Train Loss: 2836.32995615052 Test Loss: 2996.1043656991237\n",
      "Epoch 3350/4000: Train Loss: 2836.329526654344 Test Loss: 2996.103573187807\n",
      "Epoch 3351/4000: Train Loss: 2836.3290976098683 Test Loss: 2996.1027813756127\n",
      "Epoch 3352/4000: Train Loss: 2836.3286690166187 Test Loss: 2996.1019902618764\n",
      "Epoch 3353/4000: Train Loss: 2836.3282408741193 Test Loss: 2996.101199845931\n",
      "Epoch 3354/4000: Train Loss: 2836.3278131818965 Test Loss: 2996.1004101271164\n",
      "Epoch 3355/4000: Train Loss: 2836.3273859394767 Test Loss: 2996.0996211047695\n",
      "Epoch 3356/4000: Train Loss: 2836.326959146387 Test Loss: 2996.0988327782293\n",
      "Epoch 3357/4000: Train Loss: 2836.326532802155 Test Loss: 2996.098045146832\n",
      "Epoch 3358/4000: Train Loss: 2836.326106906308 Test Loss: 2996.097258209918\n",
      "Epoch 3359/4000: Train Loss: 2836.3256814583747 Test Loss: 2996.0964719668273\n",
      "Epoch 3360/4000: Train Loss: 2836.3252564578843 Test Loss: 2996.095686416902\n",
      "Epoch 3361/4000: Train Loss: 2836.3248319043664 Test Loss: 2996.094901559482\n",
      "Epoch 3362/4000: Train Loss: 2836.3244077973504 Test Loss: 2996.094117393909\n",
      "Epoch 3363/4000: Train Loss: 2836.3239841363666 Test Loss: 2996.0933339195262\n",
      "Epoch 3364/4000: Train Loss: 2836.323560920947 Test Loss: 2996.0925511356777\n",
      "Epoch 3365/4000: Train Loss: 2836.3231381506216 Test Loss: 2996.0917690417064\n",
      "Epoch 3366/4000: Train Loss: 2836.3227158249238 Test Loss: 2996.0909876369574\n",
      "Epoch 3367/4000: Train Loss: 2836.3222939433836 Test Loss: 2996.0902069207755\n",
      "Epoch 3368/4000: Train Loss: 2836.321872505536 Test Loss: 2996.0894268925063\n",
      "Epoch 3369/4000: Train Loss: 2836.321451510914 Test Loss: 2996.0886475514976\n",
      "Epoch 3370/4000: Train Loss: 2836.3210309590518 Test Loss: 2996.0878688970965\n",
      "Epoch 3371/4000: Train Loss: 2836.3206108494815 Test Loss: 2996.0870909286486\n",
      "Epoch 3372/4000: Train Loss: 2836.320191181741 Test Loss: 2996.086313645506\n",
      "Epoch 3373/4000: Train Loss: 2836.3197719553636 Test Loss: 2996.0855370470154\n",
      "Epoch 3374/4000: Train Loss: 2836.319353169886 Test Loss: 2996.084761132527\n",
      "Epoch 3375/4000: Train Loss: 2836.3189348248447 Test Loss: 2996.0839859013918\n",
      "Epoch 3376/4000: Train Loss: 2836.3185169197754 Test Loss: 2996.0832113529596\n",
      "Epoch 3377/4000: Train Loss: 2836.3180994542163 Test Loss: 2996.082437486585\n",
      "Epoch 3378/4000: Train Loss: 2836.317682427705 Test Loss: 2996.081664301617\n",
      "Epoch 3379/4000: Train Loss: 2836.3172658397793 Test Loss: 2996.0808917974105\n",
      "Epoch 3380/4000: Train Loss: 2836.316849689979 Test Loss: 2996.0801199733187\n",
      "Epoch 3381/4000: Train Loss: 2836.316433977842 Test Loss: 2996.079348828696\n",
      "Epoch 3382/4000: Train Loss: 2836.316018702909 Test Loss: 2996.0785783628976\n",
      "Epoch 3383/4000: Train Loss: 2836.31560386472 Test Loss: 2996.0778085752795\n",
      "Epoch 3384/4000: Train Loss: 2836.315189462815 Test Loss: 2996.0770394651963\n",
      "Epoch 3385/4000: Train Loss: 2836.314775496736 Test Loss: 2996.0762710320073\n",
      "Epoch 3386/4000: Train Loss: 2836.3143619660236 Test Loss: 2996.0755032750667\n",
      "Epoch 3387/4000: Train Loss: 2836.3139488702213 Test Loss: 2996.074736193737\n",
      "Epoch 3388/4000: Train Loss: 2836.3135362088706 Test Loss: 2996.073969787374\n",
      "Epoch 3389/4000: Train Loss: 2836.3131239815157 Test Loss: 2996.0732040553385\n",
      "Epoch 3390/4000: Train Loss: 2836.3127121876996 Test Loss: 2996.072438996988\n",
      "Epoch 3391/4000: Train Loss: 2836.3123008269654 Test Loss: 2996.0716746116877\n",
      "Epoch 3392/4000: Train Loss: 2836.311889898859 Test Loss: 2996.0709108987944\n",
      "Epoch 3393/4000: Train Loss: 2836.3114794029248 Test Loss: 2996.070147857673\n",
      "Epoch 3394/4000: Train Loss: 2836.3110693387084 Test Loss: 2996.069385487685\n",
      "Epoch 3395/4000: Train Loss: 2836.3106597057554 Test Loss: 2996.0686237881937\n",
      "Epoch 3396/4000: Train Loss: 2836.310250503613 Test Loss: 2996.067862758563\n",
      "Epoch 3397/4000: Train Loss: 2836.309841731828 Test Loss: 2996.067102398159\n",
      "Epoch 3398/4000: Train Loss: 2836.3094333899467 Test Loss: 2996.066342706344\n",
      "Epoch 3399/4000: Train Loss: 2836.309025477518 Test Loss: 2996.0655836824862\n",
      "Epoch 3400/4000: Train Loss: 2836.30861799409 Test Loss: 2996.0648253259506\n",
      "Epoch 3401/4000: Train Loss: 2836.3082109392117 Test Loss: 2996.0640676361054\n",
      "Epoch 3402/4000: Train Loss: 2836.307804312432 Test Loss: 2996.0633106123173\n",
      "Epoch 3403/4000: Train Loss: 2836.3073981133016 Test Loss: 2996.0625542539547\n",
      "Epoch 3404/4000: Train Loss: 2836.306992341369 Test Loss: 2996.061798560387\n",
      "Epoch 3405/4000: Train Loss: 2836.3065869961865 Test Loss: 2996.061043530983\n",
      "Epoch 3406/4000: Train Loss: 2836.3061820773046 Test Loss: 2996.0602891651138\n",
      "Epoch 3407/4000: Train Loss: 2836.3057775842753 Test Loss: 2996.0595354621496\n",
      "Epoch 3408/4000: Train Loss: 2836.3053735166504 Test Loss: 2996.0587824214626\n",
      "Epoch 3409/4000: Train Loss: 2836.304969873983 Test Loss: 2996.0580300424244\n",
      "Epoch 3410/4000: Train Loss: 2836.304566655825 Test Loss: 2996.0572783244074\n",
      "Epoch 3411/4000: Train Loss: 2836.304163861731 Test Loss: 2996.0565272667855\n",
      "Epoch 3412/4000: Train Loss: 2836.3037614912546 Test Loss: 2996.055776868933\n",
      "Epoch 3413/4000: Train Loss: 2836.303359543951 Test Loss: 2996.0550271302213\n",
      "Epoch 3414/4000: Train Loss: 2836.3029580193743 Test Loss: 2996.0542780500296\n",
      "Epoch 3415/4000: Train Loss: 2836.30255691708 Test Loss: 2996.0535296277335\n",
      "Epoch 3416/4000: Train Loss: 2836.302156236625 Test Loss: 2996.052781862706\n",
      "Epoch 3417/4000: Train Loss: 2836.301755977564 Test Loss: 2996.0520347543275\n",
      "Epoch 3418/4000: Train Loss: 2836.301356139455 Test Loss: 2996.051288301974\n",
      "Epoch 3419/4000: Train Loss: 2836.300956721855 Test Loss: 2996.0505425050233\n",
      "Epoch 3420/4000: Train Loss: 2836.3005577243216 Test Loss: 2996.049797362856\n",
      "Epoch 3421/4000: Train Loss: 2836.3001591464126 Test Loss: 2996.049052874852\n",
      "Epoch 3422/4000: Train Loss: 2836.299760987688 Test Loss: 2996.048309040389\n",
      "Epoch 3423/4000: Train Loss: 2836.2993632477064 Test Loss: 2996.0475658588493\n",
      "Epoch 3424/4000: Train Loss: 2836.2989659260265 Test Loss: 2996.0468233296133\n",
      "Epoch 3425/4000: Train Loss: 2836.298569022209 Test Loss: 2996.0460814520648\n",
      "Epoch 3426/4000: Train Loss: 2836.2981725358145 Test Loss: 2996.0453402255844\n",
      "Epoch 3427/4000: Train Loss: 2836.2977764664042 Test Loss: 2996.044599649556\n",
      "Epoch 3428/4000: Train Loss: 2836.2973808135393 Test Loss: 2996.043859723365\n",
      "Epoch 3429/4000: Train Loss: 2836.296985576782 Test Loss: 2996.0431204463925\n",
      "Epoch 3430/4000: Train Loss: 2836.2965907556936 Test Loss: 2996.0423818180275\n",
      "Epoch 3431/4000: Train Loss: 2836.2961963498383 Test Loss: 2996.041643837651\n",
      "Epoch 3432/4000: Train Loss: 2836.295802358779 Test Loss: 2996.040906504654\n",
      "Epoch 3433/4000: Train Loss: 2836.2954087820785 Test Loss: 2996.0401698184205\n",
      "Epoch 3434/4000: Train Loss: 2836.2950156193033 Test Loss: 2996.039433778339\n",
      "Epoch 3435/4000: Train Loss: 2836.2946228700152 Test Loss: 2996.038698383798\n",
      "Epoch 3436/4000: Train Loss: 2836.2942305337815 Test Loss: 2996.0379636341836\n",
      "Epoch 3437/4000: Train Loss: 2836.2938386101673 Test Loss: 2996.037229528889\n",
      "Epoch 3438/4000: Train Loss: 2836.2934470987375 Test Loss: 2996.036496067301\n",
      "Epoch 3439/4000: Train Loss: 2836.2930559990605 Test Loss: 2996.035763248812\n",
      "Epoch 3440/4000: Train Loss: 2836.292665310702 Test Loss: 2996.035031072812\n",
      "Epoch 3441/4000: Train Loss: 2836.2922750332295 Test Loss: 2996.0342995386936\n",
      "Epoch 3442/4000: Train Loss: 2836.2918851662107 Test Loss: 2996.0335686458484\n",
      "Epoch 3443/4000: Train Loss: 2836.291495709215 Test Loss: 2996.03283839367\n",
      "Epoch 3444/4000: Train Loss: 2836.2911066618094 Test Loss: 2996.032108781551\n",
      "Epoch 3445/4000: Train Loss: 2836.290718023565 Test Loss: 2996.0313798088873\n",
      "Epoch 3446/4000: Train Loss: 2836.2903297940506 Test Loss: 2996.0306514750714\n",
      "Epoch 3447/4000: Train Loss: 2836.2899419728365 Test Loss: 2996.0299237794998\n",
      "Epoch 3448/4000: Train Loss: 2836.289554559493 Test Loss: 2996.029196721569\n",
      "Epoch 3449/4000: Train Loss: 2836.2891675535907 Test Loss: 2996.0284703006746\n",
      "Epoch 3450/4000: Train Loss: 2836.288780954703 Test Loss: 2996.0277445162146\n",
      "Epoch 3451/4000: Train Loss: 2836.2883947624 Test Loss: 2996.0270193675847\n",
      "Epoch 3452/4000: Train Loss: 2836.2880089762552 Test Loss: 2996.026294854186\n",
      "Epoch 3453/4000: Train Loss: 2836.2876235958397 Test Loss: 2996.0255709754156\n",
      "Epoch 3454/4000: Train Loss: 2836.2872386207296 Test Loss: 2996.0248477306745\n",
      "Epoch 3455/4000: Train Loss: 2836.2868540504955 Test Loss: 2996.024125119361\n",
      "Epoch 3456/4000: Train Loss: 2836.2864698847143 Test Loss: 2996.0234031408777\n",
      "Epoch 3457/4000: Train Loss: 2836.286086122959 Test Loss: 2996.022681794625\n",
      "Epoch 3458/4000: Train Loss: 2836.2857027648056 Test Loss: 2996.021961080004\n",
      "Epoch 3459/4000: Train Loss: 2836.2853198098287 Test Loss: 2996.0212409964197\n",
      "Epoch 3460/4000: Train Loss: 2836.284937257605 Test Loss: 2996.0205215432725\n",
      "Epoch 3461/4000: Train Loss: 2836.284555107711 Test Loss: 2996.0198027199685\n",
      "Epoch 3462/4000: Train Loss: 2836.284173359723 Test Loss: 2996.019084525909\n",
      "Epoch 3463/4000: Train Loss: 2836.2837920132188 Test Loss: 2996.0183669605017\n",
      "Epoch 3464/4000: Train Loss: 2836.283411067776 Test Loss: 2996.0176500231514\n",
      "Epoch 3465/4000: Train Loss: 2836.283030522972 Test Loss: 2996.016933713263\n",
      "Epoch 3466/4000: Train Loss: 2836.2826503783876 Test Loss: 2996.0162180302427\n",
      "Epoch 3467/4000: Train Loss: 2836.282270633599 Test Loss: 2996.015502973501\n",
      "Epoch 3468/4000: Train Loss: 2836.281891288188 Test Loss: 2996.0147885424426\n",
      "Epoch 3469/4000: Train Loss: 2836.281512341734 Test Loss: 2996.014074736478\n",
      "Epoch 3470/4000: Train Loss: 2836.281133793817 Test Loss: 2996.013361555015\n",
      "Epoch 3471/4000: Train Loss: 2836.2807556440184 Test Loss: 2996.0126489974627\n",
      "Epoch 3472/4000: Train Loss: 2836.280377891919 Test Loss: 2996.011937063232\n",
      "Epoch 3473/4000: Train Loss: 2836.2800005371005 Test Loss: 2996.011225751735\n",
      "Epoch 3474/4000: Train Loss: 2836.2796235791457 Test Loss: 2996.0105150623795\n",
      "Epoch 3475/4000: Train Loss: 2836.2792470176364 Test Loss: 2996.0098049945814\n",
      "Epoch 3476/4000: Train Loss: 2836.278870852156 Test Loss: 2996.009095547751\n",
      "Epoch 3477/4000: Train Loss: 2836.2784950822893 Test Loss: 2996.008386721301\n",
      "Epoch 3478/4000: Train Loss: 2836.278119707618 Test Loss: 2996.0076785146466\n",
      "Epoch 3479/4000: Train Loss: 2836.2777447277276 Test Loss: 2996.0069709272025\n",
      "Epoch 3480/4000: Train Loss: 2836.2773701422034 Test Loss: 2996.0062639583807\n",
      "Epoch 3481/4000: Train Loss: 2836.276995950629 Test Loss: 2996.0055576075993\n",
      "Epoch 3482/4000: Train Loss: 2836.2766221525917 Test Loss: 2996.0048518742733\n",
      "Epoch 3483/4000: Train Loss: 2836.276248747677 Test Loss: 2996.0041467578208\n",
      "Epoch 3484/4000: Train Loss: 2836.275875735472 Test Loss: 2996.003442257656\n",
      "Epoch 3485/4000: Train Loss: 2836.2755031155625 Test Loss: 2996.002738373197\n",
      "Epoch 3486/4000: Train Loss: 2836.275130887537 Test Loss: 2996.0020351038656\n",
      "Epoch 3487/4000: Train Loss: 2836.274759050983 Test Loss: 2996.0013324490787\n",
      "Epoch 3488/4000: Train Loss: 2836.2743876054888 Test Loss: 2996.0006304082544\n",
      "Epoch 3489/4000: Train Loss: 2836.274016550643 Test Loss: 2995.9999289808134\n",
      "Epoch 3490/4000: Train Loss: 2836.273645886035 Test Loss: 2995.9992281661775\n",
      "Epoch 3491/4000: Train Loss: 2836.273275611254 Test Loss: 2995.9985279637676\n",
      "Epoch 3492/4000: Train Loss: 2836.2729057258903 Test Loss: 2995.9978283730043\n",
      "Epoch 3493/4000: Train Loss: 2836.272536229534 Test Loss: 2995.9971293933104\n",
      "Epoch 3494/4000: Train Loss: 2836.272167121777 Test Loss: 2995.9964310241103\n",
      "Epoch 3495/4000: Train Loss: 2836.2717984022097 Test Loss: 2995.995733264826\n",
      "Epoch 3496/4000: Train Loss: 2836.2714300704233 Test Loss: 2995.9950361148817\n",
      "Epoch 3497/4000: Train Loss: 2836.271062126012 Test Loss: 2995.9943395737014\n",
      "Epoch 3498/4000: Train Loss: 2836.2706945685663 Test Loss: 2995.993643640714\n",
      "Epoch 3499/4000: Train Loss: 2836.2703273976804 Test Loss: 2995.9929483153396\n",
      "Epoch 3500/4000: Train Loss: 2836.269960612947 Test Loss: 2995.99225359701\n",
      "Epoch 3501/4000: Train Loss: 2836.2695942139603 Test Loss: 2995.9915594851454\n",
      "Epoch 3502/4000: Train Loss: 2836.2692282003154 Test Loss: 2995.99086597918\n",
      "Epoch 3503/4000: Train Loss: 2836.268862571606 Test Loss: 2995.9901730785396\n",
      "Epoch 3504/4000: Train Loss: 2836.268497327427 Test Loss: 2995.9894807826518\n",
      "Epoch 3505/4000: Train Loss: 2836.268132467375 Test Loss: 2995.988789090947\n",
      "Epoch 3506/4000: Train Loss: 2836.2677679910457 Test Loss: 2995.9880980028543\n",
      "Epoch 3507/4000: Train Loss: 2836.2674038980354 Test Loss: 2995.987407517804\n",
      "Epoch 3508/4000: Train Loss: 2836.2670401879413 Test Loss: 2995.9867176352273\n",
      "Epoch 3509/4000: Train Loss: 2836.2666768603603 Test Loss: 2995.9860283545545\n",
      "Epoch 3510/4000: Train Loss: 2836.26631391489 Test Loss: 2995.9853396752205\n",
      "Epoch 3511/4000: Train Loss: 2836.2659513511285 Test Loss: 2995.984651596654\n",
      "Epoch 3512/4000: Train Loss: 2836.265589168675 Test Loss: 2995.9839641182907\n",
      "Epoch 3513/4000: Train Loss: 2836.2652273671283 Test Loss: 2995.9832772395634\n",
      "Epoch 3514/4000: Train Loss: 2836.264865946087 Test Loss: 2995.9825909599076\n",
      "Epoch 3515/4000: Train Loss: 2836.264504905151 Test Loss: 2995.981905278757\n",
      "Epoch 3516/4000: Train Loss: 2836.264144243922 Test Loss: 2995.9812201955465\n",
      "Epoch 3517/4000: Train Loss: 2836.2637839619997 Test Loss: 2995.9805357097134\n",
      "Epoch 3518/4000: Train Loss: 2836.2634240589855 Test Loss: 2995.979851820694\n",
      "Epoch 3519/4000: Train Loss: 2836.26306453448 Test Loss: 2995.979168527924\n",
      "Epoch 3520/4000: Train Loss: 2836.262705388085 Test Loss: 2995.978485830843\n",
      "Epoch 3521/4000: Train Loss: 2836.262346619405 Test Loss: 2995.977803728886\n",
      "Epoch 3522/4000: Train Loss: 2836.26198822804 Test Loss: 2995.977122221496\n",
      "Epoch 3523/4000: Train Loss: 2836.2616302135953 Test Loss: 2995.976441308109\n",
      "Epoch 3524/4000: Train Loss: 2836.261272575674 Test Loss: 2995.9757609881653\n",
      "Epoch 3525/4000: Train Loss: 2836.260915313879 Test Loss: 2995.975081261107\n",
      "Epoch 3526/4000: Train Loss: 2836.2605584278162 Test Loss: 2995.9744021263728\n",
      "Epoch 3527/4000: Train Loss: 2836.2602019170895 Test Loss: 2995.9737235834064\n",
      "Epoch 3528/4000: Train Loss: 2836.2598457813037 Test Loss: 2995.9730456316497\n",
      "Epoch 3529/4000: Train Loss: 2836.2594900200666 Test Loss: 2995.9723682705435\n",
      "Epoch 3530/4000: Train Loss: 2836.259134632982 Test Loss: 2995.9716914995315\n",
      "Epoch 3531/4000: Train Loss: 2836.258779619657 Test Loss: 2995.971015318058\n",
      "Epoch 3532/4000: Train Loss: 2836.2584249796996 Test Loss: 2995.9703397255676\n",
      "Epoch 3533/4000: Train Loss: 2836.2580707127163 Test Loss: 2995.9696647215046\n",
      "Epoch 3534/4000: Train Loss: 2836.2577168183143 Test Loss: 2995.968990305315\n",
      "Epoch 3535/4000: Train Loss: 2836.2573632961025 Test Loss: 2995.968316476443\n",
      "Epoch 3536/4000: Train Loss: 2836.25701014569 Test Loss: 2995.967643234336\n",
      "Epoch 3537/4000: Train Loss: 2836.2566573666845 Test Loss: 2995.9669705784427\n",
      "Epoch 3538/4000: Train Loss: 2836.256304958696 Test Loss: 2995.9662985082077\n",
      "Epoch 3539/4000: Train Loss: 2836.2559529213345 Test Loss: 2995.965627023081\n",
      "Epoch 3540/4000: Train Loss: 2836.25560125421 Test Loss: 2995.964956122511\n",
      "Epoch 3541/4000: Train Loss: 2836.255249956933 Test Loss: 2995.9642858059474\n",
      "Epoch 3542/4000: Train Loss: 2836.2548990291148 Test Loss: 2995.963616072839\n",
      "Epoch 3543/4000: Train Loss: 2836.2545484703664 Test Loss: 2995.9629469226356\n",
      "Epoch 3544/4000: Train Loss: 2836.2541982803 Test Loss: 2995.96227835479\n",
      "Epoch 3545/4000: Train Loss: 2836.253848458528 Test Loss: 2995.961610368752\n",
      "Epoch 3546/4000: Train Loss: 2836.253499004663 Test Loss: 2995.9609429639745\n",
      "Epoch 3547/4000: Train Loss: 2836.253149918317 Test Loss: 2995.960276139909\n",
      "Epoch 3548/4000: Train Loss: 2836.2528011991053 Test Loss: 2995.9596098960096\n",
      "Epoch 3549/4000: Train Loss: 2836.252452846641 Test Loss: 2995.9589442317274\n",
      "Epoch 3550/4000: Train Loss: 2836.2521048605377 Test Loss: 2995.9582791465195\n",
      "Epoch 3551/4000: Train Loss: 2836.251757240411 Test Loss: 2995.9576146398385\n",
      "Epoch 3552/4000: Train Loss: 2836.2514099858763 Test Loss: 2995.956950711142\n",
      "Epoch 3553/4000: Train Loss: 2836.251063096548 Test Loss: 2995.9562873598825\n",
      "Epoch 3554/4000: Train Loss: 2836.2507165720426 Test Loss: 2995.955624585519\n",
      "Epoch 3555/4000: Train Loss: 2836.2503704119763 Test Loss: 2995.9549623875077\n",
      "Epoch 3556/4000: Train Loss: 2836.2500246159657 Test Loss: 2995.954300765304\n",
      "Epoch 3557/4000: Train Loss: 2836.2496791836284 Test Loss: 2995.953639718369\n",
      "Epoch 3558/4000: Train Loss: 2836.2493341145814 Test Loss: 2995.952979246158\n",
      "Epoch 3559/4000: Train Loss: 2836.2489894084424 Test Loss: 2995.952319348132\n",
      "Epoch 3560/4000: Train Loss: 2836.2486450648307 Test Loss: 2995.9516600237484\n",
      "Epoch 3561/4000: Train Loss: 2836.2483010833644 Test Loss: 2995.9510012724686\n",
      "Epoch 3562/4000: Train Loss: 2836.247957463663 Test Loss: 2995.9503430937552\n",
      "Epoch 3563/4000: Train Loss: 2836.247614205345 Test Loss: 2995.949685487064\n",
      "Epoch 3564/4000: Train Loss: 2836.2472713080324 Test Loss: 2995.9490284518624\n",
      "Epoch 3565/4000: Train Loss: 2836.246928771343 Test Loss: 2995.9483719876084\n",
      "Epoch 3566/4000: Train Loss: 2836.2465865948993 Test Loss: 2995.9477160937668\n",
      "Epoch 3567/4000: Train Loss: 2836.246244778322 Test Loss: 2995.9470607698004\n",
      "Epoch 3568/4000: Train Loss: 2836.245903321232 Test Loss: 2995.946406015173\n",
      "Epoch 3569/4000: Train Loss: 2836.2455622232524 Test Loss: 2995.945751829348\n",
      "Epoch 3570/4000: Train Loss: 2836.245221484005 Test Loss: 2995.9450982117905\n",
      "Epoch 3571/4000: Train Loss: 2836.2448811031113 Test Loss: 2995.944445161967\n",
      "Epoch 3572/4000: Train Loss: 2836.2445410801965 Test Loss: 2995.9437926793426\n",
      "Epoch 3573/4000: Train Loss: 2836.244201414883 Test Loss: 2995.9431407633842\n",
      "Epoch 3574/4000: Train Loss: 2836.2438621067945 Test Loss: 2995.942489413556\n",
      "Epoch 3575/4000: Train Loss: 2836.2435231555555 Test Loss: 2995.94183862933\n",
      "Epoch 3576/4000: Train Loss: 2836.243184560791 Test Loss: 2995.9411884101696\n",
      "Epoch 3577/4000: Train Loss: 2836.2428463221263 Test Loss: 2995.940538755547\n",
      "Epoch 3578/4000: Train Loss: 2836.2425084391866 Test Loss: 2995.9398896649295\n",
      "Epoch 3579/4000: Train Loss: 2836.2421709115974 Test Loss: 2995.9392411377867\n",
      "Epoch 3580/4000: Train Loss: 2836.241833738986 Test Loss: 2995.9385931735887\n",
      "Epoch 3581/4000: Train Loss: 2836.2414969209776 Test Loss: 2995.937945771807\n",
      "Epoch 3582/4000: Train Loss: 2836.2411604572003 Test Loss: 2995.93729893191\n",
      "Epoch 3583/4000: Train Loss: 2836.240824347281 Test Loss: 2995.9366526533736\n",
      "Epoch 3584/4000: Train Loss: 2836.2404885908477 Test Loss: 2995.936006935666\n",
      "Epoch 3585/4000: Train Loss: 2836.2401531875294 Test Loss: 2995.935361778262\n",
      "Epoch 3586/4000: Train Loss: 2836.2398181369535 Test Loss: 2995.9347171806353\n",
      "Epoch 3587/4000: Train Loss: 2836.23948343875 Test Loss: 2995.934073142258\n",
      "Epoch 3588/4000: Train Loss: 2836.2391490925484 Test Loss: 2995.933429662605\n",
      "Epoch 3589/4000: Train Loss: 2836.2388150979773 Test Loss: 2995.9327867411525\n",
      "Epoch 3590/4000: Train Loss: 2836.238481454668 Test Loss: 2995.9321443773724\n",
      "Epoch 3591/4000: Train Loss: 2836.2381481622506 Test Loss: 2995.9315025707438\n",
      "Epoch 3592/4000: Train Loss: 2836.2378152203564 Test Loss: 2995.9308613207418\n",
      "Epoch 3593/4000: Train Loss: 2836.2374826286164 Test Loss: 2995.930220626843\n",
      "Epoch 3594/4000: Train Loss: 2836.2371503866625 Test Loss: 2995.929580488524\n",
      "Epoch 3595/4000: Train Loss: 2836.236818494126 Test Loss: 2995.928940905265\n",
      "Epoch 3596/4000: Train Loss: 2836.2364869506414 Test Loss: 2995.928301876542\n",
      "Epoch 3597/4000: Train Loss: 2836.23615575584 Test Loss: 2995.927663401837\n",
      "Epoch 3598/4000: Train Loss: 2836.2358249093554 Test Loss: 2995.9270254806265\n",
      "Epoch 3599/4000: Train Loss: 2836.2354944108215 Test Loss: 2995.926388112391\n",
      "Epoch 3600/4000: Train Loss: 2836.235164259872 Test Loss: 2995.925751296612\n",
      "Epoch 3601/4000: Train Loss: 2836.234834456142 Test Loss: 2995.9251150327686\n",
      "Epoch 3602/4000: Train Loss: 2836.234504999266 Test Loss: 2995.924479320346\n",
      "Epoch 3603/4000: Train Loss: 2836.2341758888792 Test Loss: 2995.923844158822\n",
      "Epoch 3604/4000: Train Loss: 2836.2338471246167 Test Loss: 2995.9232095476814\n",
      "Epoch 3605/4000: Train Loss: 2836.2335187061153 Test Loss: 2995.922575486406\n",
      "Epoch 3606/4000: Train Loss: 2836.2331906330105 Test Loss: 2995.921941974481\n",
      "Epoch 3607/4000: Train Loss: 2836.2328629049402 Test Loss: 2995.921309011388\n",
      "Epoch 3608/4000: Train Loss: 2836.2325355215407 Test Loss: 2995.9206765966146\n",
      "Epoch 3609/4000: Train Loss: 2836.2322084824496 Test Loss: 2995.920044729644\n",
      "Epoch 3610/4000: Train Loss: 2836.2318817873042 Test Loss: 2995.9194134099625\n",
      "Epoch 3611/4000: Train Loss: 2836.231555435744 Test Loss: 2995.918782637056\n",
      "Epoch 3612/4000: Train Loss: 2836.231229427408 Test Loss: 2995.918152410409\n",
      "Epoch 3613/4000: Train Loss: 2836.2309037619325 Test Loss: 2995.9175227295123\n",
      "Epoch 3614/4000: Train Loss: 2836.23057843896 Test Loss: 2995.9168935938515\n",
      "Epoch 3615/4000: Train Loss: 2836.2302534581286 Test Loss: 2995.9162650029143\n",
      "Epoch 3616/4000: Train Loss: 2836.2299288190793 Test Loss: 2995.9156369561892\n",
      "Epoch 3617/4000: Train Loss: 2836.229604521452 Test Loss: 2995.9150094531683\n",
      "Epoch 3618/4000: Train Loss: 2836.229280564888 Test Loss: 2995.914382493338\n",
      "Epoch 3619/4000: Train Loss: 2836.2289569490276 Test Loss: 2995.9137560761887\n",
      "Epoch 3620/4000: Train Loss: 2836.2286336735147 Test Loss: 2995.913130201212\n",
      "Epoch 3621/4000: Train Loss: 2836.2283107379894 Test Loss: 2995.9125048678998\n",
      "Epoch 3622/4000: Train Loss: 2836.2279881420955 Test Loss: 2995.911880075741\n",
      "Epoch 3623/4000: Train Loss: 2836.227665885474 Test Loss: 2995.911255824231\n",
      "Epoch 3624/4000: Train Loss: 2836.2273439677692 Test Loss: 2995.910632112861\n",
      "Epoch 3625/4000: Train Loss: 2836.227022388625 Test Loss: 2995.9100089411227\n",
      "Epoch 3626/4000: Train Loss: 2836.226701147685 Test Loss: 2995.9093863085136\n",
      "Epoch 3627/4000: Train Loss: 2836.226380244593 Test Loss: 2995.9087642145223\n",
      "Epoch 3628/4000: Train Loss: 2836.2260596789947 Test Loss: 2995.90814265865\n",
      "Epoch 3629/4000: Train Loss: 2836.2257394505345 Test Loss: 2995.907521640386\n",
      "Epoch 3630/4000: Train Loss: 2836.225419558858 Test Loss: 2995.9069011592283\n",
      "Epoch 3631/4000: Train Loss: 2836.225100003611 Test Loss: 2995.906281214676\n",
      "Epoch 3632/4000: Train Loss: 2836.224780784439 Test Loss: 2995.905661806219\n",
      "Epoch 3633/4000: Train Loss: 2836.22446190099 Test Loss: 2995.9050429333606\n",
      "Epoch 3634/4000: Train Loss: 2836.22414335291 Test Loss: 2995.9044245955956\n",
      "Epoch 3635/4000: Train Loss: 2836.2238251398458 Test Loss: 2995.9038067924225\n",
      "Epoch 3636/4000: Train Loss: 2836.2235072614462 Test Loss: 2995.90318952334\n",
      "Epoch 3637/4000: Train Loss: 2836.2231897173583 Test Loss: 2995.9025727878493\n",
      "Epoch 3638/4000: Train Loss: 2836.222872507231 Test Loss: 2995.901956585445\n",
      "Epoch 3639/4000: Train Loss: 2836.222555630713 Test Loss: 2995.9013409156323\n",
      "Epoch 3640/4000: Train Loss: 2836.222239087453 Test Loss: 2995.9007257779094\n",
      "Epoch 3641/4000: Train Loss: 2836.221922877102 Test Loss: 2995.900111171779\n",
      "Epoch 3642/4000: Train Loss: 2836.221606999308 Test Loss: 2995.8994970967406\n",
      "Epoch 3643/4000: Train Loss: 2836.2212914537226 Test Loss: 2995.898883552297\n",
      "Epoch 3644/4000: Train Loss: 2836.2209762399953 Test Loss: 2995.8982705379503\n",
      "Epoch 3645/4000: Train Loss: 2836.2206613577773 Test Loss: 2995.8976580532044\n",
      "Epoch 3646/4000: Train Loss: 2836.2203468067214 Test Loss: 2995.8970460975643\n",
      "Epoch 3647/4000: Train Loss: 2836.220032586478 Test Loss: 2995.896434670532\n",
      "Epoch 3648/4000: Train Loss: 2836.2197186966987 Test Loss: 2995.8958237716106\n",
      "Epoch 3649/4000: Train Loss: 2836.219405137037 Test Loss: 2995.8952134003093\n",
      "Epoch 3650/4000: Train Loss: 2836.2190919071454 Test Loss: 2995.894603556131\n",
      "Epoch 3651/4000: Train Loss: 2836.2187790066773 Test Loss: 2995.893994238581\n",
      "Epoch 3652/4000: Train Loss: 2836.218466435286 Test Loss: 2995.893385447168\n",
      "Epoch 3653/4000: Train Loss: 2836.218154192625 Test Loss: 2995.8927771813974\n",
      "Epoch 3654/4000: Train Loss: 2836.2178422783495 Test Loss: 2995.8921694407777\n",
      "Epoch 3655/4000: Train Loss: 2836.217530692113 Test Loss: 2995.8915622248155\n",
      "Epoch 3656/4000: Train Loss: 2836.2172194335717 Test Loss: 2995.8909555330206\n",
      "Epoch 3657/4000: Train Loss: 2836.21690850238 Test Loss: 2995.8903493649013\n",
      "Epoch 3658/4000: Train Loss: 2836.2165978981943 Test Loss: 2995.889743719966\n",
      "Epoch 3659/4000: Train Loss: 2836.2162876206708 Test Loss: 2995.8891385977286\n",
      "Epoch 3660/4000: Train Loss: 2836.215977669465 Test Loss: 2995.8885339976946\n",
      "Epoch 3661/4000: Train Loss: 2836.2156680442345 Test Loss: 2995.8879299193763\n",
      "Epoch 3662/4000: Train Loss: 2836.2153587446355 Test Loss: 2995.887326362286\n",
      "Epoch 3663/4000: Train Loss: 2836.2150497703274 Test Loss: 2995.8867233259357\n",
      "Epoch 3664/4000: Train Loss: 2836.214741120966 Test Loss: 2995.886120809837\n",
      "Epoch 3665/4000: Train Loss: 2836.2144327962114 Test Loss: 2995.885518813502\n",
      "Epoch 3666/4000: Train Loss: 2836.2141247957206 Test Loss: 2995.8849173364456\n",
      "Epoch 3667/4000: Train Loss: 2836.2138171191536 Test Loss: 2995.884316378181\n",
      "Epoch 3668/4000: Train Loss: 2836.21350976617 Test Loss: 2995.8837159382224\n",
      "Epoch 3669/4000: Train Loss: 2836.2132027364282 Test Loss: 2995.883116016085\n",
      "Epoch 3670/4000: Train Loss: 2836.2128960295895 Test Loss: 2995.8825166112815\n",
      "Epoch 3671/4000: Train Loss: 2836.2125896453135 Test Loss: 2995.881917723329\n",
      "Epoch 3672/4000: Train Loss: 2836.212283583261 Test Loss: 2995.8813193517453\n",
      "Epoch 3673/4000: Train Loss: 2836.211977843094 Test Loss: 2995.880721496045\n",
      "Epoch 3674/4000: Train Loss: 2836.211672424473 Test Loss: 2995.8801241557476\n",
      "Epoch 3675/4000: Train Loss: 2836.2113673270596 Test Loss: 2995.8795273303667\n",
      "Epoch 3676/4000: Train Loss: 2836.2110625505175 Test Loss: 2995.8789310194243\n",
      "Epoch 3677/4000: Train Loss: 2836.2107580945076 Test Loss: 2995.8783352224373\n",
      "Epoch 3678/4000: Train Loss: 2836.210453958694 Test Loss: 2995.877739938924\n",
      "Epoch 3679/4000: Train Loss: 2836.210150142739 Test Loss: 2995.877145168405\n",
      "Epoch 3680/4000: Train Loss: 2836.2098466463067 Test Loss: 2995.8765509104\n",
      "Epoch 3681/4000: Train Loss: 2836.209543469062 Test Loss: 2995.8759571644277\n",
      "Epoch 3682/4000: Train Loss: 2836.209240610667 Test Loss: 2995.8753639300144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3683/4000: Train Loss: 2836.2089380707876 Test Loss: 2995.8747712066743\n",
      "Epoch 3684/4000: Train Loss: 2836.208635849089 Test Loss: 2995.874178993934\n",
      "Epoch 3685/4000: Train Loss: 2836.2083339452365 Test Loss: 2995.8735872913144\n",
      "Epoch 3686/4000: Train Loss: 2836.2080323588957 Test Loss: 2995.872996098338\n",
      "Epoch 3687/4000: Train Loss: 2836.2077310897325 Test Loss: 2995.8724054145287\n",
      "Epoch 3688/4000: Train Loss: 2836.2074301374137 Test Loss: 2995.871815239409\n",
      "Epoch 3689/4000: Train Loss: 2836.2071295016062 Test Loss: 2995.871225572505\n",
      "Epoch 3690/4000: Train Loss: 2836.2068291819764 Test Loss: 2995.8706364133404\n",
      "Epoch 3691/4000: Train Loss: 2836.206529178192 Test Loss: 2995.8700477614393\n",
      "Epoch 3692/4000: Train Loss: 2836.206229489921 Test Loss: 2995.8694596163296\n",
      "Epoch 3693/4000: Train Loss: 2836.205930116832 Test Loss: 2995.868871977536\n",
      "Epoch 3694/4000: Train Loss: 2836.2056310585926 Test Loss: 2995.8682848445847\n",
      "Epoch 3695/4000: Train Loss: 2836.2053323148725 Test Loss: 2995.867698217003\n",
      "Epoch 3696/4000: Train Loss: 2836.20503388534 Test Loss: 2995.867112094318\n",
      "Epoch 3697/4000: Train Loss: 2836.204735769666 Test Loss: 2995.8665264760593\n",
      "Epoch 3698/4000: Train Loss: 2836.2044379675194 Test Loss: 2995.8659413617547\n",
      "Epoch 3699/4000: Train Loss: 2836.2041404785705 Test Loss: 2995.8653567509323\n",
      "Epoch 3700/4000: Train Loss: 2836.20384330249 Test Loss: 2995.86477264312\n",
      "Epoch 3701/4000: Train Loss: 2836.2035464389496 Test Loss: 2995.8641890378503\n",
      "Epoch 3702/4000: Train Loss: 2836.2032498876197 Test Loss: 2995.863605934653\n",
      "Epoch 3703/4000: Train Loss: 2836.2029536481723 Test Loss: 2995.8630233330587\n",
      "Epoch 3704/4000: Train Loss: 2836.2026577202796 Test Loss: 2995.8624412325967\n",
      "Epoch 3705/4000: Train Loss: 2836.2023621036137 Test Loss: 2995.8618596328024\n",
      "Epoch 3706/4000: Train Loss: 2836.202066797847 Test Loss: 2995.8612785332034\n",
      "Epoch 3707/4000: Train Loss: 2836.2017718026523 Test Loss: 2995.860697933336\n",
      "Epoch 3708/4000: Train Loss: 2836.201477117704 Test Loss: 2995.8601178327294\n",
      "Epoch 3709/4000: Train Loss: 2836.2011827426754 Test Loss: 2995.8595382309218\n",
      "Epoch 3710/4000: Train Loss: 2836.2008886772405 Test Loss: 2995.858959127445\n",
      "Epoch 3711/4000: Train Loss: 2836.200594921073 Test Loss: 2995.8583805218327\n",
      "Epoch 3712/4000: Train Loss: 2836.200301473849 Test Loss: 2995.857802413619\n",
      "Epoch 3713/4000: Train Loss: 2836.2000083352427 Test Loss: 2995.8572248023424\n",
      "Epoch 3714/4000: Train Loss: 2836.1997155049294 Test Loss: 2995.8566476875358\n",
      "Epoch 3715/4000: Train Loss: 2836.199422982586 Test Loss: 2995.8560710687366\n",
      "Epoch 3716/4000: Train Loss: 2836.1991307678863 Test Loss: 2995.8554949454815\n",
      "Epoch 3717/4000: Train Loss: 2836.198838860509 Test Loss: 2995.8549193173085\n",
      "Epoch 3718/4000: Train Loss: 2836.1985472601305 Test Loss: 2995.8543441837514\n",
      "Epoch 3719/4000: Train Loss: 2836.198255966427 Test Loss: 2995.8537695443533\n",
      "Epoch 3720/4000: Train Loss: 2836.197964979077 Test Loss: 2995.8531953986508\n",
      "Epoch 3721/4000: Train Loss: 2836.197674297757 Test Loss: 2995.852621746181\n",
      "Epoch 3722/4000: Train Loss: 2836.197383922147 Test Loss: 2995.852048586487\n",
      "Epoch 3723/4000: Train Loss: 2836.197093851924 Test Loss: 2995.851475919106\n",
      "Epoch 3724/4000: Train Loss: 2836.196804086767 Test Loss: 2995.850903743579\n",
      "Epoch 3725/4000: Train Loss: 2836.1965146263556 Test Loss: 2995.850332059446\n",
      "Epoch 3726/4000: Train Loss: 2836.1962254703694 Test Loss: 2995.8497608662506\n",
      "Epoch 3727/4000: Train Loss: 2836.1959366184883 Test Loss: 2995.8491901635302\n",
      "Epoch 3728/4000: Train Loss: 2836.1956480703916 Test Loss: 2995.8486199508325\n",
      "Epoch 3729/4000: Train Loss: 2836.19535982576 Test Loss: 2995.8480502276966\n",
      "Epoch 3730/4000: Train Loss: 2836.195071884275 Test Loss: 2995.847480993667\n",
      "Epoch 3731/4000: Train Loss: 2836.1947842456184 Test Loss: 2995.846912248285\n",
      "Epoch 3732/4000: Train Loss: 2836.1944969094693 Test Loss: 2995.8463439910956\n",
      "Epoch 3733/4000: Train Loss: 2836.194209875512 Test Loss: 2995.8457762216444\n",
      "Epoch 3734/4000: Train Loss: 2836.193923143428 Test Loss: 2995.845208939477\n",
      "Epoch 3735/4000: Train Loss: 2836.1936367128997 Test Loss: 2995.844642144136\n",
      "Epoch 3736/4000: Train Loss: 2836.193350583609 Test Loss: 2995.8440758351685\n",
      "Epoch 3737/4000: Train Loss: 2836.19306475524 Test Loss: 2995.843510012119\n",
      "Epoch 3738/4000: Train Loss: 2836.192779227476 Test Loss: 2995.8429446745376\n",
      "Epoch 3739/4000: Train Loss: 2836.1924940000017 Test Loss: 2995.842379821969\n",
      "Epoch 3740/4000: Train Loss: 2836.1922090725 Test Loss: 2995.8418154539604\n",
      "Epoch 3741/4000: Train Loss: 2836.1919244446567 Test Loss: 2995.8412515700616\n",
      "Epoch 3742/4000: Train Loss: 2836.1916401161557 Test Loss: 2995.84068816982\n",
      "Epoch 3743/4000: Train Loss: 2836.191356086682 Test Loss: 2995.8401252527838\n",
      "Epoch 3744/4000: Train Loss: 2836.1910723559213 Test Loss: 2995.8395628185035\n",
      "Epoch 3745/4000: Train Loss: 2836.1907889235604 Test Loss: 2995.839000866528\n",
      "Epoch 3746/4000: Train Loss: 2836.190505789284 Test Loss: 2995.838439396407\n",
      "Epoch 3747/4000: Train Loss: 2836.19022295278 Test Loss: 2995.8378784076945\n",
      "Epoch 3748/4000: Train Loss: 2836.1899404137343 Test Loss: 2995.8373178999373\n",
      "Epoch 3749/4000: Train Loss: 2836.1896581718347 Test Loss: 2995.8367578726898\n",
      "Epoch 3750/4000: Train Loss: 2836.189376226768 Test Loss: 2995.8361983255004\n",
      "Epoch 3751/4000: Train Loss: 2836.1890945782225 Test Loss: 2995.8356392579276\n",
      "Epoch 3752/4000: Train Loss: 2836.188813225887 Test Loss: 2995.8350806695175\n",
      "Epoch 3753/4000: Train Loss: 2836.188532169448 Test Loss: 2995.834522559828\n",
      "Epoch 3754/4000: Train Loss: 2836.188251408596 Test Loss: 2995.8339649284108\n",
      "Epoch 3755/4000: Train Loss: 2836.1879709430195 Test Loss: 2995.833407774822\n",
      "Epoch 3756/4000: Train Loss: 2836.187690772408 Test Loss: 2995.8328510986134\n",
      "Epoch 3757/4000: Train Loss: 2836.187410896452 Test Loss: 2995.8322948993423\n",
      "Epoch 3758/4000: Train Loss: 2836.1871313148404 Test Loss: 2995.831739176562\n",
      "Epoch 3759/4000: Train Loss: 2836.186852027264 Test Loss: 2995.8311839298317\n",
      "Epoch 3760/4000: Train Loss: 2836.1865730334143 Test Loss: 2995.8306291587046\n",
      "Epoch 3761/4000: Train Loss: 2836.186294332982 Test Loss: 2995.8300748627394\n",
      "Epoch 3762/4000: Train Loss: 2836.186015925658 Test Loss: 2995.829521041493\n",
      "Epoch 3763/4000: Train Loss: 2836.1857378111345 Test Loss: 2995.828967694522\n",
      "Epoch 3764/4000: Train Loss: 2836.185459989104 Test Loss: 2995.8284148213856\n",
      "Epoch 3765/4000: Train Loss: 2836.1851824592577 Test Loss: 2995.8278624216423\n",
      "Epoch 3766/4000: Train Loss: 2836.1849052212892 Test Loss: 2995.8273104948503\n",
      "Epoch 3767/4000: Train Loss: 2836.1846282748916 Test Loss: 2995.8267590405703\n",
      "Epoch 3768/4000: Train Loss: 2836.1843516197578 Test Loss: 2995.82620805836\n",
      "Epoch 3769/4000: Train Loss: 2836.1840752555822 Test Loss: 2995.8256575477812\n",
      "Epoch 3770/4000: Train Loss: 2836.1837991820576 Test Loss: 2995.8251075083963\n",
      "Epoch 3771/4000: Train Loss: 2836.183523398879 Test Loss: 2995.8245579397626\n",
      "Epoch 3772/4000: Train Loss: 2836.1832479057407 Test Loss: 2995.8240088414436\n",
      "Epoch 3773/4000: Train Loss: 2836.1829727023382 Test Loss: 2995.8234602130024\n",
      "Epoch 3774/4000: Train Loss: 2836.182697788367 Test Loss: 2995.822912053999\n",
      "Epoch 3775/4000: Train Loss: 2836.1824231635223 Test Loss: 2995.8223643639985\n",
      "Epoch 3776/4000: Train Loss: 2836.1821488274995 Test Loss: 2995.821817142564\n",
      "Epoch 3777/4000: Train Loss: 2836.1818747799957 Test Loss: 2995.8212703892577\n",
      "Epoch 3778/4000: Train Loss: 2836.181601020707 Test Loss: 2995.8207241036457\n",
      "Epoch 3779/4000: Train Loss: 2836.1813275493305 Test Loss: 2995.8201782852893\n",
      "Epoch 3780/4000: Train Loss: 2836.181054365564 Test Loss: 2995.819632933758\n",
      "Epoch 3781/4000: Train Loss: 2836.1807814691033 Test Loss: 2995.819088048613\n",
      "Epoch 3782/4000: Train Loss: 2836.1805088596475 Test Loss: 2995.8185436294234\n",
      "Epoch 3783/4000: Train Loss: 2836.180236536895 Test Loss: 2995.8179996757526\n",
      "Epoch 3784/4000: Train Loss: 2836.1799645005435 Test Loss: 2995.817456187171\n",
      "Epoch 3785/4000: Train Loss: 2836.1796927502915 Test Loss: 2995.8169131632417\n",
      "Epoch 3786/4000: Train Loss: 2836.179421285839 Test Loss: 2995.8163706035352\n",
      "Epoch 3787/4000: Train Loss: 2836.1791501068856 Test Loss: 2995.8158285076183\n",
      "Epoch 3788/4000: Train Loss: 2836.1788792131306 Test Loss: 2995.81528687506\n",
      "Epoch 3789/4000: Train Loss: 2836.1786086042734 Test Loss: 2995.814745705428\n",
      "Epoch 3790/4000: Train Loss: 2836.178338280016 Test Loss: 2995.8142049982935\n",
      "Epoch 3791/4000: Train Loss: 2836.178068240057 Test Loss: 2995.8136647532233\n",
      "Epoch 3792/4000: Train Loss: 2836.1777984840987 Test Loss: 2995.813124969791\n",
      "Epoch 3793/4000: Train Loss: 2836.177529011843 Test Loss: 2995.812585647564\n",
      "Epoch 3794/4000: Train Loss: 2836.17725982299 Test Loss: 2995.812046786116\n",
      "Epoch 3795/4000: Train Loss: 2836.1769909172426 Test Loss: 2995.8115083850157\n",
      "Epoch 3796/4000: Train Loss: 2836.176722294303 Test Loss: 2995.810970443837\n",
      "Epoch 3797/4000: Train Loss: 2836.1764539538735 Test Loss: 2995.8104329621497\n",
      "Epoch 3798/4000: Train Loss: 2836.176185895657 Test Loss: 2995.80989593953\n",
      "Epoch 3799/4000: Train Loss: 2836.175918119357 Test Loss: 2995.8093593755484\n",
      "Epoch 3800/4000: Train Loss: 2836.1756506246766 Test Loss: 2995.808823269779\n",
      "Epoch 3801/4000: Train Loss: 2836.17538341132 Test Loss: 2995.8082876217964\n",
      "Epoch 3802/4000: Train Loss: 2836.1751164789907 Test Loss: 2995.807752431172\n",
      "Epoch 3803/4000: Train Loss: 2836.1748498273946 Test Loss: 2995.807217697485\n",
      "Epoch 3804/4000: Train Loss: 2836.1745834562344 Test Loss: 2995.8066834203078\n",
      "Epoch 3805/4000: Train Loss: 2836.1743173652167 Test Loss: 2995.806149599217\n",
      "Epoch 3806/4000: Train Loss: 2836.1740515540464 Test Loss: 2995.8056162337866\n",
      "Epoch 3807/4000: Train Loss: 2836.1737860224293 Test Loss: 2995.805083323595\n",
      "Epoch 3808/4000: Train Loss: 2836.173520770071 Test Loss: 2995.804550868219\n",
      "Epoch 3809/4000: Train Loss: 2836.1732557966784 Test Loss: 2995.8040188672353\n",
      "Epoch 3810/4000: Train Loss: 2836.172991101958 Test Loss: 2995.803487320222\n",
      "Epoch 3811/4000: Train Loss: 2836.172726685616 Test Loss: 2995.802956226757\n",
      "Epoch 3812/4000: Train Loss: 2836.1724625473603 Test Loss: 2995.8024255864166\n",
      "Epoch 3813/4000: Train Loss: 2836.172198686898 Test Loss: 2995.801895398784\n",
      "Epoch 3814/4000: Train Loss: 2836.171935103938 Test Loss: 2995.8013656634344\n",
      "Epoch 3815/4000: Train Loss: 2836.1716717981867 Test Loss: 2995.80083637995\n",
      "Epoch 3816/4000: Train Loss: 2836.171408769354 Test Loss: 2995.800307547912\n",
      "Epoch 3817/4000: Train Loss: 2836.171146017149 Test Loss: 2995.799779166896\n",
      "Epoch 3818/4000: Train Loss: 2836.1708835412787 Test Loss: 2995.7992512364876\n",
      "Epoch 3819/4000: Train Loss: 2836.1706213414545 Test Loss: 2995.798723756267\n",
      "Epoch 3820/4000: Train Loss: 2836.1703594173846 Test Loss: 2995.7981967258156\n",
      "Epoch 3821/4000: Train Loss: 2836.1700977687806 Test Loss: 2995.7976701447155\n",
      "Epoch 3822/4000: Train Loss: 2836.1698363953515 Test Loss: 2995.79714401255\n",
      "Epoch 3823/4000: Train Loss: 2836.169575296809 Test Loss: 2995.7966183289013\n",
      "Epoch 3824/4000: Train Loss: 2836.1693144728624 Test Loss: 2995.796093093353\n",
      "Epoch 3825/4000: Train Loss: 2836.169053923224 Test Loss: 2995.7955683054893\n",
      "Epoch 3826/4000: Train Loss: 2836.1687936476055 Test Loss: 2995.7950439648944\n",
      "Epoch 3827/4000: Train Loss: 2836.1685336457176 Test Loss: 2995.7945200711533\n",
      "Epoch 3828/4000: Train Loss: 2836.1682739172743 Test Loss: 2995.7939966238496\n",
      "Epoch 3829/4000: Train Loss: 2836.168014461986 Test Loss: 2995.7934736225698\n",
      "Epoch 3830/4000: Train Loss: 2836.167755279567 Test Loss: 2995.7929510668996\n",
      "Epoch 3831/4000: Train Loss: 2836.1674963697287 Test Loss: 2995.792428956426\n",
      "Epoch 3832/4000: Train Loss: 2836.167237732186 Test Loss: 2995.7919072907357\n",
      "Epoch 3833/4000: Train Loss: 2836.166979366652 Test Loss: 2995.7913860694143\n",
      "Epoch 3834/4000: Train Loss: 2836.1667212728403 Test Loss: 2995.7908652920496\n",
      "Epoch 3835/4000: Train Loss: 2836.166463450466 Test Loss: 2995.7903449582304\n",
      "Epoch 3836/4000: Train Loss: 2836.166205899242 Test Loss: 2995.789825067546\n",
      "Epoch 3837/4000: Train Loss: 2836.165948618885 Test Loss: 2995.7893056195835\n",
      "Epoch 3838/4000: Train Loss: 2836.1656916091083 Test Loss: 2995.788786613931\n",
      "Epoch 3839/4000: Train Loss: 2836.1654348696293 Test Loss: 2995.78826805018\n",
      "Epoch 3840/4000: Train Loss: 2836.165178400162 Test Loss: 2995.78774992792\n",
      "Epoch 3841/4000: Train Loss: 2836.1649222004235 Test Loss: 2995.7872322467415\n",
      "Epoch 3842/4000: Train Loss: 2836.164666270129 Test Loss: 2995.7867150062343\n",
      "Epoch 3843/4000: Train Loss: 2836.1644106089966 Test Loss: 2995.786198205989\n",
      "Epoch 3844/4000: Train Loss: 2836.1641552167425 Test Loss: 2995.7856818456\n",
      "Epoch 3845/4000: Train Loss: 2836.163900093084 Test Loss: 2995.785165924656\n",
      "Epoch 3846/4000: Train Loss: 2836.163645237738 Test Loss: 2995.784650442751\n",
      "Epoch 3847/4000: Train Loss: 2836.163390650423 Test Loss: 2995.784135399478\n",
      "Epoch 3848/4000: Train Loss: 2836.1631363308575 Test Loss: 2995.783620794429\n",
      "Epoch 3849/4000: Train Loss: 2836.1628822787593 Test Loss: 2995.7831066271983\n",
      "Epoch 3850/4000: Train Loss: 2836.162628493847 Test Loss: 2995.7825928973807\n",
      "Epoch 3851/4000: Train Loss: 2836.16237497584 Test Loss: 2995.7820796045685\n",
      "Epoch 3852/4000: Train Loss: 2836.162121724457 Test Loss: 2995.7815667483574\n",
      "Epoch 3853/4000: Train Loss: 2836.1618687394184 Test Loss: 2995.781054328342\n",
      "Epoch 3854/4000: Train Loss: 2836.161616020443 Test Loss: 2995.7805423441187\n",
      "Epoch 3855/4000: Train Loss: 2836.1613635672516 Test Loss: 2995.7800307952843\n",
      "Epoch 3856/4000: Train Loss: 2836.1611113795657 Test Loss: 2995.7795196814327\n",
      "Epoch 3857/4000: Train Loss: 2836.160859457105 Test Loss: 2995.7790090021626\n",
      "Epoch 3858/4000: Train Loss: 2836.1606077995903 Test Loss: 2995.77849875707\n",
      "Epoch 3859/4000: Train Loss: 2836.1603564067427 Test Loss: 2995.7779889457524\n",
      "Epoch 3860/4000: Train Loss: 2836.160105278285 Test Loss: 2995.7774795678083\n",
      "Epoch 3861/4000: Train Loss: 2836.159854413939 Test Loss: 2995.776970622835\n",
      "Epoch 3862/4000: Train Loss: 2836.159603813426 Test Loss: 2995.7764621104334\n",
      "Epoch 3863/4000: Train Loss: 2836.1593534764693 Test Loss: 2995.7759540301995\n",
      "Epoch 3864/4000: Train Loss: 2836.1591034027915 Test Loss: 2995.775446381735\n",
      "Epoch 3865/4000: Train Loss: 2836.1588535921155 Test Loss: 2995.774939164639\n",
      "Epoch 3866/4000: Train Loss: 2836.1586040441653 Test Loss: 2995.7744323785128\n",
      "Epoch 3867/4000: Train Loss: 2836.158354758664 Test Loss: 2995.773926022955\n",
      "Epoch 3868/4000: Train Loss: 2836.1581057353355 Test Loss: 2995.773420097568\n",
      "Epoch 3869/4000: Train Loss: 2836.1578569739045 Test Loss: 2995.772914601952\n",
      "Epoch 3870/4000: Train Loss: 2836.1576084740955 Test Loss: 2995.7724095357107\n",
      "Epoch 3871/4000: Train Loss: 2836.1573602356334 Test Loss: 2995.771904898447\n",
      "Epoch 3872/4000: Train Loss: 2836.157112258243 Test Loss: 2995.77140068976\n",
      "Epoch 3873/4000: Train Loss: 2836.15686454165 Test Loss: 2995.7708969092546\n",
      "Epoch 3874/4000: Train Loss: 2836.15661708558 Test Loss: 2995.7703935565346\n",
      "Epoch 3875/4000: Train Loss: 2836.1563698897594 Test Loss: 2995.7698906312035\n",
      "Epoch 3876/4000: Train Loss: 2836.156122953913 Test Loss: 2995.7693881328655\n",
      "Epoch 3877/4000: Train Loss: 2836.1558762777695 Test Loss: 2995.7688860611247\n",
      "Epoch 3878/4000: Train Loss: 2836.1556298610553 Test Loss: 2995.768384415586\n",
      "Epoch 3879/4000: Train Loss: 2836.1553837034958 Test Loss: 2995.7678831958547\n",
      "Epoch 3880/4000: Train Loss: 2836.1551378048207 Test Loss: 2995.7673824015383\n",
      "Epoch 3881/4000: Train Loss: 2836.1548921647563 Test Loss: 2995.7668820322406\n",
      "Epoch 3882/4000: Train Loss: 2836.1546467830312 Test Loss: 2995.7663820875687\n",
      "Epoch 3883/4000: Train Loss: 2836.154401659374 Test Loss: 2995.76588256713\n",
      "Epoch 3884/4000: Train Loss: 2836.154156793512 Test Loss: 2995.7653834705316\n",
      "Epoch 3885/4000: Train Loss: 2836.1539121851756 Test Loss: 2995.76488479738\n",
      "Epoch 3886/4000: Train Loss: 2836.153667834093 Test Loss: 2995.7643865472855\n",
      "Epoch 3887/4000: Train Loss: 2836.1534237399937 Test Loss: 2995.763888719854\n",
      "Epoch 3888/4000: Train Loss: 2836.153179902609 Test Loss: 2995.7633913146974\n",
      "Epoch 3889/4000: Train Loss: 2836.1529363216664 Test Loss: 2995.7628943314216\n",
      "Epoch 3890/4000: Train Loss: 2836.1526929968977 Test Loss: 2995.762397769637\n",
      "Epoch 3891/4000: Train Loss: 2836.1524499280326 Test Loss: 2995.761901628954\n",
      "Epoch 3892/4000: Train Loss: 2836.152207114804 Test Loss: 2995.761405908984\n",
      "Epoch 3893/4000: Train Loss: 2836.15196455694 Test Loss: 2995.7609106093373\n",
      "Epoch 3894/4000: Train Loss: 2836.1517222541747 Test Loss: 2995.7604157296228\n",
      "Epoch 3895/4000: Train Loss: 2836.1514802062384 Test Loss: 2995.759921269454\n",
      "Epoch 3896/4000: Train Loss: 2836.1512384128628 Test Loss: 2995.7594272284437\n",
      "Epoch 3897/4000: Train Loss: 2836.150996873782 Test Loss: 2995.7589336062015\n",
      "Epoch 3898/4000: Train Loss: 2836.1507555887265 Test Loss: 2995.7584404023405\n",
      "Epoch 3899/4000: Train Loss: 2836.1505145574306 Test Loss: 2995.7579476164756\n",
      "Epoch 3900/4000: Train Loss: 2836.150273779627 Test Loss: 2995.757455248219\n",
      "Epoch 3901/4000: Train Loss: 2836.150033255048 Test Loss: 2995.7569632971836\n",
      "Epoch 3902/4000: Train Loss: 2836.1497929834286 Test Loss: 2995.7564717629853\n",
      "Epoch 3903/4000: Train Loss: 2836.1495529645026 Test Loss: 2995.755980645236\n",
      "Epoch 3904/4000: Train Loss: 2836.149313198004 Test Loss: 2995.755489943555\n",
      "Epoch 3905/4000: Train Loss: 2836.149073683667 Test Loss: 2995.754999657552\n",
      "Epoch 3906/4000: Train Loss: 2836.1488344212275 Test Loss: 2995.754509786847\n",
      "Epoch 3907/4000: Train Loss: 2836.1485954104187 Test Loss: 2995.754020331054\n",
      "Epoch 3908/4000: Train Loss: 2836.1483566509783 Test Loss: 2995.7535312897903\n",
      "Epoch 3909/4000: Train Loss: 2836.14811814264 Test Loss: 2995.753042662671\n",
      "Epoch 3910/4000: Train Loss: 2836.147879885141 Test Loss: 2995.7525544493155\n",
      "Epoch 3911/4000: Train Loss: 2836.147641878217 Test Loss: 2995.7520666493397\n",
      "Epoch 3912/4000: Train Loss: 2836.147404121604 Test Loss: 2995.7515792623635\n",
      "Epoch 3913/4000: Train Loss: 2836.147166615039 Test Loss: 2995.7510922880006\n",
      "Epoch 3914/4000: Train Loss: 2836.1469293582595 Test Loss: 2995.7506057258747\n",
      "Epoch 3915/4000: Train Loss: 2836.146692351003 Test Loss: 2995.7501195756026\n",
      "Epoch 3916/4000: Train Loss: 2836.1464555930056 Test Loss: 2995.749633836804\n",
      "Epoch 3917/4000: Train Loss: 2836.146219084006 Test Loss: 2995.749148509097\n",
      "Epoch 3918/4000: Train Loss: 2836.1459828237435 Test Loss: 2995.748663592107\n",
      "Epoch 3919/4000: Train Loss: 2836.1457468119543 Test Loss: 2995.7481790854486\n",
      "Epoch 3920/4000: Train Loss: 2836.145511048379 Test Loss: 2995.747694988743\n",
      "Epoch 3921/4000: Train Loss: 2836.1452755327555 Test Loss: 2995.7472113016147\n",
      "Epoch 3922/4000: Train Loss: 2836.145040264824 Test Loss: 2995.746728023685\n",
      "Epoch 3923/4000: Train Loss: 2836.144805244322 Test Loss: 2995.746245154573\n",
      "Epoch 3924/4000: Train Loss: 2836.1445704709918 Test Loss: 2995.745762693904\n",
      "Epoch 3925/4000: Train Loss: 2836.1443359445716 Test Loss: 2995.7452806412994\n",
      "Epoch 3926/4000: Train Loss: 2836.1441016648027 Test Loss: 2995.7447989963816\n",
      "Epoch 3927/4000: Train Loss: 2836.1438676314256 Test Loss: 2995.744317758776\n",
      "Epoch 3928/4000: Train Loss: 2836.1436338441804 Test Loss: 2995.743836928105\n",
      "Epoch 3929/4000: Train Loss: 2836.143400302809 Test Loss: 2995.7433565039923\n",
      "Epoch 3930/4000: Train Loss: 2836.1431670070524 Test Loss: 2995.7428764860647\n",
      "Epoch 3931/4000: Train Loss: 2836.1429339566525 Test Loss: 2995.742396873945\n",
      "Epoch 3932/4000: Train Loss: 2836.1427011513515 Test Loss: 2995.7419176672597\n",
      "Epoch 3933/4000: Train Loss: 2836.1424685908914 Test Loss: 2995.741438865633\n",
      "Epoch 3934/4000: Train Loss: 2836.1422362750145 Test Loss: 2995.740960468692\n",
      "Epoch 3935/4000: Train Loss: 2836.142004203464 Test Loss: 2995.740482476062\n",
      "Epoch 3936/4000: Train Loss: 2836.141772375983 Test Loss: 2995.7400048873733\n",
      "Epoch 3937/4000: Train Loss: 2836.141540792314 Test Loss: 2995.7395277022497\n",
      "Epoch 3938/4000: Train Loss: 2836.141309452201 Test Loss: 2995.7390509203187\n",
      "Epoch 3939/4000: Train Loss: 2836.141078355388 Test Loss: 2995.738574541209\n",
      "Epoch 3940/4000: Train Loss: 2836.1408475016187 Test Loss: 2995.7380985645486\n",
      "Epoch 3941/4000: Train Loss: 2836.1406168906387 Test Loss: 2995.737622989968\n",
      "Epoch 3942/4000: Train Loss: 2836.140386522191 Test Loss: 2995.7371478170917\n",
      "Epoch 3943/4000: Train Loss: 2836.140156396022 Test Loss: 2995.736673045553\n",
      "Epoch 3944/4000: Train Loss: 2836.1399265118757 Test Loss: 2995.7361986749806\n",
      "Epoch 3945/4000: Train Loss: 2836.139696869499 Test Loss: 2995.7357247050036\n",
      "Epoch 3946/4000: Train Loss: 2836.139467468636 Test Loss: 2995.735251135254\n",
      "Epoch 3947/4000: Train Loss: 2836.1392383090338 Test Loss: 2995.7347779653605\n",
      "Epoch 3948/4000: Train Loss: 2836.139009390438 Test Loss: 2995.7343051949556\n",
      "Epoch 3949/4000: Train Loss: 2836.1387807125952 Test Loss: 2995.733832823671\n",
      "Epoch 3950/4000: Train Loss: 2836.1385522752526 Test Loss: 2995.7333608511367\n",
      "Epoch 3951/4000: Train Loss: 2836.138324078158 Test Loss: 2995.7328892769883\n",
      "Epoch 3952/4000: Train Loss: 2836.1380961210566 Test Loss: 2995.732418100854\n",
      "Epoch 3953/4000: Train Loss: 2836.137868403698 Test Loss: 2995.7319473223706\n",
      "Epoch 3954/4000: Train Loss: 2836.137640925829 Test Loss: 2995.7314769411687\n",
      "Epoch 3955/4000: Train Loss: 2836.137413687198 Test Loss: 2995.7310069568844\n",
      "Epoch 3956/4000: Train Loss: 2836.137186687554 Test Loss: 2995.7305373691493\n",
      "Epoch 3957/4000: Train Loss: 2836.136959926644 Test Loss: 2995.7300681775982\n",
      "Epoch 3958/4000: Train Loss: 2836.136733404219 Test Loss: 2995.729599381868\n",
      "Epoch 3959/4000: Train Loss: 2836.136507120026 Test Loss: 2995.7291309815914\n",
      "Epoch 3960/4000: Train Loss: 2836.136281073817 Test Loss: 2995.7286629764026\n",
      "Epoch 3961/4000: Train Loss: 2836.1360552653396 Test Loss: 2995.7281953659417\n",
      "Epoch 3962/4000: Train Loss: 2836.135829694345 Test Loss: 2995.727728149841\n",
      "Epoch 3963/4000: Train Loss: 2836.1356043605824 Test Loss: 2995.72726132774\n",
      "Epoch 3964/4000: Train Loss: 2836.1353792638033 Test Loss: 2995.726794899274\n",
      "Epoch 3965/4000: Train Loss: 2836.135154403758 Test Loss: 2995.7263288640793\n",
      "Epoch 3966/4000: Train Loss: 2836.1349297801976 Test Loss: 2995.725863221794\n",
      "Epoch 3967/4000: Train Loss: 2836.134705392873 Test Loss: 2995.725397972057\n",
      "Epoch 3968/4000: Train Loss: 2836.1344812415364 Test Loss: 2995.7249331145063\n",
      "Epoch 3969/4000: Train Loss: 2836.134257325939 Test Loss: 2995.7244686487807\n",
      "Epoch 3970/4000: Train Loss: 2836.134033645834 Test Loss: 2995.724004574518\n",
      "Epoch 3971/4000: Train Loss: 2836.1338102009718 Test Loss: 2995.723540891359\n",
      "Epoch 3972/4000: Train Loss: 2836.1335869911068 Test Loss: 2995.7230775989415\n",
      "Epoch 3973/4000: Train Loss: 2836.133364015991 Test Loss: 2995.722614696906\n",
      "Epoch 3974/4000: Train Loss: 2836.133141275378 Test Loss: 2995.722152184896\n",
      "Epoch 3975/4000: Train Loss: 2836.1329187690203 Test Loss: 2995.721690062547\n",
      "Epoch 3976/4000: Train Loss: 2836.132696496673 Test Loss: 2995.721228329505\n",
      "Epoch 3977/4000: Train Loss: 2836.1324744580884 Test Loss: 2995.7207669854074\n",
      "Epoch 3978/4000: Train Loss: 2836.1322526530216 Test Loss: 2995.720306029898\n",
      "Epoch 3979/4000: Train Loss: 2836.1320310812266 Test Loss: 2995.71984546262\n",
      "Epoch 3980/4000: Train Loss: 2836.131809742458 Test Loss: 2995.719385283214\n",
      "Epoch 3981/4000: Train Loss: 2836.1315886364714 Test Loss: 2995.7189254913237\n",
      "Epoch 3982/4000: Train Loss: 2836.131367763021 Test Loss: 2995.7184660865914\n",
      "Epoch 3983/4000: Train Loss: 2836.1311471218633 Test Loss: 2995.7180070686622\n",
      "Epoch 3984/4000: Train Loss: 2836.1309267127535 Test Loss: 2995.717548437179\n",
      "Epoch 3985/4000: Train Loss: 2836.1307065354467 Test Loss: 2995.7170901917852\n",
      "Epoch 3986/4000: Train Loss: 2836.130486589701 Test Loss: 2995.7166323321276\n",
      "Epoch 3987/4000: Train Loss: 2836.130266875271 Test Loss: 2995.716174857849\n",
      "Epoch 3988/4000: Train Loss: 2836.1300473919146 Test Loss: 2995.715717768595\n",
      "Epoch 3989/4000: Train Loss: 2836.129828139389 Test Loss: 2995.715261064012\n",
      "Epoch 3990/4000: Train Loss: 2836.12960911745 Test Loss: 2995.7148047437463\n",
      "Epoch 3991/4000: Train Loss: 2836.129390325856 Test Loss: 2995.7143488074435\n",
      "Epoch 3992/4000: Train Loss: 2836.129171764365 Test Loss: 2995.7138932547505\n",
      "Epoch 3993/4000: Train Loss: 2836.1289534327343 Test Loss: 2995.7134380853126\n",
      "Epoch 3994/4000: Train Loss: 2836.128735330723 Test Loss: 2995.7129832987807\n",
      "Epoch 3995/4000: Train Loss: 2836.128517458089 Test Loss: 2995.7125288947996\n",
      "Epoch 3996/4000: Train Loss: 2836.128299814591 Test Loss: 2995.712074873018\n",
      "Epoch 3997/4000: Train Loss: 2836.1280823999887 Test Loss: 2995.7116212330848\n",
      "Epoch 3998/4000: Train Loss: 2836.127865214041 Test Loss: 2995.7111679746495\n",
      "Epoch 3999/4000: Train Loss: 2836.1276482565067 Test Loss: 2995.710715097358\n",
      "Epoch 4000/4000: Train Loss: 2836.1274315271467 Test Loss: 2995.7102626008627\n"
     ]
    }
   ],
   "source": [
    "fit(4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eb4767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
