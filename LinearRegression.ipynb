{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e8177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.value import Value\n",
    "import src.function as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e23983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_diabetes(return_X_y = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = Value(X_train, \"x_train\", requires_grad=False)\n",
    "X_test = Value(X_test, \"x_test\", requires_grad=False)\n",
    "y_train = Value(y_train, \"y_train\", requires_grad=False)\n",
    "y_test = Value(y_test, \"y_test\", requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e18c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Value(np.random.randn(X.shape[1]) * 10**(-1), \"w\", requires_grad = True)\n",
    "b = Value(np.array([np.mean(y_train.value)]), \"b\", requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "306489fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Function of type <class 'src.function._Matmul'> with name matmul_0\n",
      "Creating Function of type <class 'src.function._Add'> with name add_0\n",
      "Creating Function of type <class 'src.function._MSELoss'> with name mse_loss_0\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "\n",
    "matmul1 = F._FunctionFactory().get_new_function_of_type(F._Matmul)\n",
    "add1 = F._FunctionFactory().get_new_function_of_type(F._Add)\n",
    "loss = F._FunctionFactory().get_new_function_of_type(F._MSELoss)\n",
    "    \n",
    "def fit(num_epoch):     \n",
    "    for i in range(num_epoch):\n",
    "        \n",
    "        y = add1.forward(matmul1.forward(X_train, w), b)\n",
    "        l = loss.forward(y, y_train)\n",
    "        loss.backward()\n",
    "        l_test = loss.forward(add1.forward(matmul1.forward(X_test, w), b), y_test)\n",
    "        w.value -= lr * w.grad\n",
    "        b.value -= lr * b.grad\n",
    "        l.zero_grad()   \n",
    "        \n",
    "        # if i % 10 == 0:\n",
    "        print(f\"Epoch {i+1}/{num_epoch}: Train Loss: {l.value} Test Loss: {l_test.value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b840b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400: Train Loss: 6096.250136138994 Test Loss: 5386.034167222656\n",
      "Epoch 2/400: Train Loss: 5038.67886933126 Test Loss: 4489.435829293016\n",
      "Epoch 3/400: Train Loss: 4412.7438975395435 Test Loss: 3982.7099172011635\n",
      "Epoch 4/400: Train Loss: 4027.5076433441595 Test Loss: 3684.9387808092333\n",
      "Epoch 5/400: Train Loss: 3778.7231476784273 Test Loss: 3500.3465906830393\n",
      "Epoch 6/400: Train Loss: 3609.1502947842805 Test Loss: 3378.2888124123015\n",
      "Epoch 7/400: Train Loss: 3487.085847998379 Test Loss: 3291.9559136192606\n",
      "Epoch 8/400: Train Loss: 3394.7402326792726 Test Loss: 3227.077225315589\n",
      "Epoch 9/400: Train Loss: 3321.9388561783403 Test Loss: 3175.954041277791\n",
      "Epoch 10/400: Train Loss: 3262.7036125909176 Test Loss: 3134.323248794919\n",
      "Epoch 11/400: Train Loss: 3213.3931823785683 Test Loss: 3099.718135346535\n",
      "Epoch 12/400: Train Loss: 3171.6876617189964 Test Loss: 3070.6170803980094\n",
      "Epoch 13/400: Train Loss: 3136.0311029426584 Test Loss: 3046.004285100264\n",
      "Epoch 14/400: Train Loss: 3105.3228777473746 Test Loss: 3025.144350957214\n",
      "Epoch 15/400: Train Loss: 3078.7446492268195 Test Loss: 3007.466837983766\n",
      "Epoch 16/400: Train Loss: 3055.661586664533 Test Loss: 2992.506782761745\n",
      "Epoch 17/400: Train Loss: 3035.5645066939724 Test Loss: 2979.873353775857\n",
      "Epoch 18/400: Train Loss: 3018.0348081745 Test Loss: 2969.2324886821184\n",
      "Epoch 19/400: Train Loss: 3002.722294742972 Test Loss: 2960.2964222980763\n",
      "Epoch 20/400: Train Loss: 2989.3304423267837 Test Loss: 2952.8166213945888\n",
      "Epoch 21/400: Train Loss: 2977.6060955405906 Test Loss: 2946.578455389921\n",
      "Epoch 22/400: Train Loss: 2967.3319004113564 Test Loss: 2941.3968241394678\n",
      "Epoch 23/400: Train Loss: 2958.320506222334 Test Loss: 2937.112390424081\n",
      "Epoch 24/400: Train Loss: 2950.4099696610115 Test Loss: 2933.5882603907116\n",
      "Epoch 25/400: Train Loss: 2943.460017843389 Test Loss: 2930.7070397049597\n",
      "Epoch 26/400: Train Loss: 2937.348953358235 Test Loss: 2928.3682263349633\n",
      "Epoch 27/400: Train Loss: 2931.971057764518 Test Loss: 2926.48591183703\n",
      "Epoch 28/400: Train Loss: 2927.234393673076 Test Loss: 2924.9867654259665\n",
      "Epoch 29/400: Train Loss: 2923.0589325870683 Test Loss: 2923.808275028398\n",
      "Epoch 30/400: Train Loss: 2919.374953167941 Test Loss: 2922.897219335274\n",
      "Epoch 31/400: Train Loss: 2916.1216664651356 Test Loss: 2922.208345337066\n",
      "Epoch 32/400: Train Loss: 2913.2460330990607 Test Loss: 2921.7032270020113\n",
      "Epoch 33/400: Train Loss: 2910.7017436701835 Test Loss: 2921.3492824660098\n",
      "Epoch 34/400: Train Loss: 2908.4483385115286 Test Loss: 2921.1189291219275\n",
      "Epoch 35/400: Train Loss: 2906.450446744918 Test Loss: 2920.9888581364967\n",
      "Epoch 36/400: Train Loss: 2904.6771277153935 Test Loss: 2920.9394120463985\n",
      "Epoch 37/400: Train Loss: 2903.1013004408005 Test Loss: 2920.954051101208\n",
      "Epoch 38/400: Train Loss: 2901.699248845342 Test Loss: 2921.0188958781014\n",
      "Epoch 39/400: Train Loss: 2900.4501923332746 Test Loss: 2921.1223353675337\n",
      "Epoch 40/400: Train Loss: 2899.335912765822 Test Loss: 2921.254691215139\n",
      "Epoch 41/400: Train Loss: 2898.340430180167 Test Loss: 2921.4079301091033\n",
      "Epoch 42/400: Train Loss: 2897.449720672799 Test Loss: 2921.575417436954\n",
      "Epoch 43/400: Train Loss: 2896.6514707919105 Test Loss: 2921.7517063171126\n",
      "Epoch 44/400: Train Loss: 2895.934863570482 Test Loss: 2921.932356955678\n",
      "Epoch 45/400: Train Loss: 2895.2903920041986 Test Loss: 2922.113782004395\n",
      "Epoch 46/400: Train Loss: 2894.7096963540653 Test Loss: 2922.293114217282\n",
      "Epoch 47/400: Train Loss: 2894.185422146996 Test Loss: 2922.468093235124\n",
      "Epoch 48/400: Train Loss: 2893.7110961712747 Test Loss: 2922.6369687816355\n",
      "Epoch 49/400: Train Loss: 2893.2810181277114 Test Loss: 2922.79841794357\n",
      "Epoch 50/400: Train Loss: 2892.890165910568 Test Loss: 2922.951474539097\n",
      "Epoch 51/400: Train Loss: 2892.534112762046 Test Loss: 2923.0954688625943\n",
      "Epoch 52/400: Train Loss: 2892.2089547767578 Test Loss: 2923.2299763367605\n",
      "Epoch 53/400: Train Loss: 2891.9112474333833 Test Loss: 2923.3547738106654\n",
      "Epoch 54/400: Train Loss: 2891.637950004188 Test Loss: 2923.469802420192\n",
      "Epoch 55/400: Train Loss: 2891.386376843078 Test Loss: 2923.575136079755\n",
      "Epoch 56/400: Train Loss: 2891.1541546827557 Test Loss: 2923.6709548048034\n",
      "Epoch 57/400: Train Loss: 2890.9391851840005 Test Loss: 2923.7575221767265\n",
      "Epoch 58/400: Train Loss: 2890.7396120776575 Test Loss: 2923.83516635805\n",
      "Epoch 59/400: Train Loss: 2890.553792324527 Test Loss: 2923.9042641484693\n",
      "Epoch 60/400: Train Loss: 2890.38027079186 Test Loss: 2923.9652276433835\n",
      "Epoch 61/400: Train Loss: 2890.217758008995 Test Loss: 2924.0184931177246\n",
      "Epoch 62/400: Train Loss: 2890.0651106202363 Test Loss: 2924.064511810483\n",
      "Epoch 63/400: Train Loss: 2889.921314201346 Test Loss: 2924.1037423306407\n",
      "Epoch 64/400: Train Loss: 2889.785468148135 Test Loss: 2924.136644444284\n",
      "Epoch 65/400: Train Loss: 2889.6567723822473 Test Loss: 2924.1636740361923\n",
      "Epoch 66/400: Train Loss: 2889.534515651201 Test Loss: 2924.185279068318\n",
      "Epoch 67/400: Train Loss: 2889.418065227599 Test Loss: 2924.2018963824225\n",
      "Epoch 68/400: Train Loss: 2889.306857836731 Test Loss: 2924.213949215757\n",
      "Epoch 69/400: Train Loss: 2889.200391663011 Test Loss: 2924.221845317245\n",
      "Epoch 70/400: Train Loss: 2889.098219304239 Test Loss: 2924.2259755675987\n",
      "Epoch 71/400: Train Loss: 2888.9999415588572 Test Loss: 2924.226713020679\n",
      "Epoch 72/400: Train Loss: 2888.9052019455416 Test Loss: 2924.2244122952907\n",
      "Epoch 73/400: Train Loss: 2888.813681866852 Test Loss: 2924.219409256853\n",
      "Epoch 74/400: Train Loss: 2888.7250963394877 Test Loss: 2924.21202093732\n",
      "Epoch 75/400: Train Loss: 2888.639190223185 Test Loss: 2924.202545649248\n",
      "Epoch 76/400: Train Loss: 2888.5557348886014 Test Loss: 2924.191263256581\n",
      "Epoch 77/400: Train Loss: 2888.4745252717826 Test Loss: 2924.178435570315\n",
      "Epoch 78/400: Train Loss: 2888.3953772691953 Test Loss: 2924.164306842103\n",
      "Epoch 79/400: Train Loss: 2888.31812543289 Test Loss: 2924.149104333063\n",
      "Epoch 80/400: Train Loss: 2888.2426209302503 Test Loss: 2924.1330389385876\n",
      "Epoch 81/400: Train Loss: 2888.1687297370704 Test Loss: 2924.1163058531397\n",
      "Epoch 82/400: Train Loss: 2888.09633103649 Test Loss: 2924.099085261585\n",
      "Epoch 83/400: Train Loss: 2888.0253157995967 Test Loss: 2924.081543045913\n",
      "Epoch 84/400: Train Loss: 2887.955585526421 Test Loss: 2924.063831498155\n",
      "Epoch 85/400: Train Loss: 2887.8870511286023 Test Loss: 2924.046090031943\n",
      "Epoch 86/400: Train Loss: 2887.819631937211 Test Loss: 2924.0284458865667\n",
      "Epoch 87/400: Train Loss: 2887.753254821224 Test Loss: 2924.011014818621\n",
      "Epoch 88/400: Train Loss: 2887.687853403836 Test Loss: 2923.993901777315\n",
      "Epoch 89/400: Train Loss: 2887.623367365327 Test Loss: 2923.9772015604126\n",
      "Epoch 90/400: Train Loss: 2887.559741822544 Test Loss: 2923.9609994484845\n",
      "Epoch 91/400: Train Loss: 2887.496926776218 Test Loss: 2923.9453718157897\n",
      "Epoch 92/400: Train Loss: 2887.434876618374 Test Loss: 2923.9303867165895\n",
      "Epoch 93/400: Train Loss: 2887.373549693012 Test Loss: 2923.916104446154\n",
      "Epoch 94/400: Train Loss: 2887.3129079040173 Test Loss: 2923.902578076076\n",
      "Epoch 95/400: Train Loss: 2887.252916364972 Test Loss: 2923.88985396377\n",
      "Epoch 96/400: Train Loss: 2887.1935430861754 Test Loss: 2923.8779722363283\n",
      "Epoch 97/400: Train Loss: 2887.134758694695 Test Loss: 2923.866967249053\n",
      "Epoch 98/400: Train Loss: 2887.0765361837784 Test Loss: 2923.856868019166\n",
      "Epoch 99/400: Train Loss: 2887.018850688375 Test Loss: 2923.847698635312\n",
      "Epoch 100/400: Train Loss: 2886.961679283881 Test Loss: 2923.839478643587\n",
      "Epoch 101/400: Train Loss: 2886.9050008055615 Test Loss: 2923.832223410837\n",
      "Epoch 102/400: Train Loss: 2886.848795686392 Test Loss: 2923.825944466115\n",
      "Epoch 103/400: Train Loss: 2886.79304581132 Test Loss: 2923.8206498211307\n",
      "Epoch 104/400: Train Loss: 2886.7377343861635 Test Loss: 2923.816344270621\n",
      "Epoch 105/400: Train Loss: 2886.682845819592 Test Loss: 2923.813029673505\n",
      "Epoch 106/400: Train Loss: 2886.6283656167684 Test Loss: 2923.8107052157807\n",
      "Epoch 107/400: Train Loss: 2886.5742802834347 Test Loss: 2923.8093676560184\n",
      "Epoch 108/400: Train Loss: 2886.5205772393288 Test Loss: 2923.8090115543614\n",
      "Epoch 109/400: Train Loss: 2886.467244739953 Test Loss: 2923.809629485891\n",
      "Epoch 110/400: Train Loss: 2886.4142718058292 Test Loss: 2923.8112122392054\n",
      "Epoch 111/400: Train Loss: 2886.361648158466 Test Loss: 2923.8137490010295\n",
      "Epoch 112/400: Train Loss: 2886.3093641623354 Test Loss: 2923.817227527645\n",
      "Epoch 113/400: Train Loss: 2886.257410772268 Test Loss: 2923.821634303909\n",
      "Epoch 114/400: Train Loss: 2886.205779485694 Test Loss: 2923.8269546905894\n",
      "Epoch 115/400: Train Loss: 2886.154462299261 Test Loss: 2923.833173060722\n",
      "Epoch 116/400: Train Loss: 2886.1034516693785 Test Loss: 2923.8402729256454\n",
      "Epoch 117/400: Train Loss: 2886.0527404763075 Test Loss: 2923.848237051378\n",
      "Epoch 118/400: Train Loss: 2886.0023219914474 Test Loss: 2923.857047565916\n",
      "Epoch 119/400: Train Loss: 2885.9521898474986 Test Loss: 2923.8666860580583\n",
      "Epoch 120/400: Train Loss: 2885.9023380112376 Test Loss: 2923.8771336682776\n",
      "Epoch 121/400: Train Loss: 2885.8527607586398 Test Loss: 2923.888371172174\n",
      "Epoch 122/400: Train Loss: 2885.8034526521365 Test Loss: 2923.9003790570123\n",
      "Epoch 123/400: Train Loss: 2885.7544085197965 Test Loss: 2923.9131375917577\n",
      "Epoch 124/400: Train Loss: 2885.7056234362626 Test Loss: 2923.9266268911197\n",
      "Epoch 125/400: Train Loss: 2885.657092705269 Test Loss: 2923.9408269739556\n",
      "Epoch 126/400: Train Loss: 2885.6088118436032 Test Loss: 2923.955717816445\n",
      "Epoch 127/400: Train Loss: 2885.560776566376 Test Loss: 2923.9712794004063\n",
      "Epoch 128/400: Train Loss: 2885.512982773481 Test Loss: 2923.987491757088\n",
      "Epoch 129/400: Train Loss: 2885.4654265371423 Test Loss: 2924.0043350067544\n",
      "Epoch 130/400: Train Loss: 2885.4181040904473 Test Loss: 2924.0217893943923\n",
      "Epoch 131/400: Train Loss: 2885.371011816783 Test Loss: 2924.0398353217793\n",
      "Epoch 132/400: Train Loss: 2885.324146240092 Test Loss: 2924.058453376228\n",
      "Epoch 133/400: Train Loss: 2885.2775040158876 Test Loss: 2924.0776243562145\n",
      "Epoch 134/400: Train Loss: 2885.2310819229447 Test Loss: 2924.097329294154\n",
      "Epoch 135/400: Train Loss: 2885.184876855633 Test Loss: 2924.117549476518\n",
      "Epoch 136/400: Train Loss: 2885.1388858168157 Test Loss: 2924.1382664615185\n",
      "Epoch 137/400: Train Loss: 2885.0931059112827 Test Loss: 2924.159462094526\n",
      "Epoch 138/400: Train Loss: 2885.047534339666 Test Loss: 2924.181118521422\n",
      "Epoch 139/400: Train Loss: 2885.0021683928007 Test Loss: 2924.2032182000407\n",
      "Epoch 140/400: Train Loss: 2884.957005446495 Test Loss: 2924.2257439098394\n",
      "Epoch 141/400: Train Loss: 2884.912042956673 Test Loss: 2924.2486787599832\n",
      "Epoch 142/400: Train Loss: 2884.8672784548694 Test Loss: 2924.2720061959267\n",
      "Epoch 143/400: Train Loss: 2884.82270954404 Test Loss: 2924.2957100046565\n",
      "Epoch 144/400: Train Loss: 2884.778333894659 Test Loss: 2924.3197743186915\n",
      "Epoch 145/400: Train Loss: 2884.734149241101 Test Loss: 2924.3441836189622\n",
      "Epoch 146/400: Train Loss: 2884.690153378257 Test Loss: 2924.368922736646\n",
      "Epoch 147/400: Train Loss: 2884.646344158394 Test Loss: 2924.3939768540854\n",
      "Epoch 148/400: Train Loss: 2884.6027194882213 Test Loss: 2924.419331504837\n",
      "Epoch 149/400: Train Loss: 2884.5592773261533 Test Loss: 2924.444972572977\n",
      "Epoch 150/400: Train Loss: 2884.5160156797556 Test Loss: 2924.4708862916805\n",
      "Epoch 151/400: Train Loss: 2884.4729326033626 Test Loss: 2924.497059241209\n",
      "Epoch 152/400: Train Loss: 2884.4300261958483 Test Loss: 2924.523478346307\n",
      "Epoch 153/400: Train Loss: 2884.387294598539 Test Loss: 2924.5501308731104\n",
      "Epoch 154/400: Train Loss: 2884.3447359932698 Test Loss: 2924.577004425608\n",
      "Epoch 155/400: Train Loss: 2884.3023486005513 Test Loss: 2924.6040869416943\n",
      "Epoch 156/400: Train Loss: 2884.2601306778697 Test Loss: 2924.631366688882\n",
      "Epoch 157/400: Train Loss: 2884.218080518074 Test Loss: 2924.6588322596926\n",
      "Epoch 158/400: Train Loss: 2884.1761964478837 Test Loss: 2924.686472566794\n",
      "Epoch 159/400: Train Loss: 2884.134476826472 Test Loss: 2924.7142768378826\n",
      "Epoch 160/400: Train Loss: 2884.092920044143 Test Loss: 2924.7422346103886\n",
      "Epoch 161/400: Train Loss: 2884.0515245210904 Test Loss: 2924.7703357259975\n",
      "Epoch 162/400: Train Loss: 2884.0102887062294 Test Loss: 2924.798570325032\n",
      "Epoch 163/400: Train Loss: 2883.969211076092 Test Loss: 2924.8269288407187\n",
      "Epoch 164/400: Train Loss: 2883.9282901337997 Test Loss: 2924.8554019933686\n",
      "Epoch 165/400: Train Loss: 2883.8875244080828 Test Loss: 2924.88398078448\n",
      "Epoch 166/400: Train Loss: 2883.8469124523695 Test Loss: 2924.9126564907906\n",
      "Epoch 167/400: Train Loss: 2883.8064528439113 Test Loss: 2924.9414206583\n",
      "Epoch 168/400: Train Loss: 2883.7661441829764 Test Loss: 2924.970265096275\n",
      "Epoch 169/400: Train Loss: 2883.725985092072 Test Loss: 2924.999181871247\n",
      "Epoch 170/400: Train Loss: 2883.68597421522 Test Loss: 2925.028163301029\n",
      "Epoch 171/400: Train Loss: 2883.646110217268 Test Loss: 2925.057201948746\n",
      "Epoch 172/400: Train Loss: 2883.6063917832375 Test Loss: 2925.086290616912\n",
      "Epoch 173/400: Train Loss: 2883.5668176177105 Test Loss: 2925.1154223415356\n",
      "Epoch 174/400: Train Loss: 2883.527386444244 Test Loss: 2925.1445903862896\n",
      "Epoch 175/400: Train Loss: 2883.4880970048184 Test Loss: 2925.173788236738\n",
      "Epoch 176/400: Train Loss: 2883.448948059313 Test Loss: 2925.203009594621\n",
      "Epoch 177/400: Train Loss: 2883.4099383850084 Test Loss: 2925.232248372229\n",
      "Epoch 178/400: Train Loss: 2883.371066776116 Test Loss: 2925.26149868684\n",
      "Epoch 179/400: Train Loss: 2883.3323320433256 Test Loss: 2925.2907548552507\n",
      "Epoch 180/400: Train Loss: 2883.293733013382 Test Loss: 2925.32001138839\n",
      "Epoch 181/400: Train Loss: 2883.2552685286764 Test Loss: 2925.3492629860175\n",
      "Epoch 182/400: Train Loss: 2883.2169374468604 Test Loss: 2925.378504531529\n",
      "Epoch 183/400: Train Loss: 2883.1787386404794 Test Loss: 2925.4077310868415\n",
      "Epoch 184/400: Train Loss: 2883.1406709966154 Test Loss: 2925.4369378873953\n",
      "Epoch 185/400: Train Loss: 2883.1027334165574 Test Loss: 2925.4661203372348\n",
      "Epoch 186/400: Train Loss: 2883.0649248154778 Test Loss: 2925.495274004201\n",
      "Epoch 187/400: Train Loss: 2883.0272441221236 Test Loss: 2925.524394615232\n",
      "Epoch 188/400: Train Loss: 2882.9896902785304 Test Loss: 2925.5534780517464\n",
      "Epoch 189/400: Train Loss: 2882.9522622397326 Test Loss: 2925.5825203451495\n",
      "Epoch 190/400: Train Loss: 2882.9149589735025 Test Loss: 2925.611517672428\n",
      "Epoch 191/400: Train Loss: 2882.877779460086 Test Loss: 2925.6404663518556\n",
      "Epoch 192/400: Train Loss: 2882.8407226919608 Test Loss: 2925.669362838788\n",
      "Epoch 193/400: Train Loss: 2882.8037876735934 Test Loss: 2925.6982037215803\n",
      "Epoch 194/400: Train Loss: 2882.766973421216 Test Loss: 2925.7269857175816\n",
      "Epoch 195/400: Train Loss: 2882.730278962603 Test Loss: 2925.7557056692417\n",
      "Epoch 196/400: Train Loss: 2882.6937033368595 Test Loss: 2925.78436054032\n",
      "Epoch 197/400: Train Loss: 2882.6572455942205 Test Loss: 2925.812947412181\n",
      "Epoch 198/400: Train Loss: 2882.6209047958537 Test Loss: 2925.841463480187\n",
      "Epoch 199/400: Train Loss: 2882.584680013668 Test Loss: 2925.869906050203\n",
      "Epoch 200/400: Train Loss: 2882.5485703301333 Test Loss: 2925.8982725351666\n",
      "Epoch 201/400: Train Loss: 2882.5125748381024 Test Loss: 2925.926560451773\n",
      "Epoch 202/400: Train Loss: 2882.476692640641 Test Loss: 2925.954767417242\n",
      "Epoch 203/400: Train Loss: 2882.4409228508603 Test Loss: 2925.982891146171\n",
      "Epoch 204/400: Train Loss: 2882.405264591762 Test Loss: 2926.010929447475\n",
      "Epoch 205/400: Train Loss: 2882.369716996078 Test Loss: 2926.038880221423\n",
      "Epoch 206/400: Train Loss: 2882.3342792061226 Test Loss: 2926.0667414567442\n",
      "Epoch 207/400: Train Loss: 2882.298950373647 Test Loss: 2926.0945112278214\n",
      "Epoch 208/400: Train Loss: 2882.2637296596968 Test Loss: 2926.122187691966\n",
      "Epoch 209/400: Train Loss: 2882.2286162344762 Test Loss: 2926.1497690867723\n",
      "Epoch 210/400: Train Loss: 2882.1936092772103 Test Loss: 2926.177253727544\n",
      "Epoch 211/400: Train Loss: 2882.1587079760207 Test Loss: 2926.2046400047984\n",
      "Epoch 212/400: Train Loss: 2882.1239115277945 Test Loss: 2926.231926381845\n",
      "Epoch 213/400: Train Loss: 2882.0892191380626 Test Loss: 2926.2591113924314\n",
      "Epoch 214/400: Train Loss: 2882.0546300208816 Test Loss: 2926.286193638467\n",
      "Epoch 215/400: Train Loss: 2882.020143398712 Test Loss: 2926.3131717878045\n",
      "Epoch 216/400: Train Loss: 2881.985758502309 Test Loss: 2926.3400445720954\n",
      "Epoch 217/400: Train Loss: 2881.9514745706088 Test Loss: 2926.3668107847025\n",
      "Epoch 218/400: Train Loss: 2881.917290850616 Test Loss: 2926.3934692786893\n",
      "Epoch 219/400: Train Loss: 2881.883206597307 Test Loss: 2926.4200189648486\n",
      "Epoch 220/400: Train Loss: 2881.849221073514 Test Loss: 2926.446458809818\n",
      "Epoch 221/400: Train Loss: 2881.8153335498323 Test Loss: 2926.472787834227\n",
      "Epoch 222/400: Train Loss: 2881.7815433045175 Test Loss: 2926.4990051109185\n",
      "Epoch 223/400: Train Loss: 2881.747849623388 Test Loss: 2926.525109763223\n",
      "Epoch 224/400: Train Loss: 2881.7142517997277 Test Loss: 2926.5511009632714\n",
      "Epoch 225/400: Train Loss: 2881.680749134197 Test Loss: 2926.5769779303837\n",
      "Epoch 226/400: Train Loss: 2881.647340934737 Test Loss: 2926.6027399294867\n",
      "Epoch 227/400: Train Loss: 2881.6140265164827 Test Loss: 2926.628386269593\n",
      "Epoch 228/400: Train Loss: 2881.580805201673 Test Loss: 2926.6539163023253\n",
      "Epoch 229/400: Train Loss: 2881.5476763195616 Test Loss: 2926.679329420485\n",
      "Epoch 230/400: Train Loss: 2881.5146392063402 Test Loss: 2926.7046250566627\n",
      "Epoch 231/400: Train Loss: 2881.481693205046 Test Loss: 2926.729802681903\n",
      "Epoch 232/400: Train Loss: 2881.448837665486 Test Loss: 2926.754861804402\n",
      "Epoch 233/400: Train Loss: 2881.4160719441516 Test Loss: 2926.7798019682464\n",
      "Epoch 234/400: Train Loss: 2881.3833954041443 Test Loss: 2926.8046227522004\n",
      "Epoch 235/400: Train Loss: 2881.3508074150955 Test Loss: 2926.8293237685193\n",
      "Epoch 236/400: Train Loss: 2881.3183073530895 Test Loss: 2926.853904661811\n",
      "Epoch 237/400: Train Loss: 2881.2858946005877 Test Loss: 2926.878365107934\n",
      "Epoch 238/400: Train Loss: 2881.253568546359 Test Loss: 2926.9027048129155\n",
      "Epoch 239/400: Train Loss: 2881.2213285854 Test Loss: 2926.9269235119286\n",
      "Epoch 240/400: Train Loss: 2881.18917411887 Test Loss: 2926.951020968277\n",
      "Epoch 241/400: Train Loss: 2881.1571045540154 Test Loss: 2926.974996972433\n",
      "Epoch 242/400: Train Loss: 2881.125119304102 Test Loss: 2926.9988513410926\n",
      "Epoch 243/400: Train Loss: 2881.093217788348 Test Loss: 2927.0225839162663\n",
      "Epoch 244/400: Train Loss: 2881.061399431852 Test Loss: 2927.046194564404\n",
      "Epoch 245/400: Train Loss: 2881.0296636655326 Test Loss: 2927.069683175533\n",
      "Epoch 246/400: Train Loss: 2880.9980099260565 Test Loss: 2927.0930496624414\n",
      "Epoch 247/400: Train Loss: 2880.9664376557785 Test Loss: 2927.1162939598803\n",
      "Epoch 248/400: Train Loss: 2880.934946302677 Test Loss: 2927.1394160237846\n",
      "Epoch 249/400: Train Loss: 2880.903535320287 Test Loss: 2927.162415830536\n",
      "Epoch 250/400: Train Loss: 2880.872204167645 Test Loss: 2927.1852933762307\n",
      "Epoch 251/400: Train Loss: 2880.8409523092205 Test Loss: 2927.208048675984\n",
      "Epoch 252/400: Train Loss: 2880.809779214863 Test Loss: 2927.230681763256\n",
      "Epoch 253/400: Train Loss: 2880.7786843597346 Test Loss: 2927.2531926891925\n",
      "Epoch 254/400: Train Loss: 2880.7476672242583 Test Loss: 2927.2755815219925\n",
      "Epoch 255/400: Train Loss: 2880.716727294055 Test Loss: 2927.2978483462916\n",
      "Epoch 256/400: Train Loss: 2880.6858640598903 Test Loss: 2927.319993262584\n",
      "Epoch 257/400: Train Loss: 2880.655077017615 Test Loss: 2927.3420163866253\n",
      "Epoch 258/400: Train Loss: 2880.624365668109 Test Loss: 2927.363917848894\n",
      "Epoch 259/400: Train Loss: 2880.5937295172293 Test Loss: 2927.3856977940536\n",
      "Epoch 260/400: Train Loss: 2880.5631680757524 Test Loss: 2927.4073563804222\n",
      "Epoch 261/400: Train Loss: 2880.532680859323 Test Loss: 2927.428893779483\n",
      "Epoch 262/400: Train Loss: 2880.502267388399 Test Loss: 2927.4503101753903\n",
      "Epoch 263/400: Train Loss: 2880.471927188201 Test Loss: 2927.471605764499\n",
      "Epoch 264/400: Train Loss: 2880.441659788658 Test Loss: 2927.492780754911\n",
      "Epoch 265/400: Train Loss: 2880.411464724359 Test Loss: 2927.513835366039\n",
      "Epoch 266/400: Train Loss: 2880.381341534501 Test Loss: 2927.534769828172\n",
      "Epoch 267/400: Train Loss: 2880.3512897628398 Test Loss: 2927.555584382072\n",
      "Epoch 268/400: Train Loss: 2880.321308957639 Test Loss: 2927.5762792785677\n",
      "Epoch 269/400: Train Loss: 2880.291398671624 Test Loss: 2927.5968547781777\n",
      "Epoch 270/400: Train Loss: 2880.2615584619307 Test Loss: 2927.6173111507264\n",
      "Epoch 271/400: Train Loss: 2880.2317878900617 Test Loss: 2927.637648674994\n",
      "Epoch 272/400: Train Loss: 2880.2020865218356 Test Loss: 2927.657867638363\n",
      "Epoch 273/400: Train Loss: 2880.1724539273428 Test Loss: 2927.677968336476\n",
      "Epoch 274/400: Train Loss: 2880.1428896808993 Test Loss: 2927.69795107292\n",
      "Epoch 275/400: Train Loss: 2880.113393360999 Test Loss: 2927.7178161589018\n",
      "Epoch 276/400: Train Loss: 2880.0839645502733 Test Loss: 2927.7375639129395\n",
      "Epoch 277/400: Train Loss: 2880.0546028354415 Test Loss: 2927.757194660581\n",
      "Epoch 278/400: Train Loss: 2880.025307807273 Test Loss: 2927.7767087341012\n",
      "Epoch 279/400: Train Loss: 2879.996079060537 Test Loss: 2927.796106472239\n",
      "Epoch 280/400: Train Loss: 2879.966916193964 Test Loss: 2927.815388219923\n",
      "Epoch 281/400: Train Loss: 2879.937818810206 Test Loss: 2927.8345543280107\n",
      "Epoch 282/400: Train Loss: 2879.9087865157862 Test Loss: 2927.8536051530446\n",
      "Epoch 283/400: Train Loss: 2879.8798189210656 Test Loss: 2927.8725410570046\n",
      "Epoch 284/400: Train Loss: 2879.850915640199 Test Loss: 2927.891362407076\n",
      "Epoch 285/400: Train Loss: 2879.822076291094 Test Loss: 2927.910069575425\n",
      "Epoch 286/400: Train Loss: 2879.7933004953707 Test Loss: 2927.928662938973\n",
      "Epoch 287/400: Train Loss: 2879.764587878324 Test Loss: 2927.947142879191\n",
      "Epoch 288/400: Train Loss: 2879.7359380688845 Test Loss: 2927.9655097818913\n",
      "Epoch 289/400: Train Loss: 2879.7073506995757 Test Loss: 2927.9837640370283\n",
      "Epoch 290/400: Train Loss: 2879.678825406481 Test Loss: 2928.001906038513\n",
      "Epoch 291/400: Train Loss: 2879.650361829203 Test Loss: 2928.019936184016\n",
      "Epoch 292/400: Train Loss: 2879.6219596108267 Test Loss: 2928.037854874798\n",
      "Epoch 293/400: Train Loss: 2879.593618397882 Test Loss: 2928.05566251553\n",
      "Epoch 294/400: Train Loss: 2879.565337840307 Test Loss: 2928.0733595141287\n",
      "Epoch 295/400: Train Loss: 2879.5371175914142 Test Loss: 2928.090946281593\n",
      "Epoch 296/400: Train Loss: 2879.5089573078503 Test Loss: 2928.108423231845\n",
      "Epoch 297/400: Train Loss: 2879.4808566495653 Test Loss: 2928.1257907815775\n",
      "Epoch 298/400: Train Loss: 2879.452815279774 Test Loss: 2928.143049350114\n",
      "Epoch 299/400: Train Loss: 2879.4248328649232 Test Loss: 2928.160199359256\n",
      "Epoch 300/400: Train Loss: 2879.396909074657 Test Loss: 2928.1772412331497\n",
      "Epoch 301/400: Train Loss: 2879.369043581783 Test Loss: 2928.194175398156\n",
      "Epoch 302/400: Train Loss: 2879.3412360622387 Test Loss: 2928.2110022827187\n",
      "Epoch 303/400: Train Loss: 2879.313486195058 Test Loss: 2928.2277223172364\n",
      "Epoch 304/400: Train Loss: 2879.2857936623386 Test Loss: 2928.2443359339504\n",
      "Epoch 305/400: Train Loss: 2879.2581581492095 Test Loss: 2928.260843566817\n",
      "Epoch 306/400: Train Loss: 2879.230579343801 Test Loss: 2928.2772456514076\n",
      "Epoch 307/400: Train Loss: 2879.2030569372087 Test Loss: 2928.2935426247896\n",
      "Epoch 308/400: Train Loss: 2879.1755906234653 Test Loss: 2928.3097349254217\n",
      "Epoch 309/400: Train Loss: 2879.148180099508 Test Loss: 2928.325822993058\n",
      "Epoch 310/400: Train Loss: 2879.1208250651507 Test Loss: 2928.341807268644\n",
      "Epoch 311/400: Train Loss: 2879.0935252230483 Test Loss: 2928.3576881942217\n",
      "Epoch 312/400: Train Loss: 2879.066280278673 Test Loss: 2928.3734662128386\n",
      "Epoch 313/400: Train Loss: 2879.0390899402787 Test Loss: 2928.3891417684604\n",
      "Epoch 314/400: Train Loss: 2879.011953918875 Test Loss: 2928.4047153058764\n",
      "Epoch 315/400: Train Loss: 2878.9848719281986 Test Loss: 2928.4201872706285\n",
      "Epoch 316/400: Train Loss: 2878.957843684681 Test Loss: 2928.435558108916\n",
      "Epoch 317/400: Train Loss: 2878.930868907424 Test Loss: 2928.450828267531\n",
      "Epoch 318/400: Train Loss: 2878.9039473181706 Test Loss: 2928.465998193774\n",
      "Epoch 319/400: Train Loss: 2878.877078641273 Test Loss: 2928.4810683353844\n",
      "Epoch 320/400: Train Loss: 2878.850262603672 Test Loss: 2928.496039140469\n",
      "Epoch 321/400: Train Loss: 2878.8234989348634 Test Loss: 2928.51091105743\n",
      "Epoch 322/400: Train Loss: 2878.796787366877 Test Loss: 2928.525684534907\n",
      "Epoch 323/400: Train Loss: 2878.770127634244 Test Loss: 2928.5403600217087\n",
      "Epoch 324/400: Train Loss: 2878.743519473974 Test Loss: 2928.554937966749\n",
      "Epoch 325/400: Train Loss: 2878.7169626255295 Test Loss: 2928.5694188189873\n",
      "Epoch 326/400: Train Loss: 2878.6904568307973 Test Loss: 2928.5838030273794\n",
      "Epoch 327/400: Train Loss: 2878.6640018340645 Test Loss: 2928.598091040809\n",
      "Epoch 328/400: Train Loss: 2878.637597381995 Test Loss: 2928.612283308041\n",
      "Epoch 329/400: Train Loss: 2878.611243223601 Test Loss: 2928.626380277669\n",
      "Epoch 330/400: Train Loss: 2878.58493911022 Test Loss: 2928.6403823980663\n",
      "Epoch 331/400: Train Loss: 2878.5586847954924 Test Loss: 2928.6542901173298\n",
      "Epoch 332/400: Train Loss: 2878.5324800353324 Test Loss: 2928.6681038832426\n",
      "Epoch 333/400: Train Loss: 2878.506324587909 Test Loss: 2928.6818241432215\n",
      "Epoch 334/400: Train Loss: 2878.480218213619 Test Loss: 2928.695451344275\n",
      "Epoch 335/400: Train Loss: 2878.4541606750668 Test Loss: 2928.7089859329653\n",
      "Epoch 336/400: Train Loss: 2878.428151737036 Test Loss: 2928.7224283553583\n",
      "Epoch 337/400: Train Loss: 2878.4021911664718 Test Loss: 2928.73577905699\n",
      "Epoch 338/400: Train Loss: 2878.376278732457 Test Loss: 2928.74903848283\n",
      "Epoch 339/400: Train Loss: 2878.350414206186 Test Loss: 2928.7622070772377\n",
      "Epoch 340/400: Train Loss: 2878.3245973609482 Test Loss: 2928.7752852839353\n",
      "Epoch 341/400: Train Loss: 2878.2988279721008 Test Loss: 2928.7882735459616\n",
      "Epoch 342/400: Train Loss: 2878.2731058170525 Test Loss: 2928.801172305653\n",
      "Epoch 343/400: Train Loss: 2878.2474306752356 Test Loss: 2928.8139820045976\n",
      "Epoch 344/400: Train Loss: 2878.2218023280902 Test Loss: 2928.826703083616\n",
      "Epoch 345/400: Train Loss: 2878.1962205590407 Test Loss: 2928.8393359827214\n",
      "Epoch 346/400: Train Loss: 2878.1706851534746 Test Loss: 2928.851881141096\n",
      "Epoch 347/400: Train Loss: 2878.1451958987245 Test Loss: 2928.864338997064\n",
      "Epoch 348/400: Train Loss: 2878.119752584043 Test Loss: 2928.876709988062\n",
      "Epoch 349/400: Train Loss: 2878.0943550005895 Test Loss: 2928.8889945506157\n",
      "Epoch 350/400: Train Loss: 2878.0690029414022 Test Loss: 2928.9011931203077\n",
      "Epoch 351/400: Train Loss: 2878.0436962013846 Test Loss: 2928.9133061317625\n",
      "Epoch 352/400: Train Loss: 2878.018434577284 Test Loss: 2928.9253340186215\n",
      "Epoch 353/400: Train Loss: 2877.993217867671 Test Loss: 2928.937277213515\n",
      "Epoch 354/400: Train Loss: 2877.9680458729213 Test Loss: 2928.949136148045\n",
      "Epoch 355/400: Train Loss: 2877.9429183951984 Test Loss: 2928.9609112527633\n",
      "Epoch 356/400: Train Loss: 2877.917835238431 Test Loss: 2928.9726029571534\n",
      "Epoch 357/400: Train Loss: 2877.8927962083003 Test Loss: 2928.984211689609\n",
      "Epoch 358/400: Train Loss: 2877.8678011122147 Test Loss: 2928.995737877412\n",
      "Epoch 359/400: Train Loss: 2877.842849759298 Test Loss: 2929.0071819467244\n",
      "Epoch 360/400: Train Loss: 2877.817941960368 Test Loss: 2929.0185443225582\n",
      "Epoch 361/400: Train Loss: 2877.793077527919 Test Loss: 2929.0298254287736\n",
      "Epoch 362/400: Train Loss: 2877.768256276106 Test Loss: 2929.0410256880477\n",
      "Epoch 363/400: Train Loss: 2877.7434780207254 Test Loss: 2929.052145521874\n",
      "Epoch 364/400: Train Loss: 2877.7187425791994 Test Loss: 2929.063185350531\n",
      "Epoch 365/400: Train Loss: 2877.694049770558 Test Loss: 2929.0741455930875\n",
      "Epoch 366/400: Train Loss: 2877.669399415423 Test Loss: 2929.0850266673706\n",
      "Epoch 367/400: Train Loss: 2877.6447913359902 Test Loss: 2929.0958289899654\n",
      "Epoch 368/400: Train Loss: 2877.620225356014 Test Loss: 2929.1065529761977\n",
      "Epoch 369/400: Train Loss: 2877.5957013007915 Test Loss: 2929.117199040122\n",
      "Epoch 370/400: Train Loss: 2877.571218997146 Test Loss: 2929.12776759451\n",
      "Epoch 371/400: Train Loss: 2877.5467782734086 Test Loss: 2929.138259050839\n",
      "Epoch 372/400: Train Loss: 2877.5223789594083 Test Loss: 2929.1486738192843\n",
      "Epoch 373/400: Train Loss: 2877.498020886449 Test Loss: 2929.159012308705\n",
      "Epoch 374/400: Train Loss: 2877.4737038873027 Test Loss: 2929.169274926639\n",
      "Epoch 375/400: Train Loss: 2877.4494277961853 Test Loss: 2929.179462079291\n",
      "Epoch 376/400: Train Loss: 2877.425192448748 Test Loss: 2929.189574171521\n",
      "Epoch 377/400: Train Loss: 2877.400997682059 Test Loss: 2929.1996116068412\n",
      "Epoch 378/400: Train Loss: 2877.376843334591 Test Loss: 2929.2095747874037\n",
      "Epoch 379/400: Train Loss: 2877.3527292462045 Test Loss: 2929.2194641139986\n",
      "Epoch 380/400: Train Loss: 2877.3286552581353 Test Loss: 2929.229279986038\n",
      "Epoch 381/400: Train Loss: 2877.30462121298 Test Loss: 2929.2390228015606\n",
      "Epoch 382/400: Train Loss: 2877.2806269546772 Test Loss: 2929.2486929572106\n",
      "Epoch 383/400: Train Loss: 2877.2566723285036 Test Loss: 2929.258290848246\n",
      "Epoch 384/400: Train Loss: 2877.2327571810474 Test Loss: 2929.26781686852\n",
      "Epoch 385/400: Train Loss: 2877.2088813602068 Test Loss: 2929.2772714104876\n",
      "Epoch 386/400: Train Loss: 2877.1850447151683 Test Loss: 2929.286654865192\n",
      "Epoch 387/400: Train Loss: 2877.161247096396 Test Loss: 2929.2959676222604\n",
      "Epoch 388/400: Train Loss: 2877.137488355618 Test Loss: 2929.3052100699015\n",
      "Epoch 389/400: Train Loss: 2877.113768345815 Test Loss: 2929.3143825949\n",
      "Epoch 390/400: Train Loss: 2877.0900869212032 Test Loss: 2929.3234855826154\n",
      "Epoch 391/400: Train Loss: 2877.0664439372276 Test Loss: 2929.3325194169715\n",
      "Epoch 392/400: Train Loss: 2877.0428392505432 Test Loss: 2929.341484480462\n",
      "Epoch 393/400: Train Loss: 2877.0192727190056 Test Loss: 2929.350381154135\n",
      "Epoch 394/400: Train Loss: 2876.995744201658 Test Loss: 2929.3592098176023\n",
      "Epoch 395/400: Train Loss: 2876.9722535587202 Test Loss: 2929.3679708490317\n",
      "Epoch 396/400: Train Loss: 2876.9488006515735 Test Loss: 2929.376664625137\n",
      "Epoch 397/400: Train Loss: 2876.9253853427513 Test Loss: 2929.38529152119\n",
      "Epoch 398/400: Train Loss: 2876.902007495925 Test Loss: 2929.3938519110056\n",
      "Epoch 399/400: Train Loss: 2876.8786669758956 Test Loss: 2929.4023461669467\n",
      "Epoch 400/400: Train Loss: 2876.8553636485776 Test Loss: 2929.410774659918\n"
     ]
    }
   ],
   "source": [
    "fit(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1e5a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
