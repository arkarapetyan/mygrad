{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e8177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mygrad\n",
    "import mygrad.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e23983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_diabetes(return_X_y = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = mygrad.value(X_train, \"X_train\", requires_grad=False)\n",
    "X_test = mygrad.value(X_test, \"X_test\", requires_grad=False)\n",
    "y_train = mygrad.value(y_train, \"y_train\", requires_grad=False)\n",
    "y_test = mygrad.value(y_test, \"y_test\", requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e18c0d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w = mygrad.value(np.random.randn(X.shape[1]) * 10**(-1), \"w\", requires_grad = True)\n",
    "b = mygrad.value(np.array([np.mean(y_train.data)]), \"b\", requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "306489fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Function of type <class 'mygrad.functional.function.Matmul'> with name matmul_0\n",
      "Creating Function of type <class 'mygrad.functional.function.Add'> with name add_0\n",
      "Creating Function of type <class 'mygrad.functional.function.MSELoss'> with name mse_loss_0\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "\n",
    "matmul1 = F.matmul()\n",
    "add1 = F.add()\n",
    "loss = F.mse_loss()\n",
    "    \n",
    "def fit(num_epoch):     \n",
    "    for i in range(num_epoch):\n",
    "        \n",
    "        y = add1.forward(matmul1.forward(X_train, w), b)\n",
    "        l = loss.forward(y, y_train)\n",
    "        loss.backward()\n",
    "        l_test = loss.forward(add1.forward(matmul1.forward(X_test, w), b), y_test)\n",
    "        w.data -= lr * w.grad\n",
    "        b.data -= lr * b.grad\n",
    "        l.zero_grad()   \n",
    "        \n",
    "        print(f\"Epoch {i+1}/{num_epoch}: Train Loss: {l.data} Test Loss: {l_test.data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b840b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400: Train Loss: 6077.474581072149 Test Loss: 5533.0856490319065\n",
      "Epoch 2/400: Train Loss: 5066.030807996235 Test Loss: 4557.581135793935\n",
      "Epoch 3/400: Train Loss: 4455.078679636502 Test Loss: 3985.3682682147214\n",
      "Epoch 4/400: Train Loss: 4071.900452319066 Test Loss: 3638.7585718614364\n",
      "Epoch 5/400: Train Loss: 3820.5186935331794 Test Loss: 3420.1362011328238\n",
      "Epoch 6/400: Train Loss: 3647.254275343144 Test Loss: 3275.6446018251304\n",
      "Epoch 7/400: Train Loss: 3521.795947997143 Test Loss: 3175.3556811037834\n",
      "Epoch 8/400: Train Loss: 3426.7870282935514 Test Loss: 3102.4457704371775\n",
      "Epoch 9/400: Train Loss: 3352.0900803325194 Test Loss: 3047.289690671799\n",
      "Epoch 10/400: Train Loss: 3291.622132750584 Test Loss: 3004.2381919180775\n",
      "Epoch 11/400: Train Loss: 3241.603364339115 Test Loss: 2969.8583013930906\n",
      "Epoch 12/400: Train Loss: 3199.583823883295 Test Loss: 2941.9706322836746\n",
      "Epoch 13/400: Train Loss: 3163.899084347312 Test Loss: 2919.12064288937\n",
      "Epoch 14/400: Train Loss: 3133.3629286884707 Test Loss: 2900.286180624563\n",
      "Epoch 15/400: Train Loss: 3107.0914846526584 Test Loss: 2884.7137972183004\n",
      "Epoch 16/400: Train Loss: 3084.400641659703 Test Loss: 2871.8254156607754\n",
      "Epoch 17/400: Train Loss: 3064.7446413958464 Test Loss: 2861.1636284797055\n",
      "Epoch 18/400: Train Loss: 3047.6780655328544 Test Loss: 2852.3584053060695\n",
      "Epoch 19/400: Train Loss: 3032.831335118671 Test Loss: 2845.1058487984906\n",
      "Epoch 20/400: Train Loss: 3019.8941883653097 Test Loss: 2839.153894332822\n",
      "Epoch 21/400: Train Loss: 3008.6040099514375 Test Loss: 2834.292151053911\n",
      "Epoch 22/400: Train Loss: 2998.7372204789667 Test Loss: 2830.344326887194\n",
      "Epoch 23/400: Train Loss: 2990.1026800381137 Test Loss: 2827.162354375669\n",
      "Epoch 24/400: Train Loss: 2982.53647915869 Test Loss: 2824.621700970234\n",
      "Epoch 25/400: Train Loss: 2975.8977291142332 Test Loss: 2822.617548727434\n",
      "Epoch 26/400: Train Loss: 2970.0651016683846 Test Loss: 2821.061640719256\n",
      "Epoch 27/400: Train Loss: 2964.9339500964834 Test Loss: 2819.879655863132\n",
      "Epoch 28/400: Train Loss: 2960.4138931323764 Test Loss: 2819.0090122910115\n",
      "Epoch 29/400: Train Loss: 2956.426774968486 Test Loss: 2818.397023515879\n",
      "Epoch 30/400: Train Loss: 2952.9049351716494 Test Loss: 2817.9993477781077\n",
      "Epoch 31/400: Train Loss: 2949.789736651328 Test Loss: 2817.7786824093278\n",
      "Epoch 32/400: Train Loss: 2947.0303100758483 Test Loss: 2817.7036636449875\n",
      "Epoch 33/400: Train Loss: 2944.5824807969584 Test Loss: 2817.747939040565\n",
      "Epoch 34/400: Train Loss: 2942.4078502578263 Test Loss: 2817.889385064655\n",
      "Epoch 35/400: Train Loss: 2940.4730085420647 Test Loss: 2818.109446890385\n",
      "Epoch 36/400: Train Loss: 2938.7488585002784 Test Loss: 2818.3925810994733\n",
      "Epoch 37/400: Train Loss: 2937.210034983459 Test Loss: 2818.7257850974047\n",
      "Epoch 38/400: Train Loss: 2935.834405269673 Test Loss: 2819.0981996219775\n",
      "Epoch 39/400: Train Loss: 2934.6026389001395 Test Loss: 2819.500772894983\n",
      "Epoch 40/400: Train Loss: 2933.497836923744 Test Loss: 2819.925976785851\n",
      "Epoch 41/400: Train Loss: 2932.505212047594 Test Loss: 2820.367566883111\n",
      "Epoch 42/400: Train Loss: 2931.6118124544596 Test Loss: 2820.8203796511525\n",
      "Epoch 43/400: Train Loss: 2930.806283115303 Test Loss: 2821.280160925762\n",
      "Epoch 44/400: Train Loss: 2930.0786593286716 Test Loss: 2821.7434209052312\n",
      "Epoch 45/400: Train Loss: 2929.420187984923 Test Loss: 2822.2073115525827\n",
      "Epoch 46/400: Train Loss: 2928.8231727038483 Test Loss: 2822.669522961994\n",
      "Epoch 47/400: Train Loss: 2928.2808395474685 Test Loss: 2823.1281957785222\n",
      "Epoch 48/400: Train Loss: 2927.787220480695 Test Loss: 2823.5818472112464\n",
      "Epoch 49/400: Train Loss: 2927.337052153942 Test Loss: 2824.0293085596318\n",
      "Epoch 50/400: Train Loss: 2926.925687924204 Test Loss: 2824.469672492979\n",
      "Epoch 51/400: Train Loss: 2926.549021323639 Test Loss: 2824.9022485926234\n",
      "Epoch 52/400: Train Loss: 2926.2034194347393 Test Loss: 2825.326525894421\n",
      "Epoch 53/400: Train Loss: 2925.88566484524 Test Loss: 2825.742141361468\n",
      "Epoch 54/400: Train Loss: 2925.592905039217 Test Loss: 2826.1488533797165\n",
      "Epoch 55/400: Train Loss: 2925.3226082380684 Test Loss: 2826.5465195069064\n",
      "Epoch 56/400: Train Loss: 2925.0725248399694 Test Loss: 2826.9350778217336\n",
      "Epoch 57/400: Train Loss: 2924.840653722314 Test Loss: 2827.314531319084\n",
      "Epoch 58/400: Train Loss: 2924.6252127712664 Test Loss: 2827.6849348808746\n",
      "Epoch 59/400: Train Loss: 2924.4246130883034 Test Loss: 2828.0463844231926\n",
      "Epoch 60/400: Train Loss: 2924.2374363974463 Test Loss: 2828.3990078807315\n",
      "Epoch 61/400: Train Loss: 2924.062415240528 Test Loss: 2828.7429577408334\n",
      "Epoch 62/400: Train Loss: 2923.898415602693 Test Loss: 2829.078404882924\n",
      "Epoch 63/400: Train Loss: 2923.7444216577383 Test Loss: 2829.405533516259\n",
      "Epoch 64/400: Train Loss: 2923.5995223637856 Test Loss: 2829.7245370402416\n",
      "Epoch 65/400: Train Loss: 2923.462899675176 Test Loss: 2830.0356146784684\n",
      "Epoch 66/400: Train Loss: 2923.333818167053 Test Loss: 2830.338968760289\n",
      "Epoch 67/400: Train Loss: 2923.2116158955973 Test Loss: 2830.6348025431507\n",
      "Epoch 68/400: Train Loss: 2923.09569633981 Test Loss: 2830.9233184853583\n",
      "Epoch 69/400: Train Loss: 2922.985521290635 Test Loss: 2831.204716892927\n",
      "Epoch 70/400: Train Loss: 2922.8806045704264 Test Loss: 2831.4791948761263\n",
      "Epoch 71/400: Train Loss: 2922.7805064807867 Test Loss: 2831.7469455613627\n",
      "Epoch 72/400: Train Loss: 2922.6848288897336 Test Loss: 2832.0081575127165\n",
      "Epoch 73/400: Train Loss: 2922.593210880493 Test Loss: 2832.263014324692\n",
      "Epoch 74/400: Train Loss: 2922.5053248940176 Test Loss: 2832.511694353958\n",
      "Epoch 75/400: Train Loss: 2922.420873305852 Test Loss: 2832.754370563123\n",
      "Epoch 76/400: Train Loss: 2922.3395853854413 Test Loss: 2832.991210453988\n",
      "Epoch 77/400: Train Loss: 2922.2612145924063 Test Loss: 2833.222376071554\n",
      "Epoch 78/400: Train Loss: 2922.185536169988 Test Loss: 2833.448024063197\n",
      "Epoch 79/400: Train Loss: 2922.1123450007594 Test Loss: 2833.668305780162\n",
      "Epoch 80/400: Train Loss: 2922.0414536940075 Test Loss: 2833.88336741073\n",
      "Epoch 81/400: Train Loss: 2921.9726908779217 Test Loss: 2834.0933501364184\n",
      "Epoch 82/400: Train Loss: 2921.9058996730155 Test Loss: 2834.2983903040886\n",
      "Epoch 83/400: Train Loss: 2921.8409363260416 Test Loss: 2834.498619608251\n",
      "Epoch 84/400: Train Loss: 2921.777668986183 Test Loss: 2834.694165278955\n",
      "Epoch 85/400: Train Loss: 2921.7159766074756 Test Loss: 2834.8851502716\n",
      "Epoch 86/400: Train Loss: 2921.6557479633384 Test Loss: 2835.0716934557877\n",
      "Epoch 87/400: Train Loss: 2921.596880760762 Test Loss: 2835.253909801004\n",
      "Epoch 88/400: Train Loss: 2921.539280843172 Test Loss: 2835.431910557404\n",
      "Epoch 89/400: Train Loss: 2921.482861472281 Test Loss: 2835.6058034305115\n",
      "Epoch 90/400: Train Loss: 2921.427542680368 Test Loss: 2835.775692748882\n",
      "Epoch 91/400: Train Loss: 2921.3732506854035 Test Loss: 2835.9416796241976\n",
      "Epoch 92/400: Train Loss: 2921.31991736235 Test Loss: 2836.103862103393\n",
      "Epoch 93/400: Train Loss: 2921.2674797646773 Test Loss: 2836.262335312696\n",
      "Epoch 94/400: Train Loss: 2921.215879690859 Test Loss: 2836.4171915935212\n",
      "Epoch 95/400: Train Loss: 2921.1650632911774 Test Loss: 2836.568520630373\n",
      "Epoch 96/400: Train Loss: 2921.1149807107017 Test Loss: 2836.716409570911\n",
      "Epoch 97/400: Train Loss: 2921.0655857647516 Test Loss: 2836.8609431384734\n",
      "Epoch 98/400: Train Loss: 2921.0168356435893 Test Loss: 2837.002203737344\n",
      "Epoch 99/400: Train Loss: 2920.968690643405 Test Loss: 2837.140271551137\n",
      "Epoch 100/400: Train Loss: 2920.9211139210233 Test Loss: 2837.2752246346436\n",
      "Epoch 101/400: Train Loss: 2920.8740712699905 Test Loss: 2837.40713899953\n",
      "Epoch 102/400: Train Loss: 2920.827530915985 Test Loss: 2837.536088694278\n",
      "Epoch 103/400: Train Loss: 2920.781463329709 Test Loss: 2837.6621458787267\n",
      "Epoch 104/400: Train Loss: 2920.7358410555817 Test Loss: 2837.7853808936156\n",
      "Epoch 105/400: Train Loss: 2920.690638554782 Test Loss: 2837.90586232547\n",
      "Epoch 106/400: Train Loss: 2920.6458320612865 Test Loss: 2838.0236570671864\n",
      "Epoch 107/400: Train Loss: 2920.601399449718 Test Loss: 2838.138830374646\n",
      "Epoch 108/400: Train Loss: 2920.557320113941 Test Loss: 2838.2514459196796\n",
      "Epoch 109/400: Train Loss: 2920.51357485542 Test Loss: 2838.3615658396716\n",
      "Epoch 110/400: Train Loss: 2920.4701457804867 Test Loss: 2838.4692507840805\n",
      "Epoch 111/400: Train Loss: 2920.427016205731 Test Loss: 2838.5745599581523\n",
      "Epoch 112/400: Train Loss: 2920.3841705707964 Test Loss: 2838.677551164055\n",
      "Epoch 113/400: Train Loss: 2920.3415943579585 Test Loss: 2838.778280839657\n",
      "Epoch 114/400: Train Loss: 2920.2992740178956 Test Loss: 2838.8768040951727\n",
      "Epoch 115/400: Train Loss: 2920.257196901132 Test Loss: 2838.973174747847\n",
      "Epoch 116/400: Train Loss: 2920.2153511946804 Test Loss: 2839.0674453548745\n",
      "Epoch 117/400: Train Loss: 2920.1737258634494 Test Loss: 2839.1596672446954\n",
      "Epoch 118/400: Train Loss: 2920.1323105960346 Test Loss: 2839.2498905468237\n",
      "Epoch 119/400: Train Loss: 2920.091095754523 Test Loss: 2839.338164220354\n",
      "Epoch 120/400: Train Loss: 2920.050072327996 Test Loss: 2839.424536081236\n",
      "Epoch 121/400: Train Loss: 2920.0092318894413 Test Loss: 2839.5090528284677\n",
      "Epoch 122/400: Train Loss: 2919.968566555788 Test Loss: 2839.5917600692605\n",
      "Epoch 123/400: Train Loss: 2919.928068950837 Test Loss: 2839.6727023433195\n",
      "Epoch 124/400: Train Loss: 2919.8877321708487 Test Loss: 2839.75192314627\n",
      "Epoch 125/400: Train Loss: 2919.8475497525947 Test Loss: 2839.8294649523323\n",
      "Epoch 126/400: Train Loss: 2919.8075156436707 Test Loss: 2839.905369236305\n",
      "Epoch 127/400: Train Loss: 2919.7676241749145 Test Loss: 2839.9796764949024\n",
      "Epoch 128/400: Train Loss: 2919.727870034759 Test Loss: 2840.052426267516\n",
      "Epoch 129/400: Train Loss: 2919.688248245377 Test Loss: 2840.1236571564336\n",
      "Epoch 130/400: Train Loss: 2919.648754140495 Test Loss: 2840.1934068465594\n",
      "Epoch 131/400: Train Loss: 2919.6093833447358 Test Loss: 2840.26171212467\n",
      "Epoch 132/400: Train Loss: 2919.5701317543917 Test Loss: 2840.3286088982486\n",
      "Epoch 133/400: Train Loss: 2919.5309955195194 Test Loss: 2840.3941322139044\n",
      "Epoch 134/400: Train Loss: 2919.491971027258 Test Loss: 2840.4583162754334\n",
      "Epoch 135/400: Train Loss: 2919.4530548862886 Test Loss: 2840.5211944615016\n",
      "Epoch 136/400: Train Loss: 2919.414243912345 Test Loss: 2840.582799342999\n",
      "Epoch 137/400: Train Loss: 2919.3755351147092 Test Loss: 2840.6431627000966\n",
      "Epoch 138/400: Train Loss: 2919.336925683619 Test Loss: 2840.7023155389525\n",
      "Epoch 139/400: Train Loss: 2919.298412978518 Test Loss: 2840.7602881081602\n",
      "Epoch 140/400: Train Loss: 2919.2599945171 Test Loss: 2840.8171099149035\n",
      "Epoch 141/400: Train Loss: 2919.2216679650796 Test Loss: 2840.8728097408366\n",
      "Epoch 142/400: Train Loss: 2919.1834311266484 Test Loss: 2840.9274156576903\n",
      "Epoch 143/400: Train Loss: 2919.1452819355686 Test Loss: 2840.9809550426344\n",
      "Epoch 144/400: Train Loss: 2919.10721844685 Test Loss: 2841.0334545933815\n",
      "Epoch 145/400: Train Loss: 2919.069238828984 Test Loss: 2841.084940343033\n",
      "Epoch 146/400: Train Loss: 2919.0313413566864 Test Loss: 2841.135437674695\n",
      "Epoch 147/400: Train Loss: 2918.9935244041208 Test Loss: 2841.184971335847\n",
      "Epoch 148/400: Train Loss: 2918.9557864385647 Test Loss: 2841.233565452477\n",
      "Epoch 149/400: Train Loss: 2918.9181260144924 Test Loss: 2841.28124354297\n",
      "Epoch 150/400: Train Loss: 2918.8805417680437 Test Loss: 2841.328028531792\n",
      "Epoch 151/400: Train Loss: 2918.8430324118563 Test Loss: 2841.3739427629116\n",
      "Epoch 152/400: Train Loss: 2918.8055967302294 Test Loss: 2841.419008013022\n",
      "Epoch 153/400: Train Loss: 2918.7682335746094 Test Loss: 2841.463245504509\n",
      "Epoch 154/400: Train Loss: 2918.730941859363 Test Loss: 2841.5066759182278\n",
      "Epoch 155/400: Train Loss: 2918.6937205578247 Test Loss: 2841.549319406027\n",
      "Epoch 156/400: Train Loss: 2918.656568698603 Test Loss: 2841.5911956030627\n",
      "Epoch 157/400: Train Loss: 2918.6194853621237 Test Loss: 2841.6323236399007\n",
      "Epoch 158/400: Train Loss: 2918.582469677392 Test Loss: 2841.67272215438\n",
      "Epoch 159/400: Train Loss: 2918.54552081897 Test Loss: 2841.7124093032844\n",
      "Epoch 160/400: Train Loss: 2918.5086380041457 Test Loss: 2841.751402773768\n",
      "Epoch 161/400: Train Loss: 2918.4718204902797 Test Loss: 2841.789719794596\n",
      "Epoch 162/400: Train Loss: 2918.435067572329 Test Loss: 2841.8273771471518\n",
      "Epoch 163/400: Train Loss: 2918.398378580522 Test Loss: 2841.8643911762383\n",
      "Epoch 164/400: Train Loss: 2918.3617528781906 Test Loss: 2841.9007778006717\n",
      "Epoch 165/400: Train Loss: 2918.3251898597287 Test Loss: 2841.93655252367\n",
      "Epoch 166/400: Train Loss: 2918.288688948694 Test Loss: 2841.971730443019\n",
      "Epoch 167/400: Train Loss: 2918.252249596022 Test Loss: 2842.0063262610593\n",
      "Epoch 168/400: Train Loss: 2918.2158712783544 Test Loss: 2842.0403542944487\n",
      "Epoch 169/400: Train Loss: 2918.179553496478 Test Loss: 2842.0738284837284\n",
      "Epoch 170/400: Train Loss: 2918.143295773857 Test Loss: 2842.106762402703\n",
      "Epoch 171/400: Train Loss: 2918.1070976552614 Test Loss: 2842.139169267607\n",
      "Epoch 172/400: Train Loss: 2918.070958705483 Test Loss: 2842.171061946087\n",
      "Epoch 173/400: Train Loss: 2918.0348785081296 Test Loss: 2842.202452965996\n",
      "Epoch 174/400: Train Loss: 2917.998856664495 Test Loss: 2842.23335452398\n",
      "Epoch 175/400: Train Loss: 2917.962892792505 Test Loss: 2842.2637784938997\n",
      "Epoch 176/400: Train Loss: 2917.9269865257243 Test Loss: 2842.293736435052\n",
      "Epoch 177/400: Train Loss: 2917.8911375124294 Test Loss: 2842.323239600213\n",
      "Epoch 178/400: Train Loss: 2917.855345414735 Test Loss: 2842.3522989435064\n",
      "Epoch 179/400: Train Loss: 2917.819609907784 Test Loss: 2842.380925128087\n",
      "Epoch 180/400: Train Loss: 2917.783930678975 Test Loss: 2842.4091285336553\n",
      "Epoch 181/400: Train Loss: 2917.748307427252 Test Loss: 2842.436919263797\n",
      "Epoch 182/400: Train Loss: 2917.712739862429 Test Loss: 2842.4643071531555\n",
      "Epoch 183/400: Train Loss: 2917.6772277045607 Test Loss: 2842.4913017744407\n",
      "Epoch 184/400: Train Loss: 2917.6417706833486 Test Loss: 2842.517912445259\n",
      "Epoch 185/400: Train Loss: 2917.606368537592 Test Loss: 2842.544148234806\n",
      "Epoch 186/400: Train Loss: 2917.5710210146617 Test Loss: 2842.5700179703877\n",
      "Epoch 187/400: Train Loss: 2917.535727870018 Test Loss: 2842.5955302437796\n",
      "Epoch 188/400: Train Loss: 2917.500488866748 Test Loss: 2842.620693417448\n",
      "Epoch 189/400: Train Loss: 2917.4653037751414 Test Loss: 2842.645515630612\n",
      "Epoch 190/400: Train Loss: 2917.4301723722815 Test Loss: 2842.6700048051634\n",
      "Epoch 191/400: Train Loss: 2917.3950944416747 Test Loss: 2842.6941686514383\n",
      "Epoch 192/400: Train Loss: 2917.360069772889 Test Loss: 2842.7180146738474\n",
      "Epoch 193/400: Train Loss: 2917.3250981612246 Test Loss: 2842.741550176373\n",
      "Epoch 194/400: Train Loss: 2917.2901794074005 Test Loss: 2842.764782267924\n",
      "Epoch 195/400: Train Loss: 2917.255313317261 Test Loss: 2842.787717867558\n",
      "Epoch 196/400: Train Loss: 2917.2204997015006 Test Loss: 2842.8103637095846\n",
      "Epoch 197/400: Train Loss: 2917.1857383754036 Test Loss: 2842.8327263485144\n",
      "Epoch 198/400: Train Loss: 2917.151029158605 Test Loss: 2842.8548121639133\n",
      "Epoch 199/400: Train Loss: 2917.116371874857 Test Loss: 2842.876627365116\n",
      "Epoch 200/400: Train Loss: 2917.081766351819 Test Loss: 2842.8981779958217\n",
      "Epoch 201/400: Train Loss: 2917.047212420855 Test Loss: 2842.9194699385816\n",
      "Epoch 202/400: Train Loss: 2917.012709916842 Test Loss: 2842.9405089191564\n",
      "Epoch 203/400: Train Loss: 2916.9782586779966 Test Loss: 2842.9613005107753\n",
      "Epoch 204/400: Train Loss: 2916.943858545703 Test Loss: 2842.981850138281\n",
      "Epoch 205/400: Train Loss: 2916.9095093643596 Test Loss: 2843.002163082162\n",
      "Epoch 206/400: Train Loss: 2916.875210981229 Test Loss: 2843.0222444824894\n",
      "Epoch 207/400: Train Loss: 2916.8409632463017 Test Loss: 2843.042099342741\n",
      "Epoch 208/400: Train Loss: 2916.806766012161 Test Loss: 2843.061732533539\n",
      "Epoch 209/400: Train Loss: 2916.7726191338656 Test Loss: 2843.0811487962733\n",
      "Epoch 210/400: Train Loss: 2916.738522468829 Test Loss: 2843.1003527466423\n",
      "Epoch 211/400: Train Loss: 2916.7044758767142 Test Loss: 2843.1193488780946\n",
      "Epoch 212/400: Train Loss: 2916.6704792193286 Test Loss: 2843.1381415651813\n",
      "Epoch 213/400: Train Loss: 2916.6365323605305 Test Loss: 2843.1567350668215\n",
      "Epoch 214/400: Train Loss: 2916.6026351661353 Test Loss: 2843.1751335294744\n",
      "Epoch 215/400: Train Loss: 2916.568787503832 Test Loss: 2843.1933409902335\n",
      "Epoch 216/400: Train Loss: 2916.534989243103 Test Loss: 2843.2113613798388\n",
      "Epoch 217/400: Train Loss: 2916.501240255145 Test Loss: 2843.2291985255993\n",
      "Epoch 218/400: Train Loss: 2916.467540412803 Test Loss: 2843.24685615425\n",
      "Epoch 219/400: Train Loss: 2916.433889590496 Test Loss: 2843.2643378947205\n",
      "Epoch 220/400: Train Loss: 2916.4002876641616 Test Loss: 2843.2816472808377\n",
      "Epoch 221/400: Train Loss: 2916.366734511187 Test Loss: 2843.298787753949\n",
      "Epoch 222/400: Train Loss: 2916.3332300103602 Test Loss: 2843.3157626654797\n",
      "Epoch 223/400: Train Loss: 2916.2997740418123 Test Loss: 2843.3325752794194\n",
      "Epoch 224/400: Train Loss: 2916.266366486968 Test Loss: 2843.349228774737\n",
      "Epoch 225/400: Train Loss: 2916.2330072284985 Test Loss: 2843.3657262477345\n",
      "Epoch 226/400: Train Loss: 2916.1996961502778 Test Loss: 2843.3820707143436\n",
      "Epoch 227/400: Train Loss: 2916.166433137336 Test Loss: 2843.398265112336\n",
      "Epoch 228/400: Train Loss: 2916.133218075823 Test Loss: 2843.414312303508\n",
      "Epoch 229/400: Train Loss: 2916.100050852973 Test Loss: 2843.4302150757694\n",
      "Epoch 230/400: Train Loss: 2916.0669313570584 Test Loss: 2843.4459761452017\n",
      "Epoch 231/400: Train Loss: 2916.033859477368 Test Loss: 2843.461598158049\n",
      "Epoch 232/400: Train Loss: 2916.000835104167 Test Loss: 2843.4770836926537\n",
      "Epoch 233/400: Train Loss: 2915.9678581286694 Test Loss: 2843.4924352613434\n",
      "Epoch 234/400: Train Loss: 2915.934928443009 Test Loss: 2843.507655312257\n",
      "Epoch 235/400: Train Loss: 2915.9020459402113 Test Loss: 2843.522746231137\n",
      "Epoch 236/400: Train Loss: 2915.8692105141677 Test Loss: 2843.53771034305\n",
      "Epoch 237/400: Train Loss: 2915.836422059612 Test Loss: 2843.5525499140817\n",
      "Epoch 238/400: Train Loss: 2915.803680472095 Test Loss: 2843.567267152967\n",
      "Epoch 239/400: Train Loss: 2915.770985647965 Test Loss: 2843.5818642126883\n",
      "Epoch 240/400: Train Loss: 2915.7383374843444 Test Loss: 2843.5963431920195\n",
      "Epoch 241/400: Train Loss: 2915.7057358791103 Test Loss: 2843.6107061370344\n",
      "Epoch 242/400: Train Loss: 2915.6731807308774 Test Loss: 2843.6249550425678\n",
      "Epoch 243/400: Train Loss: 2915.6406719389756 Test Loss: 2843.6390918536413\n",
      "Epoch 244/400: Train Loss: 2915.6082094034377 Test Loss: 2843.6531184668424\n",
      "Epoch 245/400: Train Loss: 2915.5757930249797 Test Loss: 2843.6670367316747\n",
      "Epoch 246/400: Train Loss: 2915.5434227049855 Test Loss: 2843.680848451858\n",
      "Epoch 247/400: Train Loss: 2915.5110983454933 Test Loss: 2843.6945553866044\n",
      "Epoch 248/400: Train Loss: 2915.47881984918 Test Loss: 2843.708159251853\n",
      "Epoch 249/400: Train Loss: 2915.446587119349 Test Loss: 2843.721661721462\n",
      "Epoch 250/400: Train Loss: 2915.4144000599154 Test Loss: 2843.7350644283915\n",
      "Epoch 251/400: Train Loss: 2915.382258575395 Test Loss: 2843.748368965822\n",
      "Epoch 252/400: Train Loss: 2915.3501625708927 Test Loss: 2843.7615768882656\n",
      "Epoch 253/400: Train Loss: 2915.318111952088 Test Loss: 2843.7746897126362\n",
      "Epoch 254/400: Train Loss: 2915.286106625229 Test Loss: 2843.7877089192857\n",
      "Epoch 255/400: Train Loss: 2915.2541464971177 Test Loss: 2843.8006359530236\n",
      "Epoch 256/400: Train Loss: 2915.222231475102 Test Loss: 2843.813472224098\n",
      "Epoch 257/400: Train Loss: 2915.1903614670664 Test Loss: 2843.826219109148\n",
      "Epoch 258/400: Train Loss: 2915.158536381421 Test Loss: 2843.8388779521374\n",
      "Epoch 259/400: Train Loss: 2915.126756127094 Test Loss: 2843.851450065256\n",
      "Epoch 260/400: Train Loss: 2915.095020613524 Test Loss: 2843.863936729795\n",
      "Epoch 261/400: Train Loss: 2915.063329750648 Test Loss: 2843.8763391970056\n",
      "Epoch 262/400: Train Loss: 2915.031683448899 Test Loss: 2843.8886586889207\n",
      "Epoch 263/400: Train Loss: 2915.000081619193 Test Loss: 2843.900896399166\n",
      "Epoch 264/400: Train Loss: 2914.9685241729267 Test Loss: 2843.913053493735\n",
      "Epoch 265/400: Train Loss: 2914.937011021964 Test Loss: 2843.9251311117605\n",
      "Epoch 266/400: Train Loss: 2914.905542078636 Test Loss: 2843.937130366245\n",
      "Epoch 267/400: Train Loss: 2914.87411725573 Test Loss: 2843.949052344782\n",
      "Epoch 268/400: Train Loss: 2914.8427364664844 Test Loss: 2843.960898110253\n",
      "Epoch 269/400: Train Loss: 2914.811399624583 Test Loss: 2843.9726687015077\n",
      "Epoch 270/400: Train Loss: 2914.7801066441484 Test Loss: 2843.9843651340175\n",
      "Epoch 271/400: Train Loss: 2914.7488574397335 Test Loss: 2843.9959884005243\n",
      "Epoch 272/400: Train Loss: 2914.717651926322 Test Loss: 2844.0075394716528\n",
      "Epoch 273/400: Train Loss: 2914.6864900193186 Test Loss: 2844.0190192965283\n",
      "Epoch 274/400: Train Loss: 2914.655371634543 Test Loss: 2844.030428803345\n",
      "Epoch 275/400: Train Loss: 2914.624296688227 Test Loss: 2844.0417688999555\n",
      "Epoch 276/400: Train Loss: 2914.5932650970085 Test Loss: 2844.053040474417\n",
      "Epoch 277/400: Train Loss: 2914.5622767779273 Test Loss: 2844.0642443955267\n",
      "Epoch 278/400: Train Loss: 2914.5313316484194 Test Loss: 2844.0753815133558\n",
      "Epoch 279/400: Train Loss: 2914.500429626313 Test Loss: 2844.0864526597456\n",
      "Epoch 280/400: Train Loss: 2914.4695706298235 Test Loss: 2844.097458648814\n",
      "Epoch 281/400: Train Loss: 2914.4387545775485 Test Loss: 2844.108400277427\n",
      "Epoch 282/400: Train Loss: 2914.4079813884664 Test Loss: 2844.1192783256674\n",
      "Epoch 283/400: Train Loss: 2914.3772509819273 Test Loss: 2844.130093557293\n",
      "Epoch 284/400: Train Loss: 2914.346563277654 Test Loss: 2844.140846720172\n",
      "Epoch 285/400: Train Loss: 2914.3159181957344 Test Loss: 2844.15153854671\n",
      "Epoch 286/400: Train Loss: 2914.2853156566193 Test Loss: 2844.1621697542746\n",
      "Epoch 287/400: Train Loss: 2914.254755581118 Test Loss: 2844.172741045588\n",
      "Epoch 288/400: Train Loss: 2914.2242378903948 Test Loss: 2844.1832531091272\n",
      "Epoch 289/400: Train Loss: 2914.193762505965 Test Loss: 2844.1937066195055\n",
      "Epoch 290/400: Train Loss: 2914.1633293496916 Test Loss: 2844.204102237839\n",
      "Epoch 291/400: Train Loss: 2914.132938343781 Test Loss: 2844.214440612108\n",
      "Epoch 292/400: Train Loss: 2914.1025894107815 Test Loss: 2844.2247223775116\n",
      "Epoch 293/400: Train Loss: 2914.072282473577 Test Loss: 2844.2349481568026\n",
      "Epoch 294/400: Train Loss: 2914.042017455387 Test Loss: 2844.245118560618\n",
      "Epoch 295/400: Train Loss: 2914.01179427976 Test Loss: 2844.255234187802\n",
      "Epoch 296/400: Train Loss: 2913.9816128705716 Test Loss: 2844.265295625721\n",
      "Epoch 297/400: Train Loss: 2913.9514731520244 Test Loss: 2844.2753034505545\n",
      "Epoch 298/400: Train Loss: 2913.921375048638 Test Loss: 2844.2852582276087\n",
      "Epoch 299/400: Train Loss: 2913.891318485254 Test Loss: 2844.295160511583\n",
      "Epoch 300/400: Train Loss: 2913.861303387027 Test Loss: 2844.305010846861\n",
      "Epoch 301/400: Train Loss: 2913.831329679423 Test Loss: 2844.314809767776\n",
      "Epoch 302/400: Train Loss: 2913.8013972882195 Test Loss: 2844.3245577988732\n",
      "Epoch 303/400: Train Loss: 2913.771506139499 Test Loss: 2844.3342554551677\n",
      "Epoch 304/400: Train Loss: 2913.741656159646 Test Loss: 2844.343903242382\n",
      "Epoch 305/400: Train Loss: 2913.711847275349 Test Loss: 2844.353501657209\n",
      "Epoch 306/400: Train Loss: 2913.682079413594 Test Loss: 2844.3630511875153\n",
      "Epoch 307/400: Train Loss: 2913.6523525016596 Test Loss: 2844.372552312598\n",
      "Epoch 308/400: Train Loss: 2913.62266646712 Test Loss: 2844.3820055033816\n",
      "Epoch 309/400: Train Loss: 2913.593021237838 Test Loss: 2844.391411222653\n",
      "Epoch 310/400: Train Loss: 2913.5634167419653 Test Loss: 2844.4007699252516\n",
      "Epoch 311/400: Train Loss: 2913.533852907937 Test Loss: 2844.4100820582885\n",
      "Epoch 312/400: Train Loss: 2913.504329664473 Test Loss: 2844.419348061328\n",
      "Epoch 313/400: Train Loss: 2913.474846940571 Test Loss: 2844.428568366592\n",
      "Epoch 314/400: Train Loss: 2913.4454046655073 Test Loss: 2844.4377433991367\n",
      "Epoch 315/400: Train Loss: 2913.416002768834 Test Loss: 2844.44687357704\n",
      "Epoch 316/400: Train Loss: 2913.3866411803756 Test Loss: 2844.4559593115714\n",
      "Epoch 317/400: Train Loss: 2913.3573198302265 Test Loss: 2844.465001007366\n",
      "Epoch 318/400: Train Loss: 2913.328038648751 Test Loss: 2844.4739990625894\n",
      "Epoch 319/400: Train Loss: 2913.2987975665774 Test Loss: 2844.4829538690983\n",
      "Epoch 320/400: Train Loss: 2913.2695965146004 Test Loss: 2844.491865812599\n",
      "Epoch 321/400: Train Loss: 2913.240435423973 Test Loss: 2844.500735272795\n",
      "Epoch 322/400: Train Loss: 2913.2113142261105 Test Loss: 2844.509562623541\n",
      "Epoch 323/400: Train Loss: 2913.182232852683 Test Loss: 2844.518348232982\n",
      "Epoch 324/400: Train Loss: 2913.1531912356163 Test Loss: 2844.527092463695\n",
      "Epoch 325/400: Train Loss: 2913.1241893070915 Test Loss: 2844.535795672824\n",
      "Epoch 326/400: Train Loss: 2913.0952269995373 Test Loss: 2844.54445821221\n",
      "Epoch 327/400: Train Loss: 2913.066304245632 Test Loss: 2844.553080428526\n",
      "Epoch 328/400: Train Loss: 2913.037420978302 Test Loss: 2844.5616626633914\n",
      "Epoch 329/400: Train Loss: 2913.008577130717 Test Loss: 2844.5702052535034\n",
      "Epoch 330/400: Train Loss: 2912.9797726362904 Test Loss: 2844.578708530747\n",
      "Epoch 331/400: Train Loss: 2912.951007428676 Test Loss: 2844.587172822311\n",
      "Epoch 332/400: Train Loss: 2912.9222814417653 Test Loss: 2844.5955984508037\n",
      "Epoch 333/400: Train Loss: 2912.893594609689 Test Loss: 2844.6039857343562\n",
      "Epoch 334/400: Train Loss: 2912.8649468668104 Test Loss: 2844.6123349867285\n",
      "Epoch 335/400: Train Loss: 2912.836338147728 Test Loss: 2844.6206465174137\n",
      "Epoch 336/400: Train Loss: 2912.80776838727 Test Loss: 2844.628920631732\n",
      "Epoch 337/400: Train Loss: 2912.779237520495 Test Loss: 2844.637157630935\n",
      "Epoch 338/400: Train Loss: 2912.7507454826887 Test Loss: 2844.645357812295\n",
      "Epoch 339/400: Train Loss: 2912.722292209363 Test Loss: 2844.653521469192\n",
      "Epoch 340/400: Train Loss: 2912.6938776362526 Test Loss: 2844.661648891215\n",
      "Epoch 341/400: Train Loss: 2912.6655016993163 Test Loss: 2844.669740364232\n",
      "Epoch 342/400: Train Loss: 2912.637164334733 Test Loss: 2844.677796170488\n",
      "Epoch 343/400: Train Loss: 2912.608865478899 Test Loss: 2844.6858165886756\n",
      "Epoch 344/400: Train Loss: 2912.5806050684287 Test Loss: 2844.6938018940236\n",
      "Epoch 345/400: Train Loss: 2912.5523830401517 Test Loss: 2844.7017523583654\n",
      "Epoch 346/400: Train Loss: 2912.5241993311124 Test Loss: 2844.7096682502206\n",
      "Epoch 347/400: Train Loss: 2912.4960538785654 Test Loss: 2844.717549834866\n",
      "Epoch 348/400: Train Loss: 2912.4679466199773 Test Loss: 2844.725397374403\n",
      "Epoch 349/400: Train Loss: 2912.4398774930223 Test Loss: 2844.73321112783\n",
      "Epoch 350/400: Train Loss: 2912.411846435583 Test Loss: 2844.74099135111\n",
      "Epoch 351/400: Train Loss: 2912.383853385746 Test Loss: 2844.7487382972345\n",
      "Epoch 352/400: Train Loss: 2912.355898281805 Test Loss: 2844.7564522162816\n",
      "Epoch 353/400: Train Loss: 2912.3279810622535 Test Loss: 2844.7641333554902\n",
      "Epoch 354/400: Train Loss: 2912.3001016657868 Test Loss: 2844.771781959305\n",
      "Epoch 355/400: Train Loss: 2912.272260031299 Test Loss: 2844.7793982694457\n",
      "Epoch 356/400: Train Loss: 2912.2444560978847 Test Loss: 2844.7869825249622\n",
      "Epoch 357/400: Train Loss: 2912.2166898048317 Test Loss: 2844.794534962284\n",
      "Epoch 358/400: Train Loss: 2912.1889610916246 Test Loss: 2844.80205581528\n",
      "Epoch 359/400: Train Loss: 2912.161269897942 Test Loss: 2844.8095453153046\n",
      "Epoch 360/400: Train Loss: 2912.1336161636536 Test Loss: 2844.8170036912547\n",
      "Epoch 361/400: Train Loss: 2912.1059998288197 Test Loss: 2844.8244311696176\n",
      "Epoch 362/400: Train Loss: 2912.078420833691 Test Loss: 2844.831827974515\n",
      "Epoch 363/400: Train Loss: 2912.0508791187053 Test Loss: 2844.8391943277543\n",
      "Epoch 364/400: Train Loss: 2912.023374624486 Test Loss: 2844.8465304488723\n",
      "Epoch 365/400: Train Loss: 2911.9959072918446 Test Loss: 2844.853836555179\n",
      "Epoch 366/400: Train Loss: 2911.9684770617732 Test Loss: 2844.8611128618036\n",
      "Epoch 367/400: Train Loss: 2911.9410838754475 Test Loss: 2844.8683595817297\n",
      "Epoch 368/400: Train Loss: 2911.913727674225 Test Loss: 2844.8755769258437\n",
      "Epoch 369/400: Train Loss: 2911.8864083996423 Test Loss: 2844.882765102972\n",
      "Epoch 370/400: Train Loss: 2911.8591259934137 Test Loss: 2844.889924319916\n",
      "Epoch 371/400: Train Loss: 2911.8318803974325 Test Loss: 2844.897054781497\n",
      "Epoch 372/400: Train Loss: 2911.8046715537666 Test Loss: 2844.9041566905844\n",
      "Epoch 373/400: Train Loss: 2911.777499404659 Test Loss: 2844.9112302481385\n",
      "Epoch 374/400: Train Loss: 2911.750363892526 Test Loss: 2844.9182756532396\n",
      "Epoch 375/400: Train Loss: 2911.723264959956 Test Loss: 2844.9252931031274\n",
      "Epoch 376/400: Train Loss: 2911.6962025497082 Test Loss: 2844.9322827932288\n",
      "Epoch 377/400: Train Loss: 2911.669176604713 Test Loss: 2844.9392449171937\n",
      "Epoch 378/400: Train Loss: 2911.6421870680656 Test Loss: 2844.9461796669225\n",
      "Epoch 379/400: Train Loss: 2911.615233883033 Test Loss: 2844.953087232603\n",
      "Epoch 380/400: Train Loss: 2911.588316993046 Test Loss: 2844.9599678027307\n",
      "Epoch 381/400: Train Loss: 2911.5614363417008 Test Loss: 2844.9668215641464\n",
      "Epoch 382/400: Train Loss: 2911.5345918727567 Test Loss: 2844.9736487020587\n",
      "Epoch 383/400: Train Loss: 2911.5077835301363 Test Loss: 2844.9804494000746\n",
      "Epoch 384/400: Train Loss: 2911.4810112579244 Test Loss: 2844.9872238402218\n",
      "Epoch 385/400: Train Loss: 2911.4542750003648 Test Loss: 2844.99397220298\n",
      "Epoch 386/400: Train Loss: 2911.4275747018623 Test Loss: 2845.000694667305\n",
      "Epoch 387/400: Train Loss: 2911.4009103069775 Test Loss: 2845.007391410651\n",
      "Epoch 388/400: Train Loss: 2911.3742817604316 Test Loss: 2845.014062608995\n",
      "Epoch 389/400: Train Loss: 2911.347689007098 Test Loss: 2845.020708436861\n",
      "Epoch 390/400: Train Loss: 2911.3211319920083 Test Loss: 2845.0273290673463\n",
      "Epoch 391/400: Train Loss: 2911.2946106603454 Test Loss: 2845.033924672137\n",
      "Epoch 392/400: Train Loss: 2911.2681249574466 Test Loss: 2845.040495421534\n",
      "Epoch 393/400: Train Loss: 2911.241674828801 Test Loss: 2845.047041484474\n",
      "Epoch 394/400: Train Loss: 2911.215260220048 Test Loss: 2845.05356302855\n",
      "Epoch 395/400: Train Loss: 2911.1888810769774 Test Loss: 2845.060060220028\n",
      "Epoch 396/400: Train Loss: 2911.1625373455267 Test Loss: 2845.0665332238746\n",
      "Epoch 397/400: Train Loss: 2911.1362289717827 Test Loss: 2845.0729822037665\n",
      "Epoch 398/400: Train Loss: 2911.1099559019763 Test Loss: 2845.0794073221164\n",
      "Epoch 399/400: Train Loss: 2911.0837180824883 Test Loss: 2845.0858087400857\n",
      "Epoch 400/400: Train Loss: 2911.0575154598405 Test Loss: 2845.092186617608\n"
     ]
    }
   ],
   "source": [
    "fit(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1e5a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
