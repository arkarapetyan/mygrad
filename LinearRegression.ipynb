{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e8177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mygrad\n",
    "import mygrad.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e23983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_diabetes(return_X_y = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = mygrad.value(X_train, \"X_train\", requires_grad=False)\n",
    "X_test = mygrad.value(X_test, \"X_test\", requires_grad=False)\n",
    "y_train = mygrad.value(y_train, \"y_train\", requires_grad=False)\n",
    "y_test = mygrad.value(y_test, \"y_test\", requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e18c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = mygrad.value(np.random.randn(X.shape[1]) * 10**(-1), \"w\", requires_grad = True)\n",
    "b = mygrad.value(np.array([np.mean(y_train.value)]), \"b\", requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "306489fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Function of type <class 'mygrad.functional.function.Matmul'> with name matmul_0\n",
      "Creating Function of type <class 'mygrad.functional.function.Add'> with name add_0\n",
      "Creating Function of type <class 'mygrad.functional.function.MSELoss'> with name mse_loss_0\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "\n",
    "matmul1 = F.matmul()\n",
    "add1 = F.add()\n",
    "loss = F.mse_loss()\n",
    "    \n",
    "def fit(num_epoch):     \n",
    "    for i in range(num_epoch):\n",
    "        \n",
    "        y = add1.forward(matmul1.forward(X_train, w), b)\n",
    "        l = loss.forward(y, y_train)\n",
    "        loss.backward()\n",
    "        l_test = loss.forward(add1.forward(matmul1.forward(X_test, w), b), y_test)\n",
    "        w.value -= lr * w.grad\n",
    "        b.value -= lr * b.grad\n",
    "        l.zero_grad()   \n",
    "        \n",
    "        print(f\"Epoch {i+1}/{num_epoch}: Train Loss: {l.value} Test Loss: {l_test.value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b840b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400: Train Loss: 5908.847604853107 Test Loss: 5969.600473614674\n",
      "Epoch 2/400: Train Loss: 4956.583360710853 Test Loss: 4875.468053610731\n",
      "Epoch 3/400: Train Loss: 4384.507290808055 Test Loss: 4223.3863294774055\n",
      "Epoch 4/400: Train Loss: 4025.3748166305145 Test Loss: 3821.015786259402\n",
      "Epoch 5/400: Train Loss: 3787.9205603851638 Test Loss: 3562.5115072991284\n",
      "Epoch 6/400: Train Loss: 3621.989162492801 Test Loss: 3389.090513061613\n",
      "Epoch 7/400: Train Loss: 3499.7199371531397 Test Loss: 3267.6825645241565\n",
      "Epoch 8/400: Train Loss: 3405.3865237742193 Test Loss: 3179.3489279557193\n",
      "Epoch 9/400: Train Loss: 3329.903808478778 Test Loss: 3112.9825437116624\n",
      "Epoch 10/400: Train Loss: 3267.8526849883096 Test Loss: 3061.867686415448\n",
      "Epoch 11/400: Train Loss: 3215.8639343262134 Test Loss: 3021.7908035296123\n",
      "Epoch 12/400: Train Loss: 3171.736449237773 Test Loss: 2989.995084568551\n",
      "Epoch 13/400: Train Loss: 3133.952821709895 Test Loss: 2964.5956930836223\n",
      "Epoch 14/400: Train Loss: 3101.4104466297513 Test Loss: 2944.2478398192657\n",
      "Epoch 15/400: Train Loss: 3073.2699342897863 Test Loss: 2927.9546485233836\n",
      "Epoch 16/400: Train Loss: 3048.8677301087982 Test Loss: 2914.9530913469766\n",
      "Epoch 17/400: Train Loss: 3027.66417482029 Test Loss: 2904.6441208466254\n",
      "Epoch 18/400: Train Loss: 3009.2113756926096 Test Loss: 2896.5482782199197\n",
      "Epoch 19/400: Train Loss: 2993.1323586851304 Test Loss: 2890.276332166943\n",
      "Epoch 20/400: Train Loss: 2979.1068138524633 Test Loss: 2885.509044537552\n",
      "Epoch 21/400: Train Loss: 2966.8608309584993 Test Loss: 2881.98266901834\n",
      "Epoch 22/400: Train Loss: 2956.159157464688 Test Loss: 2879.478189323257\n",
      "Epoch 23/400: Train Loss: 2946.799132756655 Test Loss: 2877.8130941458435\n",
      "Epoch 24/400: Train Loss: 2938.605795893122 Test Loss: 2876.8349399984386\n",
      "Epoch 25/400: Train Loss: 2931.4278563093617 Test Loss: 2876.416218938011\n",
      "Epoch 26/400: Train Loss: 2925.134326436288 Test Loss: 2876.4502078843775\n",
      "Epoch 27/400: Train Loss: 2919.6116792766406 Test Loss: 2876.847575042824\n",
      "Epoch 28/400: Train Loss: 2914.7614327891424 Test Loss: 2877.5335821242106\n",
      "Epoch 29/400: Train Loss: 2910.4980874664366 Test Loss: 2878.4457628937434\n",
      "Epoch 30/400: Train Loss: 2906.7473597820713 Test Loss: 2879.5319872691007\n",
      "Epoch 31/400: Train Loss: 2903.4446655613774 Test Loss: 2880.748840521938\n",
      "Epoch 32/400: Train Loss: 2900.5338156709654 Test Loss: 2882.060261981854\n",
      "Epoch 33/400: Train Loss: 2897.9658927907744 Test Loss: 2883.4363987608867\n",
      "Epoch 34/400: Train Loss: 2895.6982830586467 Test Loss: 2884.8526385252353\n",
      "Epoch 35/400: Train Loss: 2893.693840441629 Test Loss: 2886.288791967648\n",
      "Epoch 36/400: Train Loss: 2891.920165032753 Test Loss: 2887.7284008694182\n",
      "Epoch 37/400: Train Loss: 2890.348979257953 Test Loss: 2889.1581518259336\n",
      "Epoch 38/400: Train Loss: 2888.955588317906 Test Loss: 2890.5673790873207\n",
      "Epoch 39/400: Train Loss: 2887.718413166563 Test Loss: 2891.947642713257\n",
      "Epoch 40/400: Train Loss: 2886.6185860049873 Test Loss: 2893.2923704912096\n",
      "Epoch 41/400: Train Loss: 2885.639599695545 Test Loss: 2894.59655392063\n",
      "Epoch 42/400: Train Loss: 2884.7670037173525 Test Loss: 2895.8564900995575\n",
      "Epoch 43/400: Train Loss: 2883.9881403220297 Test Loss: 2897.0695626250686\n",
      "Epoch 44/400: Train Loss: 2883.291915436337 Test Loss: 2898.234055682828\n",
      "Epoch 45/400: Train Loss: 2882.6685996178967 Test Loss: 2899.348996391456\n",
      "Epoch 46/400: Train Loss: 2882.1096550209963 Test Loss: 2900.4140212150774\n",
      "Epoch 47/400: Train Loss: 2881.607584887505 Test Loss: 2901.4292628866756\n",
      "Epoch 48/400: Train Loss: 2881.155802556863 Test Loss: 2902.3952548158977\n",
      "Epoch 49/400: Train Loss: 2880.748517400453 Test Loss: 2903.3128504037777\n",
      "Epoch 50/400: Train Loss: 2880.3806354392177 Test Loss: 2904.1831550671723\n",
      "Epoch 51/400: Train Loss: 2880.0476727075293 Test Loss: 2905.007469098275\n",
      "Epoch 52/400: Train Loss: 2879.7456796881147 Test Loss: 2905.7872397587416\n",
      "Epoch 53/400: Train Loss: 2879.471175368367 Test Loss: 2906.5240212412114\n",
      "Epoch 54/400: Train Loss: 2879.2210896627557 Test Loss: 2907.219441329768\n",
      "Epoch 55/400: Train Loss: 2878.992713113735 Test Loss: 2907.8751737602724\n",
      "Epoch 56/400: Train Loss: 2878.7836529283045 Test Loss: 2908.492915426263\n",
      "Epoch 57/400: Train Loss: 2878.59179453234 Test Loss: 2909.074367699628\n",
      "Epoch 58/400: Train Loss: 2878.41526793292 Test Loss: 2909.6212212409923\n",
      "Epoch 59/400: Train Loss: 2878.252418272222 Test Loss: 2910.1351437651333\n",
      "Epoch 60/400: Train Loss: 2878.101780037485 Test Loss: 2910.6177703041785\n",
      "Epoch 61/400: Train Loss: 2877.9620544614722 Test Loss: 2911.070695577568\n",
      "Epoch 62/400: Train Loss: 2877.8320897085555 Test Loss: 2911.495468134647\n",
      "Epoch 63/400: Train Loss: 2877.7108634940646 Test Loss: 2911.89358598438\n",
      "Epoch 64/400: Train Loss: 2877.5974678301695 Test Loss: 2912.2664934684394\n",
      "Epoch 65/400: Train Loss: 2877.4910956310873 Test Loss: 2912.6155791697224\n",
      "Epoch 66/400: Train Loss: 2877.39102894479 Test Loss: 2912.942174679037\n",
      "Epoch 67/400: Train Loss: 2877.2966286081764 Test Loss: 2913.247554069046\n",
      "Epoch 68/400: Train Loss: 2877.207325148652 Test Loss: 2913.5329339471064\n",
      "Epoch 69/400: Train Loss: 2877.1226107775583 Test Loss: 2913.7994739780215\n",
      "Epoch 70/400: Train Loss: 2877.0420323405483 Test Loss: 2914.0482777843486\n",
      "Epoch 71/400: Train Loss: 2876.9651851070535 Test Loss: 2914.2803941460534\n",
      "Epoch 72/400: Train Loss: 2876.891707295844 Test Loss: 2914.496818433587\n",
      "Epoch 73/400: Train Loss: 2876.8212752466784 Test Loss: 2914.6984942188237\n",
      "Epoch 74/400: Train Loss: 2876.7535991592636 Test Loss: 2914.886315017254\n",
      "Epoch 75/400: Train Loss: 2876.688419330645 Test Loss: 2915.0611261224826\n",
      "Epoch 76/400: Train Loss: 2876.625502830701 Test Loss: 2915.223726500547\n",
      "Epoch 77/400: Train Loss: 2876.564640562903 Test Loss: 2915.374870717232\n",
      "Epoch 78/400: Train Loss: 2876.5056446640833 Test Loss: 2915.515270876197\n",
      "Epoch 79/400: Train Loss: 2876.4483462026096 Test Loss: 2915.645598549847\n",
      "Epoch 80/400: Train Loss: 2876.3925931394137 Test Loss: 2915.7664866883033\n",
      "Epoch 81/400: Train Loss: 2876.338248520643 Test Loss: 2915.878531494687\n",
      "Epoch 82/400: Train Loss: 2876.285188874549 Test Loss: 2915.982294257488\n",
      "Epoch 83/400: Train Loss: 2876.233302788552 Test Loss: 2916.078303132769\n",
      "Epoch 84/400: Train Loss: 2876.182489645343 Test Loss: 2916.1670548708034\n",
      "Epoch 85/400: Train Loss: 2876.132658499458 Test Loss: 2916.249016483145\n",
      "Epoch 86/400: Train Loss: 2876.083727077979 Test Loss: 2916.3246268474086\n",
      "Epoch 87/400: Train Loss: 2876.035620890998 Test Loss: 2916.3942982480144\n",
      "Epoch 88/400: Train Loss: 2875.9882724392023 Test Loss: 2916.458417852064\n",
      "Epoch 89/400: Train Loss: 2875.941620507428 Test Loss: 2916.5173491201344\n",
      "Epoch 90/400: Train Loss: 2875.8956095343856 Test Loss: 2916.571433152412\n",
      "Epoch 91/400: Train Loss: 2875.850189049899 Test Loss: 2916.620989971006\n",
      "Epoch 92/400: Train Loss: 2875.8053131720353 Test Loss: 2916.6663197396806\n",
      "Epoch 93/400: Train Loss: 2875.7609401573914 Test Loss: 2916.707703922509\n",
      "Epoch 94/400: Train Loss: 2875.717031998593 Test Loss: 2916.7454063831683\n",
      "Epoch 95/400: Train Loss: 2875.6735540637646 Test Loss: 2916.779674426824\n",
      "Epoch 96/400: Train Loss: 2875.630474773323 Test Loss: 2916.8107397865824\n",
      "Epoch 97/400: Train Loss: 2875.587765309996 Test Loss: 2916.838819556643\n",
      "Epoch 98/400: Train Loss: 2875.5453993584365 Test Loss: 2916.8641170742985\n",
      "Epoch 99/400: Train Loss: 2875.503352871212 Test Loss: 2916.88682275297\n",
      "Epoch 100/400: Train Loss: 2875.4616038583345 Test Loss: 2916.9071148684448\n",
      "Epoch 101/400: Train Loss: 2875.4201321977885 Test Loss: 2916.925160300479\n",
      "Epoch 102/400: Train Loss: 2875.3789194648357 Test Loss: 2916.9411152319076\n",
      "Epoch 103/400: Train Loss: 2875.3379487781 Test Loss: 2916.9551258073398\n",
      "Epoch 104/400: Train Loss: 2875.297204660666 Test Loss: 2916.967328753459\n",
      "Epoch 105/400: Train Loss: 2875.256672914631 Test Loss: 2916.9778519629285\n",
      "Epoch 106/400: Train Loss: 2875.2163405077044 Test Loss: 2916.9868150437906\n",
      "Epoch 107/400: Train Loss: 2875.1761954706226 Test Loss: 2916.9943298362004\n",
      "Epoch 108/400: Train Loss: 2875.1362268042653 Test Loss: 2917.0005008982766\n",
      "Epoch 109/400: Train Loss: 2875.0964243954963 Test Loss: 2917.005425962753\n",
      "Epoch 110/400: Train Loss: 2875.0567789408387 Test Loss: 2917.0091963660557\n",
      "Epoch 111/400: Train Loss: 2875.0172818772085 Test Loss: 2917.0118974513607\n",
      "Epoch 112/400: Train Loss: 2874.9779253190063 Test Loss: 2917.013608947112\n",
      "Epoch 113/400: Train Loss: 2874.938702000933 Test Loss: 2917.0144053223944\n",
      "Epoch 114/400: Train Loss: 2874.899605225968 Test Loss: 2917.01435612052\n",
      "Epoch 115/400: Train Loss: 2874.860628818027 Test Loss: 2917.0135262720773\n",
      "Epoch 116/400: Train Loss: 2874.821767078822 Test Loss: 2917.011976388662\n",
      "Epoch 117/400: Train Loss: 2874.7830147485424 Test Loss: 2917.0097630384275\n",
      "Epoch 118/400: Train Loss: 2874.744366969979 Test Loss: 2917.0069390045323\n",
      "Epoch 119/400: Train Loss: 2874.7058192557847 Test Loss: 2917.003553527503\n",
      "Epoch 120/400: Train Loss: 2874.667367458554 Test Loss: 2916.99965253249\n",
      "Epoch 121/400: Train Loss: 2874.629007743486 Test Loss: 2916.995278842319\n",
      "Epoch 122/400: Train Loss: 2874.590736563375 Test Loss: 2916.990472377196\n",
      "Epoch 123/400: Train Loss: 2874.55255063572 Test Loss: 2916.9852703418887\n",
      "Epoch 124/400: Train Loss: 2874.514446921766 Test Loss: 2916.979707401128\n",
      "Epoch 125/400: Train Loss: 2874.4764226072994 Test Loss: 2916.973815843974\n",
      "Epoch 126/400: Train Loss: 2874.438475085035 Test Loss: 2916.9676257378223\n",
      "Epoch 127/400: Train Loss: 2874.400601938469 Test Loss: 2916.961165072654\n",
      "Epoch 128/400: Train Loss: 2874.3628009270515 Test Loss: 2916.9544598961825\n",
      "Epoch 129/400: Train Loss: 2874.325069972572 Test Loss: 2916.947534440435\n",
      "Epoch 130/400: Train Loss: 2874.2874071466517 Test Loss: 2916.94041124028\n",
      "Epoch 131/400: Train Loss: 2874.2498106592498 Test Loss: 2916.9331112444484\n",
      "Epoch 132/400: Train Loss: 2874.212278848089 Test Loss: 2916.9256539194844\n",
      "Epoch 133/400: Train Loss: 2874.1748101689263 Test Loss: 2916.918057347059\n",
      "Epoch 134/400: Train Loss: 2874.1374031866026 Test Loss: 2916.910338315098\n",
      "Epoch 135/400: Train Loss: 2874.100056566797 Test Loss: 2916.902512403071\n",
      "Epoch 136/400: Train Loss: 2874.0627690684296 Test Loss: 2916.8945940618455\n",
      "Epoch 137/400: Train Loss: 2874.0255395366676 Test Loss: 2916.8865966884077\n",
      "Epoch 138/400: Train Loss: 2873.9883668964676 Test Loss: 2916.878532695811\n",
      "Epoch 139/400: Train Loss: 2873.9512501466265 Test Loss: 2916.870413578629\n",
      "Epoch 140/400: Train Loss: 2873.9141883542907 Test Loss: 2916.862249974203\n",
      "Epoch 141/400: Train Loss: 2873.8771806498917 Test Loss: 2916.8540517199517\n",
      "Epoch 142/400: Train Loss: 2873.8402262224636 Test Loss: 2916.8458279069846\n",
      "Epoch 143/400: Train Loss: 2873.803324315328 Test Loss: 2916.8375869302686\n",
      "Epoch 144/400: Train Loss: 2873.766474222102 Test Loss: 2916.829336535546\n",
      "Epoch 145/400: Train Loss: 2873.7296752830084 Test Loss: 2916.8210838632267\n",
      "Epoch 146/400: Train Loss: 2873.6929268814724 Test Loss: 2916.812835489435\n",
      "Epoch 147/400: Train Loss: 2873.6562284409683 Test Loss: 2916.8045974644\n",
      "Epoch 148/400: Train Loss: 2873.6195794221035 Test Loss: 2916.79637534835\n",
      "Epoch 149/400: Train Loss: 2873.5829793199273 Test Loss: 2916.788174245086\n",
      "Epoch 150/400: Train Loss: 2873.5464276614357 Test Loss: 2916.779998833352\n",
      "Epoch 151/400: Train Loss: 2873.509924003262 Test Loss: 2916.7718533961793\n",
      "Epoch 152/400: Train Loss: 2873.4734679295416 Test Loss: 2916.763741848308\n",
      "Epoch 153/400: Train Loss: 2873.437059049935 Test Loss: 2916.7556677618145\n",
      "Epoch 154/400: Train Loss: 2873.400696997797 Test Loss: 2916.7476343900657\n",
      "Epoch 155/400: Train Loss: 2873.364381428475 Test Loss: 2916.739644690104\n",
      "Epoch 156/400: Train Loss: 2873.328112017746 Test Loss: 2916.7317013435522\n",
      "Epoch 157/400: Train Loss: 2873.291888460353 Test Loss: 2916.7238067761577\n",
      "Epoch 158/400: Train Loss: 2873.2557104686593 Test Loss: 2916.7159631760383\n",
      "Epoch 159/400: Train Loss: 2873.219577771395 Test Loss: 2916.708172510727\n",
      "Epoch 160/400: Train Loss: 2873.183490112501 Test Loss: 2916.7004365430853\n",
      "Epoch 161/400: Train Loss: 2873.1474472500518 Test Loss: 2916.6927568461706\n",
      "Epoch 162/400: Train Loss: 2873.111448955258 Test Loss: 2916.6851348171044\n",
      "Epoch 163/400: Train Loss: 2873.0754950115406 Test Loss: 2916.677571690025\n",
      "Epoch 164/400: Train Loss: 2873.0395852136776 Test Loss: 2916.670068548176\n",
      "Epoch 165/400: Train Loss: 2873.003719367002 Test Loss: 2916.6626263351745\n",
      "Epoch 166/400: Train Loss: 2872.967897286667 Test Loss: 2916.6552458655465\n",
      "Epoch 167/400: Train Loss: 2872.9321187969604 Test Loss: 2916.6479278345264\n",
      "Epoch 168/400: Train Loss: 2872.8963837306683 Test Loss: 2916.6406728272173\n",
      "Epoch 169/400: Train Loss: 2872.8606919284834 Test Loss: 2916.633481327122\n",
      "Epoch 170/400: Train Loss: 2872.8250432384575 Test Loss: 2916.6263537241\n",
      "Epoch 171/400: Train Loss: 2872.7894375154924 Test Loss: 2916.619290321771\n",
      "Epoch 172/400: Train Loss: 2872.753874620868 Test Loss: 2916.612291344434\n",
      "Epoch 173/400: Train Loss: 2872.718354421802 Test Loss: 2916.6053569434935\n",
      "Epoch 174/400: Train Loss: 2872.6828767910433 Test Loss: 2916.598487203454\n",
      "Epoch 175/400: Train Loss: 2872.6474416064916 Test Loss: 2916.5916821474957\n",
      "Epoch 176/400: Train Loss: 2872.612048750848 Test Loss: 2916.584941742668\n",
      "Epoch 177/400: Train Loss: 2872.5766981112865 Test Loss: 2916.578265904723\n",
      "Epoch 178/400: Train Loss: 2872.541389579148 Test Loss: 2916.571654502598\n",
      "Epoch 179/400: Train Loss: 2872.5061230496626 Test Loss: 2916.5651073626054\n",
      "Epoch 180/400: Train Loss: 2872.4708984216823 Test Loss: 2916.5586242723066\n",
      "Epoch 181/400: Train Loss: 2872.435715597443 Test Loss: 2916.5522049841225\n",
      "Epoch 182/400: Train Loss: 2872.4005744823294 Test Loss: 2916.5458492186813\n",
      "Epoch 183/400: Train Loss: 2872.3654749846714 Test Loss: 2916.539556667934\n",
      "Epoch 184/400: Train Loss: 2872.330417015547 Test Loss: 2916.533326998035\n",
      "Epoch 185/400: Train Loss: 2872.295400488593 Test Loss: 2916.52715985202\n",
      "Epoch 186/400: Train Loss: 2872.260425319846 Test Loss: 2916.521054852284\n",
      "Epoch 187/400: Train Loss: 2872.2254914275773 Test Loss: 2916.5150116028863\n",
      "Epoch 188/400: Train Loss: 2872.1905987321475 Test Loss: 2916.50902969167\n",
      "Epoch 189/400: Train Loss: 2872.1557471558717 Test Loss: 2916.5031086922386\n",
      "Epoch 190/400: Train Loss: 2872.1209366228904 Test Loss: 2916.4972481657715\n",
      "Epoch 191/400: Train Loss: 2872.0861670590543 Test Loss: 2916.4914476627087\n",
      "Epoch 192/400: Train Loss: 2872.0514383918107 Test Loss: 2916.485706724304\n",
      "Epoch 193/400: Train Loss: 2872.0167505501026 Test Loss: 2916.480024884055\n",
      "Epoch 194/400: Train Loss: 2871.9821034642746 Test Loss: 2916.474401669033\n",
      "Epoch 195/400: Train Loss: 2871.947497065982 Test Loss: 2916.4688366010896\n",
      "Epoch 196/400: Train Loss: 2871.9129312881123 Test Loss: 2916.4633291979785\n",
      "Epoch 197/400: Train Loss: 2871.8784060647004 Test Loss: 2916.457878974389\n",
      "Epoch 198/400: Train Loss: 2871.8439213308666 Test Loss: 2916.452485442888\n",
      "Epoch 199/400: Train Loss: 2871.809477022742 Test Loss: 2916.447148114778\n",
      "Epoch 200/400: Train Loss: 2871.7750730774114 Test Loss: 2916.441866500901\n",
      "Epoch 201/400: Train Loss: 2871.7407094328537 Test Loss: 2916.436640112357\n",
      "Epoch 202/400: Train Loss: 2871.706386027887 Test Loss: 2916.4314684611622\n",
      "Epoch 203/400: Train Loss: 2871.6721028021198 Test Loss: 2916.4263510608585\n",
      "Epoch 204/400: Train Loss: 2871.6378596959025 Test Loss: 2916.421287427051\n",
      "Epoch 205/400: Train Loss: 2871.6036566502867 Test Loss: 2916.416277077914\n",
      "Epoch 206/400: Train Loss: 2871.569493606982 Test Loss: 2916.4113195346363\n",
      "Epoch 207/400: Train Loss: 2871.535370508318 Test Loss: 2916.406414321822\n",
      "Epoch 208/400: Train Loss: 2871.50128729721 Test Loss: 2916.4015609678645\n",
      "Epoch 209/400: Train Loss: 2871.4672439171272 Test Loss: 2916.3967590052644\n",
      "Epoch 210/400: Train Loss: 2871.4332403120593 Test Loss: 2916.3920079709314\n",
      "Epoch 211/400: Train Loss: 2871.3992764264876 Test Loss: 2916.387307406428\n",
      "Epoch 212/400: Train Loss: 2871.365352205362 Test Loss: 2916.3826568582213\n",
      "Epoch 213/400: Train Loss: 2871.3314675940724 Test Loss: 2916.3780558778626\n",
      "Epoch 214/400: Train Loss: 2871.297622538428 Test Loss: 2916.3735040221795\n",
      "Epoch 215/400: Train Loss: 2871.263816984632 Test Loss: 2916.36900085342\n",
      "Epoch 216/400: Train Loss: 2871.2300508792646 Test Loss: 2916.3645459393865\n",
      "Epoch 217/400: Train Loss: 2871.196324169263 Test Loss: 2916.360138853546\n",
      "Epoch 218/400: Train Loss: 2871.162636801904 Test Loss: 2916.355779175118\n",
      "Epoch 219/400: Train Loss: 2871.1289887247863 Test Loss: 2916.3514664891554\n",
      "Epoch 220/400: Train Loss: 2871.0953798858154 Test Loss: 2916.3472003866\n",
      "Epoch 221/400: Train Loss: 2871.0618102331914 Test Loss: 2916.3429804643233\n",
      "Epoch 222/400: Train Loss: 2871.0282797153914 Test Loss: 2916.338806325165\n",
      "Epoch 223/400: Train Loss: 2870.9947882811607 Test Loss: 2916.334677577944\n",
      "Epoch 224/400: Train Loss: 2870.9613358794986 Test Loss: 2916.330593837473\n",
      "Epoch 225/400: Train Loss: 2870.9279224596485 Test Loss: 2916.3265547245524\n",
      "Epoch 226/400: Train Loss: 2870.8945479710874 Test Loss: 2916.3225598659574\n",
      "Epoch 227/400: Train Loss: 2870.8612123635153 Test Loss: 2916.31860889442\n",
      "Epoch 228/400: Train Loss: 2870.8279155868486 Test Loss: 2916.3147014485985\n",
      "Epoch 229/400: Train Loss: 2870.7946575912074 Test Loss: 2916.3108371730455\n",
      "Epoch 230/400: Train Loss: 2870.761438326913 Test Loss: 2916.307015718165\n",
      "Epoch 231/400: Train Loss: 2870.7282577444757 Test Loss: 2916.3032367401647\n",
      "Epoch 232/400: Train Loss: 2870.695115794591 Test Loss: 2916.2994999010025\n",
      "Epoch 233/400: Train Loss: 2870.66201242813 Test Loss: 2916.295804868335\n",
      "Epoch 234/400: Train Loss: 2870.628947596137 Test Loss: 2916.2921513154497\n",
      "Epoch 235/400: Train Loss: 2870.595921249821 Test Loss: 2916.2885389212047\n",
      "Epoch 236/400: Train Loss: 2870.5629333405514 Test Loss: 2916.284967369961\n",
      "Epoch 237/400: Train Loss: 2870.5299838198525 Test Loss: 2916.2814363515095\n",
      "Epoch 238/400: Train Loss: 2870.4970726394004 Test Loss: 2916.2779455609993\n",
      "Epoch 239/400: Train Loss: 2870.464199751013 Test Loss: 2916.2744946988623\n",
      "Epoch 240/400: Train Loss: 2870.4313651066554 Test Loss: 2916.271083470735\n",
      "Epoch 241/400: Train Loss: 2870.398568658427 Test Loss: 2916.267711587383\n",
      "Epoch 242/400: Train Loss: 2870.3658103585617 Test Loss: 2916.2643787646175\n",
      "Epoch 243/400: Train Loss: 2870.333090159425 Test Loss: 2916.2610847232168\n",
      "Epoch 244/400: Train Loss: 2870.3004080135092 Test Loss: 2916.257829188841\n",
      "Epoch 245/400: Train Loss: 2870.2677638734303 Test Loss: 2916.2546118919554\n",
      "Epoch 246/400: Train Loss: 2870.2351576919264 Test Loss: 2916.2514325677444\n",
      "Epoch 247/400: Train Loss: 2870.2025894218536 Test Loss: 2916.2482909560235\n",
      "Epoch 248/400: Train Loss: 2870.170059016185 Test Loss: 2916.245186801167\n",
      "Epoch 249/400: Train Loss: 2870.1375664280054 Test Loss: 2916.2421198520155\n",
      "Epoch 250/400: Train Loss: 2870.105111610513 Test Loss: 2916.2390898617987\n",
      "Epoch 251/400: Train Loss: 2870.072694517012 Test Loss: 2916.2360965880484\n",
      "Epoch 252/400: Train Loss: 2870.0403151009177 Test Loss: 2916.233139792518\n",
      "Epoch 253/400: Train Loss: 2870.0079733157463 Test Loss: 2916.230219241105\n",
      "Epoch 254/400: Train Loss: 2869.97566911512 Test Loss: 2916.2273347037612\n",
      "Epoch 255/400: Train Loss: 2869.9434024527613 Test Loss: 2916.224485954421\n",
      "Epoch 256/400: Train Loss: 2869.9111732824927 Test Loss: 2916.221672770915\n",
      "Epoch 257/400: Train Loss: 2869.878981558234 Test Loss: 2916.2188949348956\n",
      "Epoch 258/400: Train Loss: 2869.8468272340037 Test Loss: 2916.216152231756\n",
      "Epoch 259/400: Train Loss: 2869.8147102639127 Test Loss: 2916.2134444505564\n",
      "Epoch 260/400: Train Loss: 2869.782630602169 Test Loss: 2916.2107713839437\n",
      "Epoch 261/400: Train Loss: 2869.7505882030705 Test Loss: 2916.20813282808\n",
      "Epoch 262/400: Train Loss: 2869.7185830210087 Test Loss: 2916.205528582566\n",
      "Epoch 263/400: Train Loss: 2869.686615010464 Test Loss: 2916.2029584503716\n",
      "Epoch 264/400: Train Loss: 2869.6546841260047 Test Loss: 2916.2004222377577\n",
      "Epoch 265/400: Train Loss: 2869.6227903222907 Test Loss: 2916.197919754211\n",
      "Epoch 266/400: Train Loss: 2869.5909335540664 Test Loss: 2916.195450812376\n",
      "Epoch 267/400: Train Loss: 2869.5591137761626 Test Loss: 2916.1930152279774\n",
      "Epoch 268/400: Train Loss: 2869.527330943495 Test Loss: 2916.190612819761\n",
      "Epoch 269/400: Train Loss: 2869.495585011066 Test Loss: 2916.188243409428\n",
      "Epoch 270/400: Train Loss: 2869.4638759339578 Test Loss: 2916.185906821564\n",
      "Epoch 271/400: Train Loss: 2869.432203667338 Test Loss: 2916.183602883576\n",
      "Epoch 272/400: Train Loss: 2869.4005681664544 Test Loss: 2916.181331425638\n",
      "Epoch 273/400: Train Loss: 2869.3689693866368 Test Loss: 2916.1790922806204\n",
      "Epoch 274/400: Train Loss: 2869.3374072832953 Test Loss: 2916.176885284032\n",
      "Epoch 275/400: Train Loss: 2869.3058818119193 Test Loss: 2916.1747102739646\n",
      "Epoch 276/400: Train Loss: 2869.2743929280778 Test Loss: 2916.1725670910328\n",
      "Epoch 277/400: Train Loss: 2869.2429405874173 Test Loss: 2916.1704555783144\n",
      "Epoch 278/400: Train Loss: 2869.2115247456627 Test Loss: 2916.1683755812987\n",
      "Epoch 279/400: Train Loss: 2869.180145358616 Test Loss: 2916.1663269478317\n",
      "Epoch 280/400: Train Loss: 2869.148802382155 Test Loss: 2916.1643095280606\n",
      "Epoch 281/400: Train Loss: 2869.1174957722346 Test Loss: 2916.162323174384\n",
      "Epoch 282/400: Train Loss: 2869.0862254848853 Test Loss: 2916.1603677413996\n",
      "Epoch 283/400: Train Loss: 2869.054991476212 Test Loss: 2916.158443085853\n",
      "Epoch 284/400: Train Loss: 2869.023793702392 Test Loss: 2916.1565490665926\n",
      "Epoch 285/400: Train Loss: 2868.9926321196813 Test Loss: 2916.154685544518\n",
      "Epoch 286/400: Train Loss: 2868.961506684405 Test Loss: 2916.152852382536\n",
      "Epoch 287/400: Train Loss: 2868.930417352962 Test Loss: 2916.15104944551\n",
      "Epoch 288/400: Train Loss: 2868.899364081827 Test Loss: 2916.1492766002225\n",
      "Epoch 289/400: Train Loss: 2868.8683468275426 Test Loss: 2916.1475337153247\n",
      "Epoch 290/400: Train Loss: 2868.8373655467253 Test Loss: 2916.1458206613006\n",
      "Epoch 291/400: Train Loss: 2868.8064201960624 Test Loss: 2916.144137310418\n",
      "Epoch 292/400: Train Loss: 2868.775510732311 Test Loss: 2916.1424835366915\n",
      "Epoch 293/400: Train Loss: 2868.7446371123006 Test Loss: 2916.1408592158436\n",
      "Epoch 294/400: Train Loss: 2868.71379929293 Test Loss: 2916.139264225265\n",
      "Epoch 295/400: Train Loss: 2868.6829972311675 Test Loss: 2916.137698443973\n",
      "Epoch 296/400: Train Loss: 2868.65223088405 Test Loss: 2916.1361617525813\n",
      "Epoch 297/400: Train Loss: 2868.621500208684 Test Loss: 2916.1346540332524\n",
      "Epoch 298/400: Train Loss: 2868.590805162246 Test Loss: 2916.1331751696803\n",
      "Epoch 299/400: Train Loss: 2868.560145701978 Test Loss: 2916.1317250470397\n",
      "Epoch 300/400: Train Loss: 2868.5295217851913 Test Loss: 2916.130303551953\n",
      "Epoch 301/400: Train Loss: 2868.4989333692656 Test Loss: 2916.1289105724704\n",
      "Epoch 302/400: Train Loss: 2868.4683804116466 Test Loss: 2916.1275459980257\n",
      "Epoch 303/400: Train Loss: 2868.437862869848 Test Loss: 2916.126209719408\n",
      "Epoch 304/400: Train Loss: 2868.4073807014483 Test Loss: 2916.1249016287315\n",
      "Epoch 305/400: Train Loss: 2868.3769338640936 Test Loss: 2916.123621619408\n",
      "Epoch 306/400: Train Loss: 2868.3465223154967 Test Loss: 2916.122369586116\n",
      "Epoch 307/400: Train Loss: 2868.3161460134347 Test Loss: 2916.121145424764\n",
      "Epoch 308/400: Train Loss: 2868.2858049157508 Test Loss: 2916.119949032483\n",
      "Epoch 309/400: Train Loss: 2868.255498980353 Test Loss: 2916.1187803075773\n",
      "Epoch 310/400: Train Loss: 2868.2252281652145 Test Loss: 2916.1176391495123\n",
      "Epoch 311/400: Train Loss: 2868.1949924283726 Test Loss: 2916.11652545888\n",
      "Epoch 312/400: Train Loss: 2868.164791727929 Test Loss: 2916.1154391373834\n",
      "Epoch 313/400: Train Loss: 2868.13462602205 Test Loss: 2916.1143800878017\n",
      "Epoch 314/400: Train Loss: 2868.1044952689645 Test Loss: 2916.113348213974\n",
      "Epoch 315/400: Train Loss: 2868.0743994269665 Test Loss: 2916.112343420771\n",
      "Epoch 316/400: Train Loss: 2868.044338454411 Test Loss: 2916.1113656140765\n",
      "Epoch 317/400: Train Loss: 2868.0143123097187 Test Loss: 2916.110414700761\n",
      "Epoch 318/400: Train Loss: 2867.9843209513697 Test Loss: 2916.109490588665\n",
      "Epoch 319/400: Train Loss: 2867.9543643379097 Test Loss: 2916.1085931865705\n",
      "Epoch 320/400: Train Loss: 2867.9244424279455 Test Loss: 2916.1077224041896\n",
      "Epoch 321/400: Train Loss: 2867.8945551801444 Test Loss: 2916.1068781521362\n",
      "Epoch 322/400: Train Loss: 2867.8647025532373 Test Loss: 2916.1060603419114\n",
      "Epoch 323/400: Train Loss: 2867.834884506016 Test Loss: 2916.1052688858836\n",
      "Epoch 324/400: Train Loss: 2867.805100997333 Test Loss: 2916.1045036972682\n",
      "Epoch 325/400: Train Loss: 2867.7753519861026 Test Loss: 2916.1037646901104\n",
      "Epoch 326/400: Train Loss: 2867.7456374312997 Test Loss: 2916.103051779267\n",
      "Epoch 327/400: Train Loss: 2867.7159572919595 Test Loss: 2916.102364880392\n",
      "Epoch 328/400: Train Loss: 2867.6863115271776 Test Loss: 2916.101703909917\n",
      "Epoch 329/400: Train Loss: 2867.656700096111 Test Loss: 2916.101068785033\n",
      "Epoch 330/400: Train Loss: 2867.6271229579743 Test Loss: 2916.100459423677\n",
      "Epoch 331/400: Train Loss: 2867.5975800720416 Test Loss: 2916.0998757445186\n",
      "Epoch 332/400: Train Loss: 2867.568071397651 Test Loss: 2916.0993176669417\n",
      "Epoch 333/400: Train Loss: 2867.5385968941937 Test Loss: 2916.09878511103\n",
      "Epoch 334/400: Train Loss: 2867.509156521124 Test Loss: 2916.09827799755\n",
      "Epoch 335/400: Train Loss: 2867.479750237955 Test Loss: 2916.0977962479433\n",
      "Epoch 336/400: Train Loss: 2867.450378004256 Test Loss: 2916.097339784309\n",
      "Epoch 337/400: Train Loss: 2867.4210397796564 Test Loss: 2916.096908529388\n",
      "Epoch 338/400: Train Loss: 2867.3917355238436 Test Loss: 2916.0965024065567\n",
      "Epoch 339/400: Train Loss: 2867.362465196563 Test Loss: 2916.0961213398064\n",
      "Epoch 340/400: Train Loss: 2867.3332287576177 Test Loss: 2916.095765253736\n",
      "Epoch 341/400: Train Loss: 2867.304026166869 Test Loss: 2916.0954340735407\n",
      "Epoch 342/400: Train Loss: 2867.274857384234 Test Loss: 2916.0951277249947\n",
      "Epoch 343/400: Train Loss: 2867.2457223696906 Test Loss: 2916.0948461344437\n",
      "Epoch 344/400: Train Loss: 2867.2166210832693 Test Loss: 2916.0945892287946\n",
      "Epoch 345/400: Train Loss: 2867.18755348506 Test Loss: 2916.0943569355018\n",
      "Epoch 346/400: Train Loss: 2867.15851953521 Test Loss: 2916.0941491825583\n",
      "Epoch 347/400: Train Loss: 2867.1295191939207 Test Loss: 2916.0939658984867\n",
      "Epoch 348/400: Train Loss: 2867.100552421452 Test Loss: 2916.093807012322\n",
      "Epoch 349/400: Train Loss: 2867.0716191781194 Test Loss: 2916.0936724536127\n",
      "Epoch 350/400: Train Loss: 2867.0427194242934 Test Loss: 2916.093562152399\n",
      "Epoch 351/400: Train Loss: 2867.013853120401 Test Loss: 2916.0934760392174\n",
      "Epoch 352/400: Train Loss: 2866.985020226925 Test Loss: 2916.093414045079\n",
      "Epoch 353/400: Train Loss: 2866.956220704403 Test Loss: 2916.0933761014644\n",
      "Epoch 354/400: Train Loss: 2866.9274545134285 Test Loss: 2916.0933621403183\n",
      "Epoch 355/400: Train Loss: 2866.8987216146506 Test Loss: 2916.093372094043\n",
      "Epoch 356/400: Train Loss: 2866.8700219687717 Test Loss: 2916.0934058954763\n",
      "Epoch 357/400: Train Loss: 2866.84135553655 Test Loss: 2916.0934634779\n",
      "Epoch 358/400: Train Loss: 2866.8127222788 Test Loss: 2916.093544775024\n",
      "Epoch 359/400: Train Loss: 2866.7841221563854 Test Loss: 2916.093649720978\n",
      "Epoch 360/400: Train Loss: 2866.75555513023 Test Loss: 2916.0937782503056\n",
      "Epoch 361/400: Train Loss: 2866.7270211613077 Test Loss: 2916.0939302979587\n",
      "Epoch 362/400: Train Loss: 2866.69852021065 Test Loss: 2916.094105799291\n",
      "Epoch 363/400: Train Loss: 2866.670052239338 Test Loss: 2916.094304690042\n",
      "Epoch 364/400: Train Loss: 2866.641617208509 Test Loss: 2916.0945269063427\n",
      "Epoch 365/400: Train Loss: 2866.613215079354 Test Loss: 2916.094772384701\n",
      "Epoch 366/400: Train Loss: 2866.584845813116 Test Loss: 2916.0950410620007\n",
      "Epoch 367/400: Train Loss: 2866.5565093710916 Test Loss: 2916.095332875485\n",
      "Epoch 368/400: Train Loss: 2866.5282057146305 Test Loss: 2916.0956477627656\n",
      "Epoch 369/400: Train Loss: 2866.4999348051356 Test Loss: 2916.095985661804\n",
      "Epoch 370/400: Train Loss: 2866.471696604061 Test Loss: 2916.0963465109103\n",
      "Epoch 371/400: Train Loss: 2866.4434910729155 Test Loss: 2916.0967302487397\n",
      "Epoch 372/400: Train Loss: 2866.4153181732595 Test Loss: 2916.097136814284\n",
      "Epoch 373/400: Train Loss: 2866.3871778667035 Test Loss: 2916.0975661468633\n",
      "Epoch 374/400: Train Loss: 2866.3590701149137 Test Loss: 2916.0980181861287\n",
      "Epoch 375/400: Train Loss: 2866.3309948796064 Test Loss: 2916.098492872051\n",
      "Epoch 376/400: Train Loss: 2866.3029521225494 Test Loss: 2916.0989901449175\n",
      "Epoch 377/400: Train Loss: 2866.2749418055623 Test Loss: 2916.0995099453276\n",
      "Epoch 378/400: Train Loss: 2866.246963890517 Test Loss: 2916.1000522141844\n",
      "Epoch 379/400: Train Loss: 2866.2190183393354 Test Loss: 2916.1006168926974\n",
      "Epoch 380/400: Train Loss: 2866.191105113993 Test Loss: 2916.1012039223697\n",
      "Epoch 381/400: Train Loss: 2866.163224176514 Test Loss: 2916.1018132449985\n",
      "Epoch 382/400: Train Loss: 2866.135375488974 Test Loss: 2916.102444802671\n",
      "Epoch 383/400: Train Loss: 2866.107559013501 Test Loss: 2916.1030985377565\n",
      "Epoch 384/400: Train Loss: 2866.079774712272 Test Loss: 2916.103774392905\n",
      "Epoch 385/400: Train Loss: 2866.0520225475148 Test Loss: 2916.104472311045\n",
      "Epoch 386/400: Train Loss: 2866.024302481508 Test Loss: 2916.1051922353727\n",
      "Epoch 387/400: Train Loss: 2865.9966144765813 Test Loss: 2916.105934109356\n",
      "Epoch 388/400: Train Loss: 2865.9689584951134 Test Loss: 2916.1066978767262\n",
      "Epoch 389/400: Train Loss: 2865.9413344995323 Test Loss: 2916.1074834814735\n",
      "Epoch 390/400: Train Loss: 2865.9137424523183 Test Loss: 2916.1082908678486\n",
      "Epoch 391/400: Train Loss: 2865.886182315999 Test Loss: 2916.1091199803536\n",
      "Epoch 392/400: Train Loss: 2865.8586540531537 Test Loss: 2916.109970763744\n",
      "Epoch 393/400: Train Loss: 2865.8311576264095 Test Loss: 2916.110843163016\n",
      "Epoch 394/400: Train Loss: 2865.8036929984437 Test Loss: 2916.111737123414\n",
      "Epoch 395/400: Train Loss: 2865.7762601319832 Test Loss: 2916.1126525904237\n",
      "Epoch 396/400: Train Loss: 2865.748858989803 Test Loss: 2916.1135895097636\n",
      "Epoch 397/400: Train Loss: 2865.721489534728 Test Loss: 2916.1145478273907\n",
      "Epoch 398/400: Train Loss: 2865.6941517296323 Test Loss: 2916.1155274894904\n",
      "Epoch 399/400: Train Loss: 2865.6668455374374 Test Loss: 2916.1165284424774\n",
      "Epoch 400/400: Train Loss: 2865.6395709211142 Test Loss: 2916.117550632991\n"
     ]
    }
   ],
   "source": [
    "fit(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1e5a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
