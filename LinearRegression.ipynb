{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e8177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mygrad\n",
    "import mygrad.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e23983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_diabetes(return_X_y = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = mygrad.value(X_train, \"X_train\", requires_grad=False)\n",
    "X_test = mygrad.value(X_test, \"X_test\", requires_grad=False)\n",
    "y_train = mygrad.value(y_train, \"y_train\", requires_grad=False)\n",
    "y_test = mygrad.value(y_test, \"y_test\", requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e18c0d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w = mygrad.value(np.random.randn(X.shape[1]) * 10**(-1), \"w\", requires_grad = True)\n",
    "b = mygrad.value(np.array([np.mean(y_train.data)]), \"b\", requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "306489fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Function of type <class 'mygrad.functional.function.Matmul'> with name matmul_0\n",
      "Creating Function of type <class 'mygrad.functional.function.Add'> with name add_0\n",
      "Creating Function of type <class 'mygrad.functional.function.MSELoss'> with name mse_loss_0\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "\n",
    "matmul1 = F.matmul()\n",
    "add1 = F.add()\n",
    "loss = F.mse_loss()\n",
    "    \n",
    "def fit(num_epoch):     \n",
    "    for i in range(num_epoch):\n",
    "        \n",
    "        y = add1.forward(matmul1.forward(X_train, w), b)\n",
    "        l = loss.forward(y, y_train)\n",
    "        loss.backward()\n",
    "        l_test = loss.forward(add1.forward(matmul1.forward(X_test, w), b), y_test)\n",
    "        w.data -= lr * w.grad\n",
    "        b.data -= lr * b.grad\n",
    "        l.zero_grad()   \n",
    "        \n",
    "        print(f\"Epoch {i+1}/{num_epoch}: Train Loss: {l.data} Test Loss: {l_test.data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b840b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400: Train Loss: 6036.686814418272 Test Loss: 5672.4417973655945\n",
      "Epoch 2/400: Train Loss: 4974.5701165403725 Test Loss: 4745.530497291949\n",
      "Epoch 3/400: Train Loss: 4348.846499463638 Test Loss: 4218.825920212131\n",
      "Epoch 4/400: Train Loss: 3967.241293199174 Test Loss: 3907.056950791428\n",
      "Epoch 5/400: Train Loss: 3724.233928085321 Test Loss: 3711.4795099277203\n",
      "Epoch 6/400: Train Loss: 3561.594086578472 Test Loss: 3579.604097820217\n",
      "Epoch 7/400: Train Loss: 3446.922330609477 Test Loss: 3483.579311482025\n",
      "Epoch 8/400: Train Loss: 3361.9719781433164 Test Loss: 3408.6223620816972\n",
      "Epoch 9/400: Train Loss: 3296.2864601366023 Test Loss: 3346.849091899688\n",
      "Epoch 10/400: Train Loss: 3243.7266354264066 Test Loss: 3293.9982089684113\n",
      "Epoch 11/400: Train Loss: 3200.5704249965515 Test Loss: 3247.701722023045\n",
      "Epoch 12/400: Train Loss: 3164.46877188151 Test Loss: 3206.577937749097\n",
      "Epoch 13/400: Train Loss: 3133.8686990109964 Test Loss: 3169.7597112266326\n",
      "Epoch 14/400: Train Loss: 3107.6917416703386 Test Loss: 3136.6514372223896\n",
      "Epoch 15/400: Train Loss: 3085.1524792212085 Test Loss: 3106.805205648345\n",
      "Epoch 16/400: Train Loss: 3065.6543163086376 Test Loss: 3079.8583439945896\n",
      "Epoch 17/400: Train Loss: 3048.7281765952916 Test Loss: 3055.502130817746\n",
      "Epoch 18/400: Train Loss: 3033.995292471716 Test Loss: 3033.466047520427\n",
      "Epoch 19/400: Train Loss: 3021.1437330033996 Test Loss: 3013.5095957923268\n",
      "Epoch 20/400: Train Loss: 3009.9129310806957 Test Loss: 2995.4176931030383\n",
      "Epoch 21/400: Train Loss: 3000.082999568335 Test Loss: 2978.9977050449447\n",
      "Epoch 22/400: Train Loss: 2991.4670164196577 Test Loss: 2964.077205664495\n",
      "Epoch 23/400: Train Loss: 2983.9052274407068 Test Loss: 2950.5020647503648\n",
      "Epoch 24/400: Train Loss: 2977.260544093845 Test Loss: 2938.1347017359794\n",
      "Epoch 25/400: Train Loss: 2971.414955718525 Test Loss: 2926.8524534293674\n",
      "Epoch 26/400: Train Loss: 2966.266614452336 Test Loss: 2916.5460460748072\n",
      "Epoch 27/400: Train Loss: 2961.727432690772 Test Loss: 2907.118176031326\n",
      "Epoch 28/400: Train Loss: 2957.721082220963 Test Loss: 2898.482204703343\n",
      "Epoch 29/400: Train Loss: 2954.1813150370126 Test Loss: 2890.560970342871\n",
      "Epoch 30/400: Train Loss: 2951.0505459745605 Test Loss: 2883.2857155222073\n",
      "Epoch 31/400: Train Loss: 2948.27865099806 Test Loss: 2876.5951258353916\n",
      "Epoch 32/400: Train Loss: 2945.8219446869266 Test Loss: 2870.434473109713\n",
      "Epoch 33/400: Train Loss: 2943.642307617015 Test Loss: 2864.7548550583206\n",
      "Epoch 34/400: Train Loss: 2941.706439766377 Test Loss: 2859.5125226999044\n",
      "Epoch 35/400: Train Loss: 2939.985220306923 Test Loss: 2854.6682868067323\n",
      "Epoch 36/400: Train Loss: 2938.453157505777 Test Loss: 2850.186994938943\n",
      "Epoch 37/400: Train Loss: 2937.0879151699 Test Loss: 2846.0370711405894\n",
      "Epoch 38/400: Train Loss: 2935.8699042752182 Test Loss: 2842.1901110090107\n",
      "Epoch 39/400: Train Loss: 2934.7819302346898 Test Loss: 2838.6205255327163\n",
      "Epoch 40/400: Train Loss: 2933.808887758203 Test Loss: 2835.3052277774304\n",
      "Epoch 41/400: Train Loss: 2932.937496501552 Test Loss: 2832.2233571562756\n",
      "Epoch 42/400: Train Loss: 2932.1560717391785 Test Loss: 2829.3560366315833\n",
      "Epoch 43/400: Train Loss: 2931.454325163238 Test Loss: 2826.6861587545372\n",
      "Epoch 44/400: Train Loss: 2930.8231916396403 Test Loss: 2824.1981969520325\n",
      "Epoch 45/400: Train Loss: 2930.254678364259 Test Loss: 2821.878038918761\n",
      "Epoch 46/400: Train Loss: 2929.74173337901 Test Loss: 2819.712839369402\n",
      "Epoch 47/400: Train Loss: 2929.278130844168 Test Loss: 2817.6908897549456\n",
      "Epoch 48/400: Train Loss: 2928.8583708331166 Test Loss: 2815.8015028531936\n",
      "Epoch 49/400: Train Loss: 2928.477591729838 Test Loss: 2814.0349104107213\n",
      "Epoch 50/400: Train Loss: 2928.131493576522 Test Loss: 2812.38217224663\n",
      "Epoch 51/400: Train Loss: 2927.8162709464696 Test Loss: 2810.835095431272\n",
      "Epoch 52/400: Train Loss: 2927.5285541119206 Test Loss: 2809.38616232957\n",
      "Epoch 53/400: Train Loss: 2927.26535744289 Test Loss: 2808.028466451929\n",
      "Epoch 54/400: Train Loss: 2927.0240341157228 Test Loss: 2806.755655189097\n",
      "Epoch 55/400: Train Loss: 2926.8022363326018 Test Loss: 2805.5618786231594\n",
      "Epoch 56/400: Train Loss: 2926.5978803585745 Test Loss: 2804.441743707731\n",
      "Epoch 57/400: Train Loss: 2926.409115773443 Test Loss: 2803.3902731979692\n",
      "Epoch 58/400: Train Loss: 2926.234298414133 Test Loss: 2802.4028687873965\n",
      "Epoch 59/400: Train Loss: 2926.071966550852 Test Loss: 2801.4752779749856\n",
      "Epoch 60/400: Train Loss: 2925.920819898831 Test Loss: 2800.6035642438424\n",
      "Epoch 61/400: Train Loss: 2925.7797011181915 Test Loss: 2799.784080183366\n",
      "Epoch 62/400: Train Loss: 2925.6475794984253 Test Loss: 2799.0134432309105\n",
      "Epoch 63/400: Train Loss: 2925.5235365622 Test Loss: 2798.2885137474123\n",
      "Epoch 64/400: Train Loss: 2925.4067533563825 Test Loss: 2797.606375175274\n",
      "Epoch 65/400: Train Loss: 2925.296499227081 Test Loss: 2796.964316056158\n",
      "Epoch 66/400: Train Loss: 2925.192121900686 Test Loss: 2796.359813712269\n",
      "Epoch 67/400: Train Loss: 2925.0930387148283 Test Loss: 2795.7905194173027\n",
      "Epoch 68/400: Train Loss: 2924.9987288623324 Test Loss: 2795.2542449031566\n",
      "Epoch 69/400: Train Loss: 2924.9087265279873 Test Loss: 2794.748950065891\n",
      "Epoch 70/400: Train Loss: 2924.822614812574 Test Loss: 2794.272731749854\n",
      "Epoch 71/400: Train Loss: 2924.7400203513894 Test Loss: 2793.8238135023116\n",
      "Epoch 72/400: Train Loss: 2924.6606085457047 Test Loss: 2793.400536202877\n",
      "Epoch 73/400: Train Loss: 2924.5840793354196 Test Loss: 2793.0013494824666\n",
      "Epoch 74/400: Train Loss: 2924.5101634497696 Test Loss: 2792.6248038557865\n",
      "Epoch 75/400: Train Loss: 2924.438619080481 Test Loss: 2792.2695434995157\n",
      "Epoch 76/400: Train Loss: 2924.3692289284154 Test Loss: 2791.9342996155497\n",
      "Epoch 77/400: Train Loss: 2924.3017975805215 Test Loss: 2791.6178843251023\n",
      "Epoch 78/400: Train Loss: 2924.236149179066 Test Loss: 2791.3191850450416\n",
      "Epoch 79/400: Train Loss: 2924.172125349558 Test Loss: 2791.03715930293\n",
      "Epoch 80/400: Train Loss: 2924.1095833577674 Test Loss: 2790.7708299516416\n",
      "Epoch 81/400: Train Loss: 2924.048394469692 Test Loss: 2790.519280748374\n",
      "Epoch 82/400: Train Loss: 2923.9884424913885 Test Loss: 2790.2816522664184\n",
      "Epoch 83/400: Train Loss: 2923.9296224682776 Test Loss: 2790.057138111132\n",
      "Epoch 84/400: Train Loss: 2923.871839525899 Test Loss: 2789.8449814143924\n",
      "Epoch 85/400: Train Loss: 2923.815007836189 Test Loss: 2789.64447158425\n",
      "Epoch 86/400: Train Loss: 2923.759049695196 Test Loss: 2789.454941288738\n",
      "Epoch 87/400: Train Loss: 2923.7038946997523 Test Loss: 2789.2757636547713\n",
      "Epoch 88/400: Train Loss: 2923.6494790120946 Test Loss: 2789.1063496648426\n",
      "Epoch 89/400: Train Loss: 2923.59574470265 Test Loss: 2788.946145735814\n",
      "Epoch 90/400: Train Loss: 2923.542639162336 Test Loss: 2788.7946314655037\n",
      "Epoch 91/400: Train Loss: 2923.4901145767303 Test Loss: 2788.651317534099\n",
      "Epoch 92/400: Train Loss: 2923.438127455285 Test Loss: 2788.515743748495\n",
      "Epoch 93/400: Train Loss: 2923.3866382095944 Test Loss: 2788.3874772187874\n",
      "Epoch 94/400: Train Loss: 2923.3356107753507 Test Loss: 2788.266110656998\n",
      "Epoch 95/400: Train Loss: 2923.2850122732584 Test Loss: 2788.1512607889986\n",
      "Epoch 96/400: Train Loss: 2923.234812704689 Test Loss: 2788.0425668713488\n",
      "Epoch 97/400: Train Loss: 2923.184984678347 Test Loss: 2787.9396893054304\n",
      "Epoch 98/400: Train Loss: 2923.135503164619 Test Loss: 2787.8423083419393\n",
      "Epoch 99/400: Train Loss: 2923.0863452746635 Test Loss: 2787.75012286926\n",
      "Epoch 100/400: Train Loss: 2923.037490061596 Test Loss: 2787.6628492798864\n",
      "Epoch 101/400: Train Loss: 2922.988918341465 Test Loss: 2787.5802204093998\n",
      "Epoch 102/400: Train Loss: 2922.940612531908 Test Loss: 2787.5019845430247\n",
      "Epoch 103/400: Train Loss: 2922.892556506662 Test Loss: 2787.427904485109\n",
      "Epoch 104/400: Train Loss: 2922.844735464265 Test Loss: 2787.357756687279\n",
      "Epoch 105/400: Train Loss: 2922.7971358094887 Test Loss: 2787.291330431298\n",
      "Epoch 106/400: Train Loss: 2922.7497450461797 Test Loss: 2787.2284270629657\n",
      "Epoch 107/400: Train Loss: 2922.7025516803583 Test Loss: 2787.168859273694\n",
      "Epoch 108/400: Train Loss: 2922.655545132516 Test Loss: 2787.112450426581\n",
      "Epoch 109/400: Train Loss: 2922.6087156581816 Test Loss: 2787.0590339240966\n",
      "Epoch 110/400: Train Loss: 2922.5620542759375 Test Loss: 2787.0084526146484\n",
      "Epoch 111/400: Train Loss: 2922.5155527021207 Test Loss: 2786.96055823553\n",
      "Epoch 112/400: Train Loss: 2922.4692032915614 Test Loss: 2786.9152108898925\n",
      "Epoch 113/400: Train Loss: 2922.4229989837504 Test Loss: 2786.872278555573\n",
      "Epoch 114/400: Train Loss: 2922.3769332539096 Test Loss: 2786.831636623759\n",
      "Epoch 115/400: Train Loss: 2922.3310000684824 Test Loss: 2786.7931674655642\n",
      "Epoch 116/400: Train Loss: 2922.285193844619 Test Loss: 2786.7567600248035\n",
      "Epoch 117/400: Train Loss: 2922.239509413269 Test Loss: 2786.722309435279\n",
      "Epoch 118/400: Train Loss: 2922.1939419855366 Test Loss: 2786.6897166610706\n",
      "Epoch 119/400: Train Loss: 2922.1484871219923 Test Loss: 2786.6588881583825\n",
      "Epoch 120/400: Train Loss: 2922.1031407046544 Test Loss: 2786.629735557626\n",
      "Epoch 121/400: Train Loss: 2922.0578989113965 Test Loss: 2786.602175364456\n",
      "Epoch 122/400: Train Loss: 2922.0127581925544 Test Loss: 2786.5761286786396\n",
      "Epoch 123/400: Train Loss: 2921.9677152495233 Test Loss: 2786.551520929606\n",
      "Epoch 124/400: Train Loss: 2921.9227670151718 Test Loss: 2786.528281627703\n",
      "Epoch 125/400: Train Loss: 2921.8779106358984 Test Loss: 2786.5063441301795\n",
      "Epoch 126/400: Train Loss: 2921.833143455187 Test Loss: 2786.48564542099\n",
      "Epoch 127/400: Train Loss: 2921.788462998524 Test Loss: 2786.466125903601\n",
      "Epoch 128/400: Train Loss: 2921.743866959562 Test Loss: 2786.4477292060005\n",
      "Epoch 129/400: Train Loss: 2921.6993531874045 Test Loss: 2786.4304019971637\n",
      "Epoch 130/400: Train Loss: 2921.6549196749324 Test Loss: 2786.414093814302\n",
      "Epoch 131/400: Train Loss: 2921.6105645480657 Test Loss: 2786.3987569002434\n",
      "Epoch 132/400: Train Loss: 2921.5662860558896 Test Loss: 2786.3843460503076\n",
      "Epoch 133/400: Train Loss: 2921.5220825615625 Test Loss: 2786.3708184681577\n",
      "Epoch 134/400: Train Loss: 2921.4779525339445 Test Loss: 2786.3581336300354\n",
      "Epoch 135/400: Train Loss: 2921.433894539886 Test Loss: 2786.3462531569257\n",
      "Epoch 136/400: Train Loss: 2921.3899072371123 Test Loss: 2786.3351406941397\n",
      "Epoch 137/400: Train Loss: 2921.345989367669 Test Loss: 2786.3247617979096\n",
      "Epoch 138/400: Train Loss: 2921.3021397518633 Test Loss: 2786.315083828546\n",
      "Epoch 139/400: Train Loss: 2921.2583572826775 Test Loss: 2786.3060758497963\n",
      "Epoch 140/400: Train Loss: 2921.214640920604 Test Loss: 2786.297708534003\n",
      "Epoch 141/400: Train Loss: 2921.1709896888747 Test Loss: 2786.289954072769\n",
      "Epoch 142/400: Train Loss: 2921.1274026690467 Test Loss: 2786.2827860927373\n",
      "Epoch 143/400: Train Loss: 2921.083878996922 Test Loss: 2786.276179576255\n",
      "Epoch 144/400: Train Loss: 2921.040417858769 Test Loss: 2786.270110786568\n",
      "Epoch 145/400: Train Loss: 2920.9970184878266 Test Loss: 2786.264557197335\n",
      "Epoch 146/400: Train Loss: 2920.953680161064 Test Loss: 2786.2594974261606\n",
      "Epoch 147/400: Train Loss: 2920.9104021961775 Test Loss: 2786.2549111719495\n",
      "Epoch 148/400: Train Loss: 2920.8671839488143 Test Loss: 2786.2507791558223\n",
      "Epoch 149/400: Train Loss: 2920.824024809984 Test Loss: 2786.2470830654192\n",
      "Epoch 150/400: Train Loss: 2920.7809242036747 Test Loss: 2786.243805502369\n",
      "Epoch 151/400: Train Loss: 2920.7378815846228 Test Loss: 2786.2409299327396\n",
      "Epoch 152/400: Train Loss: 2920.6948964362577 Test Loss: 2786.2384406403144\n",
      "Epoch 153/400: Train Loss: 2920.651968268787 Test Loss: 2786.236322682501\n",
      "Epoch 154/400: Train Loss: 2920.6090966174156 Test Loss: 2786.2345618487507\n",
      "Epoch 155/400: Train Loss: 2920.5662810406934 Test Loss: 2786.2331446213157\n",
      "Epoch 156/400: Train Loss: 2920.523521118981 Test Loss: 2786.2320581382246\n",
      "Epoch 157/400: Train Loss: 2920.480816453021 Test Loss: 2786.2312901583387\n",
      "Epoch 158/400: Train Loss: 2920.438166662607 Test Loss: 2786.2308290283736\n",
      "Epoch 159/400: Train Loss: 2920.395571385353 Test Loss: 2786.230663651767\n",
      "Epoch 160/400: Train Loss: 2920.3530302755366 Test Loss: 2786.2307834592925\n",
      "Epoch 161/400: Train Loss: 2920.310543003036 Test Loss: 2786.231178381318\n",
      "Epoch 162/400: Train Loss: 2920.2681092523258 Test Loss: 2786.231838821598\n",
      "Epoch 163/400: Train Loss: 2920.2257287215525 Test Loss: 2786.2327556325363\n",
      "Epoch 164/400: Train Loss: 2920.1834011216697 Test Loss: 2786.2339200918227\n",
      "Epoch 165/400: Train Loss: 2920.1411261756307 Test Loss: 2786.2353238803485\n",
      "Epoch 166/400: Train Loss: 2920.0989036176366 Test Loss: 2786.2369590613666\n",
      "Epoch 167/400: Train Loss: 2920.0567331924385 Test Loss: 2786.2388180607786\n",
      "Epoch 168/400: Train Loss: 2920.0146146546817 Test Loss: 2786.240893648522\n",
      "Epoch 169/400: Train Loss: 2919.9725477682964 Test Loss: 2786.2431789209713\n",
      "Epoch 170/400: Train Loss: 2919.930532305931 Test Loss: 2786.245667284311\n",
      "Epoch 171/400: Train Loss: 2919.8885680484213 Test Loss: 2786.2483524388044\n",
      "Epoch 172/400: Train Loss: 2919.8466547842936 Test Loss: 2786.2512283639353\n",
      "Epoch 173/400: Train Loss: 2919.804792309302 Test Loss: 2786.254289304344\n",
      "Epoch 174/400: Train Loss: 2919.7629804260014 Test Loss: 2786.257529756539\n",
      "Epoch 175/400: Train Loss: 2919.721218943338 Test Loss: 2786.2609444563136\n",
      "Epoch 176/400: Train Loss: 2919.679507676277 Test Loss: 2786.264528366853\n",
      "Epoch 177/400: Train Loss: 2919.6378464454474 Test Loss: 2786.2682766674798\n",
      "Epoch 178/400: Train Loss: 2919.596235076816 Test Loss: 2786.2721847429902\n",
      "Epoch 179/400: Train Loss: 2919.5546734013774 Test Loss: 2786.2762481735826\n",
      "Epoch 180/400: Train Loss: 2919.5131612548666 Test Loss: 2786.280462725303\n",
      "Epoch 181/400: Train Loss: 2919.471698477491 Test Loss: 2786.284824341013\n",
      "Epoch 182/400: Train Loss: 2919.4302849136784 Test Loss: 2786.28932913183\n",
      "Epoch 183/400: Train Loss: 2919.388920411842 Test Loss: 2786.293973369014\n",
      "Epoch 184/400: Train Loss: 2919.3476048241564 Test Loss: 2786.298753476304\n",
      "Epoch 185/400: Train Loss: 2919.3063380063572 Test Loss: 2786.303666022623\n",
      "Epoch 186/400: Train Loss: 2919.265119817543 Test Loss: 2786.3087077152004\n",
      "Epoch 187/400: Train Loss: 2919.2239501199983 Test Loss: 2786.3138753930257\n",
      "Epoch 188/400: Train Loss: 2919.1828287790195 Test Loss: 2786.3191660206603\n",
      "Epoch 189/400: Train Loss: 2919.141755662759 Test Loss: 2786.3245766823566\n",
      "Epoch 190/400: Train Loss: 2919.100730642078 Test Loss: 2786.330104576497\n",
      "Epoch 191/400: Train Loss: 2919.0597535904035 Test Loss: 2786.3357470102974\n",
      "Epoch 192/400: Train Loss: 2919.0188243835987 Test Loss: 2786.341501394805\n",
      "Epoch 193/400: Train Loss: 2918.977942899842 Test Loss: 2786.3473652401394\n",
      "Epoch 194/400: Train Loss: 2918.9371090195104 Test Loss: 2786.3533361509676\n",
      "Epoch 195/400: Train Loss: 2918.896322625071 Test Loss: 2786.359411822232\n",
      "Epoch 196/400: Train Loss: 2918.8555836009814 Test Loss: 2786.365590035072\n",
      "Epoch 197/400: Train Loss: 2918.8148918335937 Test Loss: 2786.371868652959\n",
      "Epoch 198/400: Train Loss: 2918.7742472110644 Test Loss: 2786.3782456180343\n",
      "Epoch 199/400: Train Loss: 2918.733649623272 Test Loss: 2786.384718947605\n",
      "Epoch 200/400: Train Loss: 2918.6930989617367 Test Loss: 2786.3912867308436\n",
      "Epoch 201/400: Train Loss: 2918.652595119551 Test Loss: 2786.397947125631\n",
      "Epoch 202/400: Train Loss: 2918.6121379912993 Test Loss: 2786.404698355562\n",
      "Epoch 203/400: Train Loss: 2918.571727473007 Test Loss: 2786.4115387070915\n",
      "Epoch 204/400: Train Loss: 2918.531363462064 Test Loss: 2786.418466526834\n",
      "Epoch 205/400: Train Loss: 2918.4910458571803 Test Loss: 2786.425480218983\n",
      "Epoch 206/400: Train Loss: 2918.450774558322 Test Loss: 2786.43257824285\n",
      "Epoch 207/400: Train Loss: 2918.4105494666637 Test Loss: 2786.4397591105403\n",
      "Epoch 208/400: Train Loss: 2918.3703704845393 Test Loss: 2786.4470213847258\n",
      "Epoch 209/400: Train Loss: 2918.330237515397 Test Loss: 2786.4543636765256\n",
      "Epoch 210/400: Train Loss: 2918.290150463757 Test Loss: 2786.4617846434967\n",
      "Epoch 211/400: Train Loss: 2918.2501092351686 Test Loss: 2786.4692829877054\n",
      "Epoch 212/400: Train Loss: 2918.2101137361747 Test Loss: 2786.4768574539016\n",
      "Epoch 213/400: Train Loss: 2918.170163874273 Test Loss: 2786.484506827771\n",
      "Epoch 214/400: Train Loss: 2918.130259557885 Test Loss: 2786.4922299342784\n",
      "Epoch 215/400: Train Loss: 2918.0904006963215 Test Loss: 2786.5000256360745\n",
      "Epoch 216/400: Train Loss: 2918.0505871997543 Test Loss: 2786.5078928319826\n",
      "Epoch 217/400: Train Loss: 2918.0108189791845 Test Loss: 2786.5158304555594\n",
      "Epoch 218/400: Train Loss: 2917.971095946419 Test Loss: 2786.5238374737173\n",
      "Epoch 219/400: Train Loss: 2917.9314180140414 Test Loss: 2786.5319128854057\n",
      "Epoch 220/400: Train Loss: 2917.891785095391 Test Loss: 2786.5400557203557\n",
      "Epoch 221/400: Train Loss: 2917.852197104536 Test Loss: 2786.548265037886\n",
      "Epoch 222/400: Train Loss: 2917.812653956256 Test Loss: 2786.5565399257475\n",
      "Epoch 223/400: Train Loss: 2917.7731555660184 Test Loss: 2786.5648794990457\n",
      "Epoch 224/400: Train Loss: 2917.7337018499597 Test Loss: 2786.5732828991718\n",
      "Epoch 225/400: Train Loss: 2917.694292724866 Test Loss: 2786.5817492928236\n",
      "Epoch 226/400: Train Loss: 2917.654928108157 Test Loss: 2786.590277871037\n",
      "Epoch 227/400: Train Loss: 2917.615607917868 Test Loss: 2786.598867848275\n",
      "Epoch 228/400: Train Loss: 2917.576332072635 Test Loss: 2786.607518461552\n",
      "Epoch 229/400: Train Loss: 2917.5371004916797 Test Loss: 2786.6162289695985\n",
      "Epoch 230/400: Train Loss: 2917.4979130947904 Test Loss: 2786.624998652059\n",
      "Epoch 231/400: Train Loss: 2917.458769802317 Test Loss: 2786.6338268087225\n",
      "Epoch 232/400: Train Loss: 2917.41967053515 Test Loss: 2786.6427127587885\n",
      "Epoch 233/400: Train Loss: 2917.3806152147135 Test Loss: 2786.651655840166\n",
      "Epoch 234/400: Train Loss: 2917.3416037629486 Test Loss: 2786.660655408796\n",
      "Epoch 235/400: Train Loss: 2917.302636102306 Test Loss: 2786.6697108380076\n",
      "Epoch 236/400: Train Loss: 2917.263712155734 Test Loss: 2786.678821517897\n",
      "Epoch 237/400: Train Loss: 2917.224831846666 Test Loss: 2786.6879868547335\n",
      "Epoch 238/400: Train Loss: 2917.185995099012 Test Loss: 2786.6972062703912\n",
      "Epoch 239/400: Train Loss: 2917.147201837153 Test Loss: 2786.706479201806\n",
      "Epoch 240/400: Train Loss: 2917.1084519859246 Test Loss: 2786.7158051004394\n",
      "Epoch 241/400: Train Loss: 2917.069745470614 Test Loss: 2786.72518343179\n",
      "Epoch 242/400: Train Loss: 2917.0310822169495 Test Loss: 2786.7346136749047\n",
      "Epoch 243/400: Train Loss: 2916.9924621510922 Test Loss: 2786.74409532191\n",
      "Epoch 244/400: Train Loss: 2916.9538851996294 Test Loss: 2786.7536278775774\n",
      "Epoch 245/400: Train Loss: 2916.9153512895655 Test Loss: 2786.7632108588846\n",
      "Epoch 246/400: Train Loss: 2916.8768603483177 Test Loss: 2786.7728437946125\n",
      "Epoch 247/400: Train Loss: 2916.8384123037067 Test Loss: 2786.7825262249503\n",
      "Epoch 248/400: Train Loss: 2916.800007083951 Test Loss: 2786.7922577011054\n",
      "Epoch 249/400: Train Loss: 2916.7616446176607 Test Loss: 2786.8020377849566\n",
      "Epoch 250/400: Train Loss: 2916.72332483383 Test Loss: 2786.811866048686\n",
      "Epoch 251/400: Train Loss: 2916.685047661834 Test Loss: 2786.8217420744513\n",
      "Epoch 252/400: Train Loss: 2916.64681303142 Test Loss: 2786.831665454057\n",
      "Epoch 253/400: Train Loss: 2916.6086208727056 Test Loss: 2786.8416357886445\n",
      "Epoch 254/400: Train Loss: 2916.5704711161693 Test Loss: 2786.85165268839\n",
      "Epoch 255/400: Train Loss: 2916.5323636926473 Test Loss: 2786.8617157722133\n",
      "Epoch 256/400: Train Loss: 2916.4942985333296 Test Loss: 2786.871824667505\n",
      "Epoch 257/400: Train Loss: 2916.4562755697525 Test Loss: 2786.8819790098496\n",
      "Epoch 258/400: Train Loss: 2916.4182947337977 Test Loss: 2786.8921784427744\n",
      "Epoch 259/400: Train Loss: 2916.3803559576822 Test Loss: 2786.9024226174956\n",
      "Epoch 260/400: Train Loss: 2916.342459173961 Test Loss: 2786.912711192682\n",
      "Epoch 261/400: Train Loss: 2916.3046043155155 Test Loss: 2786.9230438342192\n",
      "Epoch 262/400: Train Loss: 2916.2667913155547 Test Loss: 2786.9334202149917\n",
      "Epoch 263/400: Train Loss: 2916.229020107609 Test Loss: 2786.9438400146632\n",
      "Epoch 264/400: Train Loss: 2916.191290625525 Test Loss: 2786.9543029194724\n",
      "Epoch 265/400: Train Loss: 2916.1536028034657 Test Loss: 2786.9648086220313\n",
      "Epoch 266/400: Train Loss: 2916.115956575902 Test Loss: 2786.975356821134\n",
      "Epoch 267/400: Train Loss: 2916.0783518776125 Test Loss: 2786.9859472215676\n",
      "Epoch 268/400: Train Loss: 2916.040788643677 Test Loss: 2786.9965795339367\n",
      "Epoch 269/400: Train Loss: 2916.003266809476 Test Loss: 2787.0072534744863\n",
      "Epoch 270/400: Train Loss: 2915.965786310687 Test Loss: 2787.017968764936\n",
      "Epoch 271/400: Train Loss: 2915.9283470832756 Test Loss: 2787.028725132321\n",
      "Epoch 272/400: Train Loss: 2915.8909490635033 Test Loss: 2787.039522308826\n",
      "Epoch 273/400: Train Loss: 2915.853592187912 Test Loss: 2787.0503600316497\n",
      "Epoch 274/400: Train Loss: 2915.8162763933296 Test Loss: 2787.061238042849\n",
      "Epoch 275/400: Train Loss: 2915.779001616864 Test Loss: 2787.0721560891966\n",
      "Epoch 276/400: Train Loss: 2915.7417677958983 Test Loss: 2787.0831139220554\n",
      "Epoch 277/400: Train Loss: 2915.704574868092 Test Loss: 2787.094111297234\n",
      "Epoch 278/400: Train Loss: 2915.6674227713743 Test Loss: 2787.1051479748744\n",
      "Epoch 279/400: Train Loss: 2915.6303114439434 Test Loss: 2787.1162237193107\n",
      "Epoch 280/400: Train Loss: 2915.5932408242634 Test Loss: 2787.1273382989743\n",
      "Epoch 281/400: Train Loss: 2915.556210851061 Test Loss: 2787.138491486254\n",
      "Epoch 282/400: Train Loss: 2915.519221463322 Test Loss: 2787.1496830574033\n",
      "Epoch 283/400: Train Loss: 2915.482272600293 Test Loss: 2787.160912792424\n",
      "Epoch 284/400: Train Loss: 2915.4453642014732 Test Loss: 2787.172180474965\n",
      "Epoch 285/400: Train Loss: 2915.408496206615 Test Loss: 2787.183485892225\n",
      "Epoch 286/400: Train Loss: 2915.371668555722 Test Loss: 2787.194828834846\n",
      "Epoch 287/400: Train Loss: 2915.3348811890423 Test Loss: 2787.2062090968275\n",
      "Epoch 288/400: Train Loss: 2915.2981340470737 Test Loss: 2787.217626475437\n",
      "Epoch 289/400: Train Loss: 2915.2614270705553 Test Loss: 2787.229080771112\n",
      "Epoch 290/400: Train Loss: 2915.2247602004654 Test Loss: 2787.2405717873817\n",
      "Epoch 291/400: Train Loss: 2915.188133378022 Test Loss: 2787.2520993307844\n",
      "Epoch 292/400: Train Loss: 2915.1515465446782 Test Loss: 2787.263663210785\n",
      "Epoch 293/400: Train Loss: 2915.114999642123 Test Loss: 2787.275263239697\n",
      "Epoch 294/400: Train Loss: 2915.0784926122756 Test Loss: 2787.2868992326103\n",
      "Epoch 295/400: Train Loss: 2915.0420253972848 Test Loss: 2787.298571007316\n",
      "Epoch 296/400: Train Loss: 2915.005597939528 Test Loss: 2787.3102783842373\n",
      "Epoch 297/400: Train Loss: 2914.969210181605 Test Loss: 2787.322021186359\n",
      "Epoch 298/400: Train Loss: 2914.9328620663428 Test Loss: 2787.333799239165\n",
      "Epoch 299/400: Train Loss: 2914.896553536788 Test Loss: 2787.3456123705673\n",
      "Epoch 300/400: Train Loss: 2914.860284536205 Test Loss: 2787.3574604108544\n",
      "Epoch 301/400: Train Loss: 2914.8240550080786 Test Loss: 2787.369343192619\n",
      "Epoch 302/400: Train Loss: 2914.7878648961064 Test Loss: 2787.3812605507087\n",
      "Epoch 303/400: Train Loss: 2914.7517141442004 Test Loss: 2787.393212322163\n",
      "Epoch 304/400: Train Loss: 2914.7156026964835 Test Loss: 2787.4051983461663\n",
      "Epoch 305/400: Train Loss: 2914.6795304972898 Test Loss: 2787.4172184639847\n",
      "Epoch 306/400: Train Loss: 2914.64349749116 Test Loss: 2787.429272518919\n",
      "Epoch 307/400: Train Loss: 2914.60750362284 Test Loss: 2787.4413603562584\n",
      "Epoch 308/400: Train Loss: 2914.571548837281 Test Loss: 2787.453481823222\n",
      "Epoch 309/400: Train Loss: 2914.5356330796385 Test Loss: 2787.4656367689226\n",
      "Epoch 310/400: Train Loss: 2914.4997562952644 Test Loss: 2787.477825044314\n",
      "Epoch 311/400: Train Loss: 2914.463918429713 Test Loss: 2787.490046502147\n",
      "Epoch 312/400: Train Loss: 2914.4281194287337 Test Loss: 2787.502300996928\n",
      "Epoch 313/400: Train Loss: 2914.3923592382735 Test Loss: 2787.514588384876\n",
      "Epoch 314/400: Train Loss: 2914.35663780447 Test Loss: 2787.5269085238847\n",
      "Epoch 315/400: Train Loss: 2914.3209550736574 Test Loss: 2787.5392612734754\n",
      "Epoch 316/400: Train Loss: 2914.285310992356 Test Loss: 2787.551646494771\n",
      "Epoch 317/400: Train Loss: 2914.249705507278 Test Loss: 2787.564064050446\n",
      "Epoch 318/400: Train Loss: 2914.2141385653213 Test Loss: 2787.576513804697\n",
      "Epoch 319/400: Train Loss: 2914.1786101135713 Test Loss: 2787.5889956232077\n",
      "Epoch 320/400: Train Loss: 2914.1431200992947 Test Loss: 2787.601509373115\n",
      "Epoch 321/400: Train Loss: 2914.107668469944 Test Loss: 2787.614054922971\n",
      "Epoch 322/400: Train Loss: 2914.072255173151 Test Loss: 2787.626632142717\n",
      "Epoch 323/400: Train Loss: 2914.0368801567265 Test Loss: 2787.6392409036466\n",
      "Epoch 324/400: Train Loss: 2914.0015433686617 Test Loss: 2787.6518810783837\n",
      "Epoch 325/400: Train Loss: 2913.966244757122 Test Loss: 2787.664552540843\n",
      "Epoch 326/400: Train Loss: 2913.9309842704492 Test Loss: 2787.6772551662043\n",
      "Epoch 327/400: Train Loss: 2913.8957618571594 Test Loss: 2787.6899888308935\n",
      "Epoch 328/400: Train Loss: 2913.86057746594 Test Loss: 2787.70275341254\n",
      "Epoch 329/400: Train Loss: 2913.8254310456487 Test Loss: 2787.7155487899618\n",
      "Epoch 330/400: Train Loss: 2913.790322545314 Test Loss: 2787.7283748431387\n",
      "Epoch 331/400: Train Loss: 2913.7552519141323 Test Loss: 2787.7412314531834\n",
      "Epoch 332/400: Train Loss: 2913.720219101466 Test Loss: 2787.7541185023188\n",
      "Epoch 333/400: Train Loss: 2913.6852240568433 Test Loss: 2787.7670358738546\n",
      "Epoch 334/400: Train Loss: 2913.650266729956 Test Loss: 2787.779983452166\n",
      "Epoch 335/400: Train Loss: 2913.61534707066 Test Loss: 2787.7929611226714\n",
      "Epoch 336/400: Train Loss: 2913.5804650289715 Test Loss: 2787.805968771805\n",
      "Epoch 337/400: Train Loss: 2913.545620555066 Test Loss: 2787.819006287004\n",
      "Epoch 338/400: Train Loss: 2913.5108135992787 Test Loss: 2787.832073556683\n",
      "Epoch 339/400: Train Loss: 2913.476044112104 Test Loss: 2787.8451704702175\n",
      "Epoch 340/400: Train Loss: 2913.441312044189 Test Loss: 2787.8582969179192\n",
      "Epoch 341/400: Train Loss: 2913.4066173463398 Test Loss: 2787.871452791024\n",
      "Epoch 342/400: Train Loss: 2913.371959969513 Test Loss: 2787.8846379816646\n",
      "Epoch 343/400: Train Loss: 2913.3373398648187 Test Loss: 2787.8978523828646\n",
      "Epoch 344/400: Train Loss: 2913.302756983521 Test Loss: 2787.9110958885085\n",
      "Epoch 345/400: Train Loss: 2913.2682112770294 Test Loss: 2787.924368393338\n",
      "Epoch 346/400: Train Loss: 2913.233702696907 Test Loss: 2787.9376697929215\n",
      "Epoch 347/400: Train Loss: 2913.1992311948625 Test Loss: 2787.950999983649\n",
      "Epoch 348/400: Train Loss: 2913.1647967227514 Test Loss: 2787.9643588627096\n",
      "Epoch 349/400: Train Loss: 2913.1303992325747 Test Loss: 2787.977746328085\n",
      "Epoch 350/400: Train Loss: 2913.0960386764796 Test Loss: 2787.9911622785207\n",
      "Epoch 351/400: Train Loss: 2913.061715006753 Test Loss: 2788.0046066135237\n",
      "Epoch 352/400: Train Loss: 2913.027428175828 Test Loss: 2788.0180792333485\n",
      "Epoch 353/400: Train Loss: 2912.993178136275 Test Loss: 2788.0315800389735\n",
      "Epoch 354/400: Train Loss: 2912.958964840809 Test Loss: 2788.045108932095\n",
      "Epoch 355/400: Train Loss: 2912.9247882422796 Test Loss: 2788.0586658151174\n",
      "Epoch 356/400: Train Loss: 2912.890648293676 Test Loss: 2788.072250591128\n",
      "Epoch 357/400: Train Loss: 2912.856544948125 Test Loss: 2788.085863163901\n",
      "Epoch 358/400: Train Loss: 2912.822478158887 Test Loss: 2788.0995034378716\n",
      "Epoch 359/400: Train Loss: 2912.78844787936 Test Loss: 2788.113171318132\n",
      "Epoch 360/400: Train Loss: 2912.754454063074 Test Loss: 2788.12686671042\n",
      "Epoch 361/400: Train Loss: 2912.72049666369 Test Loss: 2788.140589521101\n",
      "Epoch 362/400: Train Loss: 2912.6865756350057 Test Loss: 2788.1543396571665\n",
      "Epoch 363/400: Train Loss: 2912.652690930944 Test Loss: 2788.1681170262186\n",
      "Epoch 364/400: Train Loss: 2912.6188425055616 Test Loss: 2788.1819215364562\n",
      "Epoch 365/400: Train Loss: 2912.5850303130414 Test Loss: 2788.1957530966756\n",
      "Epoch 366/400: Train Loss: 2912.551254307695 Test Loss: 2788.209611616249\n",
      "Epoch 367/400: Train Loss: 2912.5175144439618 Test Loss: 2788.223497005119\n",
      "Epoch 368/400: Train Loss: 2912.4838106764055 Test Loss: 2788.237409173799\n",
      "Epoch 369/400: Train Loss: 2912.450142959716 Test Loss: 2788.251348033344\n",
      "Epoch 370/400: Train Loss: 2912.416511248705 Test Loss: 2788.2653134953625\n",
      "Epoch 371/400: Train Loss: 2912.382915498312 Test Loss: 2788.279305471993\n",
      "Epoch 372/400: Train Loss: 2912.3493556635926 Test Loss: 2788.293323875904\n",
      "Epoch 373/400: Train Loss: 2912.3158316997287 Test Loss: 2788.3073686202847\n",
      "Epoch 374/400: Train Loss: 2912.2823435620207 Test Loss: 2788.32143961883\n",
      "Epoch 375/400: Train Loss: 2912.2488912058884 Test Loss: 2788.335536785741\n",
      "Epoch 376/400: Train Loss: 2912.21547458687 Test Loss: 2788.349660035719\n",
      "Epoch 377/400: Train Loss: 2912.1820936606237 Test Loss: 2788.363809283942\n",
      "Epoch 378/400: Train Loss: 2912.148748382921 Test Loss: 2788.3779844460805\n",
      "Epoch 379/400: Train Loss: 2912.1154387096526 Test Loss: 2788.392185438274\n",
      "Epoch 380/400: Train Loss: 2912.0821645968235 Test Loss: 2788.4064121771266\n",
      "Epoch 381/400: Train Loss: 2912.048926000553 Test Loss: 2788.4206645797058\n",
      "Epoch 382/400: Train Loss: 2912.015722877073 Test Loss: 2788.4349425635323\n",
      "Epoch 383/400: Train Loss: 2911.9825551827316 Test Loss: 2788.449246046571\n",
      "Epoch 384/400: Train Loss: 2911.949422873985 Test Loss: 2788.4635749472313\n",
      "Epoch 385/400: Train Loss: 2911.916325907403 Test Loss: 2788.4779291843533\n",
      "Epoch 386/400: Train Loss: 2911.883264239665 Test Loss: 2788.492308677209\n",
      "Epoch 387/400: Train Loss: 2911.8502378275593 Test Loss: 2788.506713345492\n",
      "Epoch 388/400: Train Loss: 2911.8172466279857 Test Loss: 2788.5211431093103\n",
      "Epoch 389/400: Train Loss: 2911.7842905979487 Test Loss: 2788.535597889187\n",
      "Epoch 390/400: Train Loss: 2911.7513696945634 Test Loss: 2788.5500776060494\n",
      "Epoch 391/400: Train Loss: 2911.7184838750486 Test Loss: 2788.5645821812236\n",
      "Epoch 392/400: Train Loss: 2911.6856330967307 Test Loss: 2788.5791115364314\n",
      "Epoch 393/400: Train Loss: 2911.652817317041 Test Loss: 2788.59366559379\n",
      "Epoch 394/400: Train Loss: 2911.620036493515 Test Loss: 2788.608244275794\n",
      "Epoch 395/400: Train Loss: 2911.587290583792 Test Loss: 2788.6228475053204\n",
      "Epoch 396/400: Train Loss: 2911.5545795456137 Test Loss: 2788.637475205626\n",
      "Epoch 397/400: Train Loss: 2911.521903336825 Test Loss: 2788.6521273003314\n",
      "Epoch 398/400: Train Loss: 2911.4892619153716 Test Loss: 2788.6668037134286\n",
      "Epoch 399/400: Train Loss: 2911.4566552393007 Test Loss: 2788.681504369269\n",
      "Epoch 400/400: Train Loss: 2911.4240832667597 Test Loss: 2788.696229192562\n"
     ]
    }
   ],
   "source": [
    "fit(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1e5a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
